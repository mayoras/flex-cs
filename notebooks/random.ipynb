{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzFVvanFrrwe"
      },
      "source": [
        "# Selección de clientes RandomSampling - Método _baseline_\n",
        "\n",
        "En este notebook vamos a entrenar un modelo de ML utilizando el paradigma de Aprendizaje Federado [1] para los problemas de clasificación de dígitos manuscritos MNIST [2] y clasificación de imágenes a color CIFAR-10 [3]. Usaremos estos dos problemas de visión por computador para la implementación y análisis de rendimiento del método RandomSampling para la selección de clientes.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1602.05629\n",
        ">\n",
        "> [2] http://yann.lecun.com/exdb/mnist\n",
        ">\n",
        "> [3] https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4BfI6bDrg3o",
        "outputId": "d9883671-6383-4466-fb97-d876b35d03fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLEXible is not installed.\n",
            "Installing dependency flexible-fl...\n",
            "Collecting flexible-fl\n",
            "  Downloading flexible_fl-0.6.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.25.2)\n",
            "Collecting multiprocess (from flexible-fl)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.3.2)\n",
            "Collecting cardinality (from flexible-fl)\n",
            "  Downloading cardinality-0.1.1.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sultan (from flexible-fl)\n",
            "  Downloading sultan-0.9.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (4.66.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.13.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (5.1.0)\n",
            "Collecting tensorly (from flexible-fl)\n",
            "  Downloading tensorly-0.8.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (2.31.0)\n",
            "Collecting dill>=0.3.8 (from multiprocess->flexible-fl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->flexible-fl) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->flexible-fl) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->flexible-fl) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (1.7.1)\n",
            "Downloading flexible_fl-0.6.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sultan-0.9.1-py3-none-any.whl (16 kB)\n",
            "Downloading tensorly-0.8.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cardinality\n",
            "  Building wheel for cardinality (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cardinality: filename=cardinality-0.1.1-py3-none-any.whl size=2588 sha256=541af036a083cb81d37827ace8f4b0073e65505bdbe39d1ad2edda5ea8552137\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/19/d1/2665c004b583a7d1880fa59055a3e462d6e35841a01b57010b\n",
            "Successfully built cardinality\n",
            "Installing collected packages: sultan, cardinality, dill, tensorly, multiprocess, flexible-fl\n",
            "Successfully installed cardinality-0.1.1 dill-0.3.8 flexible-fl-0.6.1 multiprocess-0.70.16 sultan-0.9.1 tensorly-0.8.1\n"
          ]
        }
      ],
      "source": [
        "# install FLEXible framework if not installed\n",
        "try:\n",
        "    import flex\n",
        "    print(\"FLEXible is installed.\")\n",
        "except:\n",
        "    print(\"FLEXible is not installed.\\nInstalling dependency flexible-fl...\")\n",
        "    !pip install flexible-fl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-31n0URu8tVs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_accuracy(loss, accuracy, title=\"Learning Curves\"):\n",
        "    # Example data\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, 'b', label='Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, 'g', label='Accuracy')\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "\n",
        "    # Show the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5jdIu00Ftc1u",
        "outputId": "c458fa40-5310-47ae-9f61-419d13d91960"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# select device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar datasets - MNIST, CIFAR-10\n",
        "\n",
        "Cargaremos $2$ de los datasets que usaremos para entrenar el modelo de ML.\n"
      ],
      "metadata": {
        "id": "mga9Y9UoOZ1f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIIPVTjuSl00"
      },
      "source": [
        "## MNIST\n",
        "El primero será MNIST (Modified National Institute of Standards and Technology database), que consiste en imágenes de $28$ pixeles de ancho y alto en escala de grises de dígitos manuscritos del $0$ al $9$. Formalmente, definimos el problema de ML como un problema de clasificación multiclase, donde definimos una imagen de entrada $X_n \\in \\mathcal X$ se define como una matriz $X_{28\\times 28}, x_i \\in \\{1, ..., 255\\}$, donde $x_i$ es un pixel de la imagen $X$. Cada imágen de entrada $X_n\\in \\mathcal X$ es etiquetada con una clase $y_n \\in \\mathcal Y, y = \\{0, 1, ..., 9\\}$ que representa el dígito al que corresponde la imagen. El conjunto de entrenamiento consta de $N=240,000$ imágenes.\n",
        "\n",
        "El dataset descargado será el de _Extended MNIST_ [1], que comprende una versión extendida del concepto original de MNIST para proporcionar dígitos y letras manuscritas, una cantidad más grande de datos, diferentes formas de separar los datos (solo dígitos, letras, por clase, ...), etc. Nosotros solo utilizaremos los dígitos para entrenar el clásico MNIST, del cual su versión extendida consta de $N = 280,000$ imágenes de dígitos manuscritos.\n",
        "\n",
        "Una característica importante de EMNIST es que cada imagen tendrá asociado un autor (i.e. la persona que ha escrito el dígito o letra), tal que un dígito es escrito por un solo autor pero un autor puede escribir muchos dígitos. Esto nos da la ventaja de poder distribuir los datos de forma No-IID tal que cada cliente tendrá los datos con autoría de un solo autor.\n",
        "\n",
        "FLEX nos da utilidades para cargar EMNIST y federarlo automáticamente con la función `load`, donde además descargamos el dataset de test y solo usaremos el dataset de dígitos clásico de MNIST. Definimos también las transformaciones a realizar a los datos que son simplemente normalizar cada pixel a valores de $[0,1]$ (función `ToTensor()`) y normalizar los valores de cada pixel con una media y desviación estándar de $0.5$.\n",
        "\n",
        "> [1] https://www.nist.gov/itl/products-and-services/emnist-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYOy-Bmsu0_J",
        "outputId": "89ae429d-73f3-4286-ff98-27c9a6095eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fl9fRPPxTUxnC56ACzZ8JiLiew0SMFwt\n",
            "From (redirected): https://drive.google.com/uc?id=1fl9fRPPxTUxnC56ACzZ8JiLiew0SMFwt&confirm=t&uuid=7a472c79-35c3-45f0-b7c7-9e3e6f9d2f12\n",
            "To: /content/emnist-digits.mat\n",
            "100%|██████████| 90.7M/90.7M [00:03<00:00, 22.7MB/s]\n",
            "\u001b[36m[sultan]: md5 -q ./emnist-digits.mat;\u001b[0m\n",
            "DEBUG:sultan:md5 -q ./emnist-digits.mat;\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | NoneType: None\u001b[0m\n",
            "CRITICAL:sultan:| NoneType: None\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: --{ STDERR }-------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ STDERR }-------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | /bin/sh: 1: md5: not found\u001b[0m\n",
            "CRITICAL:sultan:| /bin/sh: 1: md5: not found\n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | Traceback (most recent call last):\u001b[0m\n",
            "CRITICAL:sultan:| Traceback (most recent call last):\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\n",
            "\u001b[01;31m[sultan]: |     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\u001b[0m\n",
            "CRITICAL:sultan:|     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\n",
            "\u001b[01;31m[sultan]: |     self.dump_exception()\u001b[0m\n",
            "CRITICAL:sultan:|     self.dump_exception()\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise self._exception\u001b[0m\n",
            "CRITICAL:sultan:|     raise self._exception\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\u001b[0m\n",
            "CRITICAL:sultan:|     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\n",
            "\u001b[01;31m[sultan]: | subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\u001b[0m\n",
            "CRITICAL:sultan:| subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[36m[sultan]: md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\u001b[0m\n",
            "DEBUG:sultan:md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\n",
            "\u001b[36m[sultan]: md5 -q ./emnist-digits.mat;\u001b[0m\n",
            "DEBUG:sultan:md5 -q ./emnist-digits.mat;\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | NoneType: None\u001b[0m\n",
            "CRITICAL:sultan:| NoneType: None\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: --{ STDERR }-------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ STDERR }-------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | /bin/sh: 1: md5: not found\u001b[0m\n",
            "CRITICAL:sultan:| /bin/sh: 1: md5: not found\n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | Traceback (most recent call last):\u001b[0m\n",
            "CRITICAL:sultan:| Traceback (most recent call last):\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\n",
            "\u001b[01;31m[sultan]: |     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\u001b[0m\n",
            "CRITICAL:sultan:|     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\n",
            "\u001b[01;31m[sultan]: |     self.dump_exception()\u001b[0m\n",
            "CRITICAL:sultan:|     self.dump_exception()\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise self._exception\u001b[0m\n",
            "CRITICAL:sultan:|     raise self._exception\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\u001b[0m\n",
            "CRITICAL:sultan:|     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\n",
            "\u001b[01;31m[sultan]: | subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\u001b[0m\n",
            "CRITICAL:sultan:| subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[36m[sultan]: md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\u001b[0m\n",
            "DEBUG:sultan:md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\n"
          ]
        }
      ],
      "source": [
        "from flex.datasets import load\n",
        "from torchvision import transforms\n",
        "\n",
        "flex_dataset_mnist, test_data = load(\"federated_emnist\", return_test=True, split=\"digits\")\n",
        "\n",
        "# Assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_mnist[server_id] = test_data\n",
        "\n",
        "# apply transforms\n",
        "mnist_transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6KW3xiYW3e"
      },
      "source": [
        "## CIFAR-10\n",
        "\n",
        "El segundo dataset es CIFAR-10 (_Canadian Institute for Advanced Research_)[1], el cual es otro de los datasets junto con MNIST, más utilizados en el campo del Deep Learning y Visión por Computador. CIFAR-10 consiste en una colección de imágenes de $32$ pixeles de altura y de ancho a color ($3$ canales RGB) representando $10$ objetos reales:\n",
        "- Avión (airplane)\n",
        "- Automóvil (automobile)\n",
        "- Pájaro (bird)\n",
        "- Gato (cat)\n",
        "- Ciervo (deer)\n",
        "- Perro (dog)\n",
        "- Rana (frog)\n",
        "- Caballo (horse)\n",
        "- Barco (ship)\n",
        "- Camión (truck)\n",
        "\n",
        "Formalmente, definimos el problema de ML como un problema de clasificación multiclase, donde definimos una imagen de entrada $X_n \\in \\mathcal X$ se define como una matriz $X_{32\\times 32 \\times 3}, x_i \\in \\{1, ..., 255\\}$, donde $x_i^c$ es un pixel de la imagen $X$ en el canal $c$. Cada imágen de entrada $X_n\\in \\mathcal X$ es etiquetada con una clase $y_n \\in \\mathcal Y, y = \\{0, 1, ..., 9\\}$ tal que indexa la lista de clases $C=(c_0=\\text{plane}, c_1=\\text{automobile}, ..., c_9=\\text{truck})$. El conjunto de entrenamiento consta de $N=50,000$ datos de entrada.\n",
        "\n",
        "A diferencia de EMNIST, FLEX no carga por defecto CIFAR-10, por lo que deberemos de descargar y federar manualmente el dataset. Utilizaremos el dataset descargado desde `torchvision` de PyTorch y federamos utilizando una configuración por defecto de FLEX con un número de nodos o clientes de $K = 100$ clientes. Definimos también las transformaciones para escalar los valores a $[0,1]$ y normalizar con medias y desviaciones estándar específicas de CIFAR-10 [2].\n",
        "\n",
        "> [1] https://www.cs.toronto.edu/~kriz/cifar.html\n",
        ">\n",
        "> [2] https://github.com/kuangliu/pytorch-cifar/issues/19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErtVS1zPd-Zb",
        "outputId": "0dd7f159-3217-470d-b9ca-0979df6891d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 88228868.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from flex.data import FedDataDistribution, FedDatasetConfig, Dataset\n",
        "\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=\".\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=None,  # we apply them later in training process\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\".\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=None,  # we apply them later in training process\n",
        ")\n",
        "\n",
        "config = FedDatasetConfig(seed=33)\n",
        "config.replacement = False\n",
        "config.n_nodes = 100\n",
        "\n",
        "# create Federated data distribution of CIFAR-10\n",
        "flex_dataset_cifar = FedDataDistribution.from_config(\n",
        "    centralized_data=Dataset.from_torchvision_dataset(train_data),\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_cifar[server_id] = Dataset.from_torchvision_dataset(test_data)\n",
        "\n",
        "# apply transforms\n",
        "cifar_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalizar con las medias y desviaciones estándar específicas de CIFAR-10\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJLc-rB4gfzS"
      },
      "source": [
        "# Modelos de aprendizaje\n",
        "\n",
        "Definimos dos modelos de Machine Learning, uno para cada uno de los dos datasets que hemos definido anteriormente. Para ambos problemas, usaremos como función de pérdida o _criterion_ la función de entropía cruzada o _Cross Entropy Loss_ cuya definición [1] la define como (para una muestra $n$):\n",
        "\n",
        "$$l_n = -w_{y_n} \\cdot \\log \\frac{\\exp (x_n, y_n)}{\\sum_{c=0}^C \\exp (x_n, c)} =-w_{y_n}\\cdot \\log (\\text{Softmax} (x_n, y_n))$$\n",
        "\n",
        "En ambos problemas utilizaremos Adam [2] como optimizador o algoritmo de aprendizaje. Utilizaremos los mismos parámetros que en [2] al haber hecho los experimentos sobre modelos similares y con los mismos datasets que estamos usando y que han demostrando buenos resultados: $\\eta = 0.01$ y $\\beta_1 = 0.9,\\beta_2 = 0.999$. Estos parámetros están definidos por defecto en la librería de PyTorch [3].\n",
        "\n",
        "> [1] https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        ">\n",
        "> [2] https://arxiv.org/abs/1412.6980\n",
        ">\n",
        "> [3] https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPizwuhm28X"
      },
      "source": [
        "## Perceptrón Multicapa (MNIST)\n",
        "\n",
        "El primero, que lo usaremos para ajustar MNIST, será un Perceptrón Multicapa (MLP) de dos capas ocultas de $128$ parámetros la primera y $10$ la segunda (correspondiente al número de clases), y usando ReLU [1] como función de activación.\n",
        "\n",
        "El modelo inicialmente lo tendrá el servidor y será copiado a cada uno de los clientes. En FLEX usamos el decorador `@init_model_server` para inicializar el modelo en el servidor, donde también podemos además de nuestra arquitectura del modelo, el optimizador y la función de pérdida a usar.\n",
        "\n",
        "> [1] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wro1kxtkw1w0"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer Perceptron classifier with two hidden layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features: Tuple[int, int], hidden_features: int, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "\n",
        "        width, height = in_features\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(width * height, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AOdn5HT0iNg"
      },
      "source": [
        "## Red Neuronal Convolucional (CNN) - CIFAR-10\n",
        "\n",
        "Para nuestro segundo problema, que consta de ajustar un modelo para clasificar CIFAR-10, utilizaremos un red neuronal convolucional o CNN. A diferencia de MNIST, CIFAR-10 tiene unos datos de entrada con una dimensionalidad más compleja, por lo que utilizar una CNN reduciría la cantidad de parámetros necesarios a entrenar.\n",
        "\n",
        "Usaremos la red neuronal convolucional utilizada en los experimentos de el algoritmo GreedyFed [1], debido a que es una arquitectura con pocos parámetros a entrenar, lo que nos permite obtener los resultados deseados en poco tiempo. Concretamente, la arquitectura de la red a implementar será la de una CNN estándar que comprende dos capas convolucionales (CONV) $4\\times 4$:\n",
        "- Ambas con $8$ canales de salida.\n",
        "- Cada uno de ellos activados por ReLU [2].\n",
        "- Y aplicando _Max Pooling_ $2\\times 2$ con _stride_ de $2$ en la salida de cada capa convolucional para reducir la dimensionalidad del los mapas de activación.\n",
        "\n",
        "Seguido de $1$ capa _Fully-Connected_ de $10$ unidades activada por Softmax para la capa de final de salida.\n",
        "\n",
        "> [1] https://arxiv.org/abs/2312.09108\n",
        ">\n",
        "> [2] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iHpTXr6CbRw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolucional Neural Net classifier for CIFAR-10 image recognition (32x32 images).\n",
        "\n",
        "    Model architecture based on GreedyFed experimental setting:\n",
        "    - https://github.com/pringlesinghal/GreedyFed/blob/b2d928670bcd014035830c531bea9f76c57b4b70/model.py#L46\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, in_width: int = 32, in_height: int = 32, output_dim: int = 10):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.input_w = in_width\n",
        "        self.input_h = in_height\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # First CONV layer\n",
        "        out_channels_conv_1 = 8\n",
        "        kernel_conv_1_size = 4\n",
        "        padding_conv_1 = 1\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels=out_channels_conv_1,\n",
        "            kernel_size=kernel_conv_1_size,\n",
        "            padding=padding_conv_1,\n",
        "        )\n",
        "\n",
        "        output_w_1, output_h_1 = self.__conv_output_dims(\n",
        "            self.input_w, self.input_h, kernel_conv_1_size, padding_conv_1\n",
        "        )\n",
        "\n",
        "        # First MAXPOOL layer\n",
        "        kernel_pool_size = 2\n",
        "        stride_pool = 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=kernel_pool_size, stride=stride_pool)\n",
        "\n",
        "        output_w_2, output_h_2 = self.__pool_output_dims(\n",
        "            output_w_1, output_h_1, kernel_pool_size, stride_pool\n",
        "        )\n",
        "\n",
        "        # Second CONV layer\n",
        "        in_channels_conv_2 = out_channels_conv_1\n",
        "        out_channels_conv_2 = 8\n",
        "        kernel_conv_2_size = 4\n",
        "        padding_conv_2 = 1\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels_conv_2,\n",
        "            out_channels=out_channels_conv_2,\n",
        "            kernel_size=kernel_conv_2_size,\n",
        "            padding=padding_conv_2\n",
        "        )\n",
        "\n",
        "        output_w_3, output_h_3 = self.__conv_output_dims(\n",
        "            output_w_2, output_h_2, kernel_conv_2_size, padding_conv_2\n",
        "        )\n",
        "\n",
        "        # after second MAXPOOL layer, compute final out width and height\n",
        "        output_w_4, output_h_4 = self.__pool_output_dims(\n",
        "            output_w_3, output_h_3, kernel_pool_size, stride_pool\n",
        "        )\n",
        "\n",
        "        # Fully-Connected layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        input_size_4 = int(output_w_4 * output_h_4 * out_channels_conv_2)\n",
        "        self.fc = nn.Linear(input_size_4, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        # x = self.flatten(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def __conv_output_dims(self, width: int, height: int, kernel_size: int, padding: int = 1):\n",
        "        output_w = width - kernel_size + 2 * padding + 1\n",
        "        output_h = height - kernel_size + 2 * padding + 1\n",
        "        return output_w, output_h\n",
        "\n",
        "    def __pool_output_dims(self, width: int, height: int, kernel_size: int, stride: int = 1):\n",
        "        output_w = np.floor((width - kernel_size) / stride) + 1\n",
        "        output_h = np.floor((height - kernel_size) / stride) + 1\n",
        "\n",
        "        return output_w, output_h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3LoDmezyvud"
      },
      "source": [
        "## Configuración básica de escenario de FL\n",
        "\n",
        "Configuraremos un escenario de aprendizaje federado centralizado (CFL) usando la librería FLEXible. FLEXible [1], o simplemente FLEX, es una librería de Python que proporciona un framework para la construcción de entornos de aprendizaje federado para fines de investigación y simulación. FLEX pretende dar flexibilidad en cuanto la gran variedad de escenarios y necesidades que se pueden llegar a plantear para experimentar en entornos federados.\n",
        "\n",
        "Uno de nuestros objetivos es _integrar_ un método o técnica de selección de clientes en un escenario inicial de FL. La flexibilidad de FLEX nos permitirá conseguir este objetivo gracias a su flujo de mensajes entre entidades separadas por roles, y por la arquitectura modular de estas que nos permiten almacenar información de manera conveniente [1].\n",
        "\n",
        "> [1] https://arxiv.org/abs/2404.06127\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOLo8WYFBnwR"
      },
      "source": [
        "### Inicialización del modelo del servidor\n",
        "\n",
        "El primer paso de nuestro bucle de entrenamiento en aprendizaje federado es inicializar el modelo FLEX del servidor (que posteriormente se distribuirá a los clientes).\n",
        "\n",
        "Con FLEX podemos hacer uso del decorador `init_server_model` para facilitarnos esta tarea. Esta función entonces debe de instanciar y devolver un modelo de FLEX en donde además definimos el modelo de aprendizaje $^1$, la función de pérdida, el optimizador y cualquier otra información que se usará en las demás fases de la ronda de entrenamiento.\n",
        "\n",
        "Definimos dos funciones que realizan esta tarea, dos para cada una de nuestras arquitecturas, el Perceptrón Multicapa y la Red Neuronal Convolucional. Como habíamos especificado en el planteamiento del problema de ML a resolver, utilizaremos el optimizador SGD-Adam con sus parámetros por defecto, y la Cross-Entropy como función de pérdida.\n",
        "\n",
        "> $^1$ No se debe confundir el modelo de tipo `FlexModel` con un modelo de ML. El primero implementa el bloque fundamental que define una entidad en el escenario de FL (donde almacenamos información, sus datos locales, su modelo de aprendizaje, ...). El segundo es un modelo aprendizaje como lo puede ser un módulo de PyTorch o Tensorflow que realiza el aprendizaje automático y la predicción sobre los datos. Se dejará claro la diferencia entre los dos tipos de modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zAxWjwavQJ0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from flex.pool import init_server_model\n",
        "from flex.pool import FlexPool\n",
        "from flex.model import FlexModel\n",
        "\n",
        "mnist_in_features = (28, 28)\n",
        "mnist_hidden_features = 128\n",
        "\n",
        "@init_server_model\n",
        "def build_server_model_mlp():\n",
        "    server_flex_model = FlexModel()\n",
        "\n",
        "    server_flex_model[\"model\"] = MLP(mnist_in_features, mnist_hidden_features)\n",
        "\n",
        "    # Required to store this for later stages of the FL training process\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
        "    return server_flex_model\n",
        "\n",
        "cifar_in_channels = 3\n",
        "cifar_num_classes = 10\n",
        "\n",
        "@init_server_model\n",
        "def build_server_model_cnn():\n",
        "    server_flex_model = FlexModel()\n",
        "\n",
        "    server_flex_model[\"model\"] = CNN(in_channels=cifar_in_channels, output_dim=cifar_num_classes)\n",
        "\n",
        "    # Required to store this for later stages of the FL training process\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
        "    return server_flex_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5LzYqxUzIaU"
      },
      "source": [
        "---\n",
        "### Distribuir modelo del servidor\n",
        "\n",
        "El siguiente paso a realizar en un flujo de entrenamiento de aprendizaje federado es la distribución del modelo del servidor a los cientes. Con FLEX, podemos utilizar el decorador `@deploy_server_model` para distribuir el modelo del servidor a los clientes, definiendo una función que devuelva el modelo a almacenar en cada cliente.\n",
        "\n",
        "En este caso, realizamos una copia profunda del modelo del servidor que será asignado a cada cliente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwxXBCoCytZh"
      },
      "outputs": [],
      "source": [
        "from flex.pool import deploy_server_model\n",
        "import copy\n",
        "\n",
        "\n",
        "@deploy_server_model\n",
        "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
        "    return copy.deepcopy(server_flex_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1S01aEFzevA"
      },
      "source": [
        "### Actualización del modelo del lado del cliente\n",
        "\n",
        "Definimos la función encargada de realizar el entrenamiento del modelo sobre los datos locales del cliente. A esta función, es conveniente pasarle como parámetros los hiperparámetros de entrenamiento de un modelo de ML convencional como el número de épocas $E$ y el tamaño de _batch_ $B$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvjgKEJ7zdzd"
      },
      "outputs": [],
      "source": [
        "from flex.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train(client_flex_model: FlexModel, client_data: Dataset, **kwargs):\n",
        "    # parse kwargs\n",
        "    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 5\n",
        "    batch_size = kwargs[\"batch_size\"] if \"batch_size\" in kwargs else 20\n",
        "    transforms = kwargs[\"transforms\"] if \"transforms\" in kwargs else None\n",
        "\n",
        "    # get client data as a torchvision object\n",
        "    train_dataset = client_data.to_torchvision_dataset(transform=transforms)\n",
        "    client_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    # get model\n",
        "    model = client_flex_model[\"model\"]\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    model = model.train()\n",
        "    model = model.to(device)\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "\n",
        "    # train model\n",
        "    for _ in range(epochs):\n",
        "        for imgs, labels in client_dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(imgs)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMSabAvG0u2J"
      },
      "source": [
        "### Obtener los parámetros de los clientes\n",
        "\n",
        "Ahora implementamos la función que hace que el servidor (con rol de agregador) recupere los nuevos parámetros actualizados de los clientes. Con el decorador `@collect_clients_weights` recuperamos los pesos de PyTorch de cada cliente seleccionado para esa ronda. En el caso de PyTorch, el modelo devuelve los pesos en forma de un diccionario con `state_dict` para el que cada nombre representa una capa de la red y sus parámetros, lo que hacemos será devolver una lista con los valores de ese diccionario correspondientes a los pesos de la red entera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggbUniMr0oGy"
      },
      "outputs": [],
      "source": [
        "from flex.pool import collect_clients_weights\n",
        "\n",
        "\n",
        "@collect_clients_weights\n",
        "def get_clients_weights(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    return [weight_dict[name] for name in weight_dict]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DT76cJ41uFY"
      },
      "source": [
        "### Agregación de los parámetros\n",
        "\n",
        "El servidor/agregador agrega entonces estos nuevos parámetros para conseguir el nuevo modelo global. Utilizamos el decorador `@aggregate_weights` para poder agregar los pesos que hemos recuperado de los clientes en la fase anterior computando la media de los pesos de manera uniforme, conocido como agregador FedAvg [1], donde realizamos la media por columnas para cada capa de pesos.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1602.05629"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR27tyd51cfq"
      },
      "outputs": [],
      "source": [
        "from flex.pool import aggregate_weights\n",
        "import tensorly as tl\n",
        "\n",
        "tl.set_backend(\"pytorch\")\n",
        "\n",
        "\n",
        "@aggregate_weights\n",
        "def aggregate_with_fedavg(list_of_weights: list):\n",
        "    agg_weights = []\n",
        "    for layer_index in range(len(list_of_weights[0])):\n",
        "        weights_per_layer = [weights[layer_index] for weights in list_of_weights]\n",
        "        weights_per_layer = tl.stack(weights_per_layer)\n",
        "        agg_layer = tl.mean(weights_per_layer, axis=0)\n",
        "        agg_weights.append(agg_layer)\n",
        "    return agg_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQtCzF0K2i92"
      },
      "source": [
        "Finalmente, agregamos los pesos al modelo de nuestro servidor/agregador. Sencillamente, para cada capa de nuestro modelo, realizamo una copia del nuevo que hemos agregado en la fase anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fk-VxE52ZNH"
      },
      "outputs": [],
      "source": [
        "from flex.pool import set_aggregated_weights\n",
        "\n",
        "\n",
        "@set_aggregated_weights\n",
        "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
        "    with torch.no_grad():\n",
        "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
        "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
        "            weight_dict[layer_key].copy_(new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB5N8y2a27JU"
      },
      "source": [
        "### Evaluación del modelo global\n",
        "\n",
        "Podemos evaluar el modelo del servidor sobre el dataset de test que hemos definido anteriormente que residía en el mismo servidor. Para ello, definimos una función `evaluate_global_model` que obtenga las predicciones del modelo con el dataset de test y devuelva las metricas resultantes, que en este caso son simplemente la pérdida y la _accuracy_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFZA3_Sn20tn"
      },
      "outputs": [],
      "source": [
        "def evaluate_global_model(server_flex_model: FlexModel, test_data: Dataset):\n",
        "    model = server_flex_model[\"model\"]\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    total_count = 0\n",
        "    model = model.to(device)\n",
        "    criterion = server_flex_model[\"criterion\"]\n",
        "    # get test data as a torchvision object\n",
        "    test_dataset = test_data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=256, shuffle=True, pin_memory=False\n",
        "    )\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_dataloader:\n",
        "            total_count += target.size(0)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            losses.append(criterion(output, target).item())\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
        "\n",
        "    test_loss = sum(losses) / len(losses)\n",
        "    test_acc /= total_count\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p233PYL9Lum"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "A continuación se muestra el bucle de entrenamiento aplicando todas las fases que hemos implementado anteriormente. En esta versión básica se implementa una selección aleatoria (RandomSampling), similar a la propuesta en [1] donde se selecciona de manera aleatoria y uniforme $M$ clientes para la ronda actual. Este proceso se repite de forma iterativa un número determinado de rondas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK5TLa219FLP"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "Problem = Literal[\"mnist\", \"cifar\"]\n",
        "\n",
        "def train_n_rounds(pool: FlexPool, n_rounds: int, clients_per_round=20, problem: Problem = \"mnist\"):\n",
        "    \"\"\"\n",
        "    FL training loop for a certain number of rounds and clients selected.\n",
        "    \"\"\"\n",
        "    # select transformations depending on problem to solve\n",
        "    transforms = mnist_transforms if problem == \"mnist\" else cifar_transforms\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    for i in range(n_rounds):\n",
        "        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n",
        "        selected_clients_pool = pool.clients.select(clients_per_round)\n",
        "        selected_clients = selected_clients_pool.clients\n",
        "        print(f\"Selected clients for this round: {len(selected_clients)}\")\n",
        "        # Deploy the server model to the selected clients\n",
        "        pool.servers.map(copy_server_model_to_clients, selected_clients)\n",
        "        # Each selected client trains her model\n",
        "        selected_clients.map(train, transforms=transforms)\n",
        "        # The aggregador collects weights from the selected clients and aggregates them\n",
        "        pool.aggregators.map(get_clients_weights, selected_clients)\n",
        "        pool.aggregators.map(aggregate_with_fedavg)\n",
        "        # The aggregator send its aggregated weights to the server\n",
        "        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n",
        "        metrics = pool.servers.map(evaluate_global_model)\n",
        "        loss, acc = metrics[0]\n",
        "        losses.append(loss)\n",
        "        accuracies.append(acc)\n",
        "        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}\")\n",
        "\n",
        "    return losses, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "AzJNwpca9rbm",
        "outputId": "23f24c67-8ad9-4581-8be8-c879a82d9738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running round: 1 of 20\n",
            "Selected clients for this round: 70\n",
            "Server: Test acc: 0.6446, test loss: 1.5062\n",
            "\n",
            "Running round: 2 of 20\n",
            "Selected clients for this round: 70\n",
            "Server: Test acc: 0.8067, test loss: 0.8913\n",
            "\n",
            "Running round: 3 of 20\n",
            "Selected clients for this round: 70\n",
            "Server: Test acc: 0.8292, test loss: 0.6349\n",
            "\n",
            "Running round: 4 of 20\n",
            "Selected clients for this round: 70\n",
            "Server: Test acc: 0.8497, test loss: 0.5177\n",
            "\n",
            "Running round: 5 of 20\n",
            "Selected clients for this round: 70\n",
            "Server: Test acc: 0.8578, test loss: 0.4646\n",
            "\n",
            "Running round: 6 of 20\n",
            "Selected clients for this round: 70\n",
            "Server: Test acc: 0.8676, test loss: 0.4263\n",
            "\n",
            "Running round: 7 of 20\n",
            "Selected clients for this round: 70\n",
            "Server: Test acc: 0.8720, test loss: 0.4065\n",
            "\n",
            "Running round: 8 of 20\n",
            "Selected clients for this round: 70\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-255ef6a55df0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpool_cifar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlexPool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_server_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflex_dataset_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_server_model_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_n_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients_per_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mnist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplot_loss_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Learning curves on MNIST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0274f831d2ac>\u001b[0m in \u001b[0;36mtrain_n_rounds\u001b[0;34m(pool, n_rounds, clients_per_round, problem)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_server_model_to_clients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Each selected client trains her model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mselected_clients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# The aggregador collects weights from the selected clients and aggregates them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_clients_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flex/pool/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, dst_pool, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdst_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             res = [\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flex/pool/pool.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdst_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             res = [\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             ]\n",
            "\u001b[0;32m<ipython-input-10-7d005dda7652>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(client_flex_model, client_data, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flex/data/dataset_pt_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"std evaluated to zero after conversion to {dtype}, leading to division by zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "pool_mnist = FlexPool.client_server_pool(flex_dataset_mnist, init_func=build_server_model_mlp)\n",
        "pool_cifar = FlexPool.client_server_pool(flex_dataset_cifar, init_func=build_server_model_cnn)\n",
        "\n",
        "losses, accuracies = train_n_rounds(pool_mnist, n_rounds=20, clients_per_round=70, problem=\"mnist\")\n",
        "plot_loss_accuracy(losses, accuracies, title=\"Learning curves on MNIST\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses, accuracies = train_n_rounds(pool_cifar, n_rounds=20, clients_per_round=20, problem=\"cifar\")\n",
        "plot_loss_accuracy(losses, accuracies, title=\"Learning curves on CIFAR10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "geMgarHnf3cD",
        "outputId": "dbfea7a5-e348-4e43-c297-e1a782a3affe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running round: 1 of 20\n",
            "Selected clients for this round: 20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d4c39d6bc89f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_n_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients_per_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cifar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_loss_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Learning curves on CIFAR10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0274f831d2ac>\u001b[0m in \u001b[0;36mtrain_n_rounds\u001b[0;34m(pool, n_rounds, clients_per_round, problem)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_server_model_to_clients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Each selected client trains her model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mselected_clients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# The aggregador collects weights from the selected clients and aggregates them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_clients_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flex/pool/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, dst_pool, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdst_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             res = [\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flex/pool/pool.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdst_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             res = [\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             ]\n",
            "\u001b[0;32m<ipython-input-10-7d005dda7652>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(client_flex_model, client_data, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3QVTxK9o4tN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}