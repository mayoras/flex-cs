{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzFVvanFrrwe"
      },
      "source": [
        "# Selección de clientes con Active Federated Learning - MNIST\n",
        "\n",
        "En este notebook vamos a utilizar nuestro modelo de entrenamiento en Aprendizaje Federado utilizando AFL como algoritmo de selección de clientes [1] sobre el dataset de MNIST, que es un conjunto de imágenes de dígitos manuscritos del 0 al 9 [2]. Se trata de un problema de visión por computador al que usaremos para comparar el rendimiento del método de selección AFL frente al convencional.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1909.12641.\n",
        ">\n",
        "> [2] http://yann.lecun.com/exdb/mnist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4BfI6bDrg3o",
        "outputId": "387707ab-5f40-4fc7-9e79-fbafef884e49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FLEXible is installed.\n"
          ]
        }
      ],
      "source": [
        "# install FLEXible framework if not installed\n",
        "try:\n",
        "    import flex\n",
        "    print(\"FLEXible is installed.\")\n",
        "except:\n",
        "    print(\"FLEXible is not installed.\\nInstalling dependency flexible-fl...\")\n",
        "    !pip install flexible-fl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5jdIu00Ftc1u",
        "outputId": "6fe1f84d-0053-477d-bb4a-ea1097fa6f8c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# select device\u001b[39;00m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# select device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CYOy-Bmsu0_J",
        "outputId": "1e2131d4-8e9e-46d7-ac33-34946d95c68c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fl9fRPPxTUxnC56ACzZ8JiLiew0SMFwt\n",
            "From (redirected): https://drive.google.com/uc?id=1fl9fRPPxTUxnC56ACzZ8JiLiew0SMFwt&confirm=t&uuid=93c643af-f5bb-4be3-8813-87bf80cfcd91\n",
            "To: /content/emnist-digits.mat\n",
            "100%|██████████| 90.7M/90.7M [00:01<00:00, 60.5MB/s]\n",
            "\u001b[36m[sultan]: md5 -q ./emnist-digits.mat;\u001b[0m\n",
            "DEBUG:sultan:md5 -q ./emnist-digits.mat;\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | NoneType: None\u001b[0m\n",
            "CRITICAL:sultan:| NoneType: None\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: --{ STDERR }-------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ STDERR }-------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | /bin/sh: 1: md5: not found\u001b[0m\n",
            "CRITICAL:sultan:| /bin/sh: 1: md5: not found\n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | Traceback (most recent call last):\u001b[0m\n",
            "CRITICAL:sultan:| Traceback (most recent call last):\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\n",
            "\u001b[01;31m[sultan]: |     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\u001b[0m\n",
            "CRITICAL:sultan:|     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\n",
            "\u001b[01;31m[sultan]: |     self.dump_exception()\u001b[0m\n",
            "CRITICAL:sultan:|     self.dump_exception()\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise self._exception\u001b[0m\n",
            "CRITICAL:sultan:|     raise self._exception\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\u001b[0m\n",
            "CRITICAL:sultan:|     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\n",
            "\u001b[01;31m[sultan]: | subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\u001b[0m\n",
            "CRITICAL:sultan:| subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[36m[sultan]: md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\u001b[0m\n",
            "DEBUG:sultan:md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\n",
            "\u001b[36m[sultan]: md5 -q ./emnist-digits.mat;\u001b[0m\n",
            "DEBUG:sultan:md5 -q ./emnist-digits.mat;\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | NoneType: None\u001b[0m\n",
            "CRITICAL:sultan:| NoneType: None\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: --{ STDERR }-------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ STDERR }-------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | /bin/sh: 1: md5: not found\u001b[0m\n",
            "CRITICAL:sultan:| /bin/sh: 1: md5: not found\n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | Traceback (most recent call last):\u001b[0m\n",
            "CRITICAL:sultan:| Traceback (most recent call last):\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\n",
            "\u001b[01;31m[sultan]: |     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\u001b[0m\n",
            "CRITICAL:sultan:|     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\n",
            "\u001b[01;31m[sultan]: |     self.dump_exception()\u001b[0m\n",
            "CRITICAL:sultan:|     self.dump_exception()\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise self._exception\u001b[0m\n",
            "CRITICAL:sultan:|     raise self._exception\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\u001b[0m\n",
            "CRITICAL:sultan:|     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\n",
            "\u001b[01;31m[sultan]: | subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\u001b[0m\n",
            "CRITICAL:sultan:| subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[36m[sultan]: md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\u001b[0m\n",
            "DEBUG:sultan:md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\n"
          ]
        }
      ],
      "source": [
        "from flex.datasets import load\n",
        "from torchvision import transforms\n",
        "\n",
        "flex_dataset, test_data = load(\"federated_emnist\", return_test=True, split=\"digits\")\n",
        "\n",
        "# Assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset[server_id] = test_data\n",
        "\n",
        "mnist_transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB9yluACv2rm"
      },
      "source": [
        "Usamos el decorador `@init_model_server` para inicializar el modelo en el servidor. Aprovechamos esta fase para simplemente definir nuestra arquitectura de red así como el optimizador y la función de pérdida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zAxWjwavQJ0",
        "outputId": "c2394095-6328-4d24-d4c9-60cbc4092d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes in the pool 3580: 1 server plus 3579 clients. The server is also an aggregator\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from flex.pool import init_server_model\n",
        "from flex.pool import FlexPool\n",
        "from flex.model import FlexModel\n",
        "\n",
        "\n",
        "# Simple two Fully-Connected layer net\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "@init_server_model\n",
        "def build_server_model():\n",
        "    server_flex_model = FlexModel()\n",
        "\n",
        "    server_flex_model[\"model\"] = SimpleNet()\n",
        "    # Required to store this for later stages of the FL training process\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
        "    return server_flex_model\n",
        "\n",
        "\n",
        "flex_pool = FlexPool.client_server_pool(\n",
        "    flex_dataset, server_id=server_id, init_func=build_server_model\n",
        ")\n",
        "\n",
        "clients = flex_pool.clients\n",
        "servers = flex_pool.servers\n",
        "aggregators = flex_pool.aggregators\n",
        "\n",
        "print(\n",
        "    f\"Number of nodes in the pool {len(flex_pool)}: {len(servers)} server plus {len(clients)} clients. The server is also an aggregator\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3LoDmezyvud"
      },
      "source": [
        "Para comparar nuestro modelo con selección AFL, vamos a primero construir un método de selección de clientes aleatorio uniforme, por ejemplo, de $20$ clientes por ronda. Más adelante, implementaremos la selección con AFL con $m=20$ clientes por ronda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFaDpYaWxbWm",
        "outputId": "0e0ad1f8-8185-4eca-b5bf-d4ad45643578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Server node is indentified by key \"server\"\n",
            "Selected 20 client nodes of a total of 3579\n"
          ]
        }
      ],
      "source": [
        "# Select clients\n",
        "clients_per_round = 20\n",
        "selected_clients_pool = clients.select(clients_per_round)\n",
        "selected_clients = selected_clients_pool.clients\n",
        "\n",
        "print(f'Server node is indentified by key \"{servers.actor_ids[0]}\"')\n",
        "print(\n",
        "    f\"Selected {len(selected_clients.actor_ids)} client nodes of a total of {len(clients.actor_ids)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5LzYqxUzIaU"
      },
      "source": [
        "Utilizamos el decorador `@deploy_server_model` para distribuir el modelo del servidor a los clientes. Usamos `map` sobre los clientes seleccionados: `selected_clients`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UwxXBCoCytZh"
      },
      "outputs": [],
      "source": [
        "from flex.pool import deploy_server_model\n",
        "import copy\n",
        "\n",
        "\n",
        "@deploy_server_model\n",
        "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
        "    return copy.deepcopy(server_flex_model)\n",
        "\n",
        "\n",
        "servers.map(copy_server_model_to_clients, selected_clients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1S01aEFzevA"
      },
      "source": [
        "Implementamos la función de training para el que cada cliente avanza un número $n$ de pasos del optimizador (En este caso SGD-Adam [1]) sobre un _batch_ de $b$ imágenes. En nuestro caso, utilizaremos los mismo parámetros que en el ejemplo de FLEX [2]: $n=5, b=20$.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1412.6980.\n",
        ">\n",
        "> [2] [Federated MNIST PT Example](https://github.com/FLEXible-FL/FLEXible/blob/main/notebooks/Federated%20MNIST%20PT%20example%20with%20flexible%20decorators.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UvjgKEJ7zdzd"
      },
      "outputs": [],
      "source": [
        "from flex.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
        "    train_dataset = client_data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    client_dataloader = DataLoader(train_dataset, batch_size=20)\n",
        "    model = client_flex_model[\"model\"]\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    model = model.train()\n",
        "    model = model.to(device)\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "    for _ in range(5):\n",
        "        for imgs, labels in client_dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(imgs)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "selected_clients.map(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMSabAvG0u2J"
      },
      "source": [
        "Con el decorador `@collect_clients_weights` recuperamos los pesos de PyTorch de cada cliente seleccionado para esa ronda. En el caso de PyTorch, el modelo devuelve los pesos en forma de un diccionario con `state_dict` para el que cada nombre representa una capa de la red y sus parámetros, lo que hacemos será devolver una lista con los valores de ese diccionario correspondientes a los pesos de la red entera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ggbUniMr0oGy"
      },
      "outputs": [],
      "source": [
        "from flex.pool import collect_clients_weights\n",
        "\n",
        "\n",
        "@collect_clients_weights\n",
        "def get_clients_weights(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    return [weight_dict[name] for name in weight_dict]\n",
        "\n",
        "\n",
        "aggregators.map(get_clients_weights, selected_clients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DT76cJ41uFY"
      },
      "source": [
        "Utilizamos el decorador `@aggregate_weights` para poder agregar los pesos que hemos recuperado de los clientes en la fase anterior computando la media de los pesos, conocido como agregador FedAvg, donde realizamos la media por columnas para cada capa de pesos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YR27tyd51cfq"
      },
      "outputs": [],
      "source": [
        "from flex.pool import aggregate_weights\n",
        "import tensorly as tl\n",
        "\n",
        "tl.set_backend(\"pytorch\")\n",
        "\n",
        "\n",
        "@aggregate_weights\n",
        "def aggregate_with_fedavg(list_of_weights: list):\n",
        "    agg_weights = []\n",
        "    for layer_index in range(len(list_of_weights[0])):\n",
        "        weights_per_layer = [weights[layer_index] for weights in list_of_weights]\n",
        "        weights_per_layer = tl.stack(weights_per_layer)\n",
        "        agg_layer = tl.mean(weights_per_layer, axis=0)\n",
        "        agg_weights.append(agg_layer)\n",
        "    return agg_weights\n",
        "\n",
        "\n",
        "# Aggregate weights\n",
        "aggregators.map(aggregate_with_fedavg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQtCzF0K2i92"
      },
      "source": [
        "Finalmente, agregamos los pesos al modelo de nuestro servidor/agregador. Sencillamente, para cada capa de nuestro modelo, realizamos una copia del nuevo que hemos agregado en la fase anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6Fk-VxE52ZNH"
      },
      "outputs": [],
      "source": [
        "from flex.pool import set_aggregated_weights\n",
        "\n",
        "\n",
        "@set_aggregated_weights\n",
        "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
        "    with torch.no_grad():\n",
        "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
        "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
        "            weight_dict[layer_key].copy_(new)\n",
        "\n",
        "\n",
        "aggregators.map(set_agreggated_weights_to_server, servers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB5N8y2a27JU"
      },
      "source": [
        "Podemos evaluar el modelo del servidor sobre el dataset de test que hemos definido anteriormente que residía en el mismo servidor. Para ello, definimos una función `evaluate_global_model` que obtenga las predicciones del modelo con el dataset de test y devuelva las metricas resultantes, que en este caso son simplemente la pérdida y la _accuracy_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFZA3_Sn20tn",
        "outputId": "509e607c-3afa-4a86-838f-4ebe3b2577a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss (test): 1.5552050596589495\n",
            "Accuracy (test): 0.51695\n"
          ]
        }
      ],
      "source": [
        "def evaluate_global_model(server_flex_model: FlexModel, test_data: Dataset):\n",
        "    model = server_flex_model[\"model\"]\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    total_count = 0\n",
        "    model = model.to(device)\n",
        "    criterion = server_flex_model[\"criterion\"]\n",
        "    # get test data as a torchvision object\n",
        "    test_dataset = test_data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=256, shuffle=True, pin_memory=False\n",
        "    )\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_dataloader:\n",
        "            total_count += target.size(0)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            losses.append(criterion(output, target).item())\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
        "\n",
        "    test_loss = sum(losses) / len(losses)\n",
        "    test_acc /= total_count\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "metrics = servers.map(evaluate_global_model)\n",
        "print(\"Loss (test):\", metrics[0][0])\n",
        "print(\"Accuracy (test):\", metrics[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPUTKW4d3gEd"
      },
      "outputs": [],
      "source": [
        "flex.datasets"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
