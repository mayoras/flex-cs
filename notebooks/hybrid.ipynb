{
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Selección de clientes HybridFL - Método basado en recursos\n",
        "\n",
        "En este notebook vamos a entrenar un modelo de ML utilizando el paradigma de Aprendizaje Federado [1] para los problemas de clasificación de dígitos manuscritos MNIST [2] y clasificación de imágenes a color CIFAR-10 [3]. Usaremos estos dos problemas de visión por computador para la implementación y análisis de rendimiento del método de selección de clientes basado en recursos HybridFL [4].\n",
        "> [1] https://arxiv.org/abs/1602.05629\n",
        ">\n",
        "> [2] http://yann.lecun.com/exdb/mnist\n",
        ">\n",
        "> [3] https://www.cs.toronto.edu/~kriz/cifar.html\n",
        ">\n",
        "> [4] https://arxiv.org/abs/1905.07210"
      ],
      "metadata": {
        "id": "QzFVvanFrrwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install FLEXible framework if not installed\n",
        "try:\n",
        "    import flex\n",
        "    print(\"FLEXible is installed.\")\n",
        "except:\n",
        "    print(\"FLEXible is not installed.\\nInstalling dependency flexible-fl...\")\n",
        "    !pip install flexible-fl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4BfI6bDrg3o",
        "outputId": "946e9a26-1405-456d-aa38-9048cd8a9c6f",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:30.656728Z",
          "iopub.execute_input": "2024-07-26T17:31:30.657909Z",
          "iopub.status.idle": "2024-07-26T17:31:30.668119Z",
          "shell.execute_reply.started": "2024-07-26T17:31:30.657852Z",
          "shell.execute_reply": "2024-07-26T17:31:30.666366Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLEXible is not installed.\n",
            "Installing dependency flexible-fl...\n",
            "Collecting flexible-fl\n",
            "  Downloading flexible_fl-0.6.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.25.2)\n",
            "Collecting multiprocess (from flexible-fl)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.3.2)\n",
            "Collecting cardinality (from flexible-fl)\n",
            "  Downloading cardinality-0.1.1.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sultan (from flexible-fl)\n",
            "  Downloading sultan-0.9.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (4.66.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.13.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (5.1.0)\n",
            "Collecting tensorly (from flexible-fl)\n",
            "  Downloading tensorly-0.8.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (2.31.0)\n",
            "Collecting dill>=0.3.8 (from multiprocess->flexible-fl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->flexible-fl) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->flexible-fl) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->flexible-fl) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (1.7.1)\n",
            "Downloading flexible_fl-0.6.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sultan-0.9.1-py3-none-any.whl (16 kB)\n",
            "Downloading tensorly-0.8.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cardinality\n",
            "  Building wheel for cardinality (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cardinality: filename=cardinality-0.1.1-py3-none-any.whl size=2588 sha256=2c7cab96a635271aaf182d67cb6fde9f0bde58aa43fd78717099aebb21884be6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/19/d1/2665c004b583a7d1880fa59055a3e462d6e35841a01b57010b\n",
            "Successfully built cardinality\n",
            "Installing collected packages: sultan, cardinality, dill, tensorly, multiprocess, flexible-fl\n",
            "Successfully installed cardinality-0.1.1 dill-0.3.8 flexible-fl-0.6.1 multiprocess-0.70.16 sultan-0.9.1 tensorly-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_accuracy(loss, accuracy, title=\"Learning Curves\"):\n",
        "    # Example data\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, 'b', label='Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, 'g', label='Accuracy')\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "\n",
        "    # Show the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-31n0URu8tVs",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:30.670537Z",
          "iopub.execute_input": "2024-07-26T17:31:30.671158Z",
          "iopub.status.idle": "2024-07-26T17:31:30.682460Z",
          "shell.execute_reply.started": "2024-07-26T17:31:30.671104Z",
          "shell.execute_reply": "2024-07-26T17:31:30.680979Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# select device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5jdIu00Ftc1u",
        "outputId": "f171e8f4-687e-4956-8e80-b7633a7c28ba",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:30.684265Z",
          "iopub.execute_input": "2024-07-26T17:31:30.684752Z",
          "iopub.status.idle": "2024-07-26T17:31:30.702373Z",
          "shell.execute_reply.started": "2024-07-26T17:31:30.684708Z",
          "shell.execute_reply": "2024-07-26T17:31:30.700905Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar datasets - MNIST, CIFAR-10\n",
        "\n",
        "Cargaremos $2$ de los datasets que usaremos para entrenar el modelo de ML."
      ],
      "metadata": {
        "id": "9UZfbqtUDaFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST\n",
        "El primero será MNIST (Modified National Institute of Standards and Technology database), que consiste en imágenes de $28$ pixeles de ancho y alto en escala de grises de dígitos manuscritos del $0$ al $9$. Formalmente, definimos el problema de ML como un problema de clasificación multiclase, donde definimos una imagen de entrada $X_n \\in \\mathcal X$ se define como una matriz $X_{28\\times 28}, x_i \\in \\{1, ..., 255\\}$, donde $x_i$ es un pixel de la imagen $X$. Cada imágen de entrada $X_n\\in \\mathcal X$ es etiquetada con una clase $y_n \\in \\mathcal Y, y = \\{0, 1, ..., 9\\}$ que representa el dígito al que corresponde la imagen. El conjunto de entrenamiento consta de $N=240,000$ imágenes.\n",
        "\n",
        "El dataset descargado será el de _Extended MNIST_ [1], que comprende una versión extendida del concepto original de MNIST para proporcionar dígitos y letras manuscritas, una cantidad más grande de datos, diferentes formas de separar los datos (solo dígitos, letras, por clase, ...), etc. Nosotros solo utilizaremos los dígitos para entrenar el clásico MNIST, del cual su versión extendida consta de $N = 280,000$ imágenes de dígitos manuscritos.\n",
        "\n",
        "Una característica importante de EMNIST es que cada imagen tendrá asociado un autor (i.e. la persona que ha escrito el dígito o letra), tal que un dígito es escrito por un solo autor pero un autor puede escribir muchos dígitos. Esto nos da la ventaja de poder distribuir los datos de forma No-IID tal que cada cliente tendrá los datos con autoría de un solo autor.\n",
        "\n",
        "FLEX nos da utilidades para cargar EMNIST y federarlo automáticamente con la función `load`, donde además descargamos el dataset de test y solo usaremos el dataset de dígitos clásico de MNIST. Definimos también las transformaciones a realizar a los datos que son simplemente normalizar cada pixel a valores de $[0,1]$ (función `ToTensor()`) y normalizar los valores de cada pixel con una media y desviación estándar de $0.5$.\n",
        "\n",
        "> [1] https://www.nist.gov/itl/products-and-services/emnist-dataset"
      ],
      "metadata": {
        "id": "kIIPVTjuSl00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.datasets import load\n",
        "from flex.datasets.standard_datasets import emnist\n",
        "from torchvision import transforms\n",
        "\n",
        "flex_dataset_mnist, test_data = load(\"federated_emnist\", return_test=True, split=\"digits\")\n",
        "\n",
        "# Assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_mnist[server_id] = test_data\n",
        "\n",
        "# apply transforms\n",
        "mnist_transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYOy-Bmsu0_J",
        "outputId": "c823f069-08d4-489a-8743-9cac013103f7",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:30.704144Z",
          "iopub.execute_input": "2024-07-26T17:31:30.704576Z",
          "iopub.status.idle": "2024-07-26T17:31:36.568627Z",
          "shell.execute_reply.started": "2024-07-26T17:31:30.704528Z",
          "shell.execute_reply": "2024-07-26T17:31:36.567531Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fl9fRPPxTUxnC56ACzZ8JiLiew0SMFwt\n",
            "From (redirected): https://drive.google.com/uc?id=1fl9fRPPxTUxnC56ACzZ8JiLiew0SMFwt&confirm=t&uuid=964b12eb-6cc6-46bb-8183-f09bb6dc7e7e\n",
            "To: /content/emnist-digits.mat\n",
            "100%|██████████| 90.7M/90.7M [00:03<00:00, 27.9MB/s]\n",
            "\u001b[36m[sultan]: md5 -q ./emnist-digits.mat;\u001b[0m\n",
            "DEBUG:sultan:md5 -q ./emnist-digits.mat;\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | NoneType: None\u001b[0m\n",
            "CRITICAL:sultan:| NoneType: None\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: --{ STDERR }-------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ STDERR }-------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | /bin/sh: 1: md5: not found\u001b[0m\n",
            "CRITICAL:sultan:| /bin/sh: 1: md5: not found\n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | Traceback (most recent call last):\u001b[0m\n",
            "CRITICAL:sultan:| Traceback (most recent call last):\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\n",
            "\u001b[01;31m[sultan]: |     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\u001b[0m\n",
            "CRITICAL:sultan:|     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\n",
            "\u001b[01;31m[sultan]: |     self.dump_exception()\u001b[0m\n",
            "CRITICAL:sultan:|     self.dump_exception()\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise self._exception\u001b[0m\n",
            "CRITICAL:sultan:|     raise self._exception\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\u001b[0m\n",
            "CRITICAL:sultan:|     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\n",
            "\u001b[01;31m[sultan]: | subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\u001b[0m\n",
            "CRITICAL:sultan:| subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[36m[sultan]: md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\u001b[0m\n",
            "DEBUG:sultan:md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\n",
            "\u001b[36m[sultan]: md5 -q ./emnist-digits.mat;\u001b[0m\n",
            "DEBUG:sultan:md5 -q ./emnist-digits.mat;\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | NoneType: None\u001b[0m\n",
            "CRITICAL:sultan:| NoneType: None\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: --{ STDERR }-------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ STDERR }-------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | /bin/sh: 1: md5: not found\u001b[0m\n",
            "CRITICAL:sultan:| /bin/sh: 1: md5: not found\n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | Traceback (most recent call last):\u001b[0m\n",
            "CRITICAL:sultan:| Traceback (most recent call last):\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\n",
            "\u001b[01;31m[sultan]: |     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\u001b[0m\n",
            "CRITICAL:sultan:|     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\n",
            "\u001b[01;31m[sultan]: |     self.dump_exception()\u001b[0m\n",
            "CRITICAL:sultan:|     self.dump_exception()\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise self._exception\u001b[0m\n",
            "CRITICAL:sultan:|     raise self._exception\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\u001b[0m\n",
            "CRITICAL:sultan:|     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\n",
            "\u001b[01;31m[sultan]: | subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\u001b[0m\n",
            "CRITICAL:sultan:| subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[36m[sultan]: md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\u001b[0m\n",
            "DEBUG:sultan:md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10\n",
        "\n",
        "El segundo dataset es CIFAR-10 (_Canadian Institute for Advanced Research_)[1], el cual es otro de los datasets junto con MNIST, más utilizados en el campo del Deep Learning y Visión por Computador. CIFAR-10 consiste en una colección de imágenes de $32$ pixeles de altura y de ancho a color ($3$ canales RGB) representando $10$ objetos reales:\n",
        "- Avión (airplane)\n",
        "- Automóvil (automobile)\n",
        "- Pájaro (bird)\n",
        "- Gato (cat)\n",
        "- Ciervo (deer)\n",
        "- Perro (dog)\n",
        "- Rana (frog)\n",
        "- Caballo (horse)\n",
        "- Barco (ship)\n",
        "- Camión (truck)\n",
        "\n",
        "Formalmente, definimos el problema de ML como un problema de clasificación multiclase, donde definimos una imagen de entrada $X_n \\in \\mathcal X$ se define como una matriz $X_{32\\times 32 \\times 3}, x_i \\in \\{1, ..., 255\\}$, donde $x_i^c$ es un pixel de la imagen $X$ en el canal $c$. Cada imágen de entrada $X_n\\in \\mathcal X$ es etiquetada con una clase $y_n \\in \\mathcal Y, y = \\{0, 1, ..., 9\\}$ tal que indexa la lista de clases $C=(c_0=\\text{plane}, c_1=\\text{automobile}, ..., c_9=\\text{truck})$. El conjunto de entrenamiento consta de $N=50,000$ datos de entrada.\n",
        "\n",
        "A diferencia de EMNIST, FLEX no carga por defecto CIFAR-10, por lo que deberemos de descargar y federar manualmente el dataset. Utilizaremos el dataset descargado desde `torchvision` de PyTorch y federamos utilizando una configuración por defecto de FLEX con un número de nodos o clientes de $K = 1000$ clientes. Definimos también las transformaciones para escalar los valores a $[0,1]$ y normalizar con medias y desviaciones estándar específicas de CIFAR-10 [2].\n",
        "\n",
        "> [1] https://www.cs.toronto.edu/~kriz/cifar.html\n",
        ">\n",
        "> [2] https://github.com/kuangliu/pytorch-cifar/issues/19"
      ],
      "metadata": {
        "id": "CW6KW3xiYW3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from flex.data import FedDataDistribution, FedDatasetConfig, Dataset\n",
        "\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=\".\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=None,  # we apply them later in training process\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\".\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=None,  # we apply them later in training process\n",
        ")\n",
        "\n",
        "# CIFAR-10 labels\n",
        "CIFAR_LABELS = list(set(train_data.targets))\n",
        "print(f\"CIFAR-10 labels: {CIFAR_LABELS}\")\n",
        "\n",
        "config = FedDatasetConfig(seed=33)\n",
        "config.replacement = False\n",
        "config.n_nodes = 1000\n",
        "\n",
        "# create Federated data distribution of CIFAR-10\n",
        "flex_dataset_cifar = FedDataDistribution.from_config(\n",
        "    centralized_data=Dataset.from_torchvision_dataset(train_data),\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_cifar[server_id] = Dataset.from_torchvision_dataset(test_data)\n",
        "\n",
        "# apply transforms\n",
        "cifar_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalizar con las medias y desviaciones estándar específicas de CIFAR-10\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErtVS1zPd-Zb",
        "outputId": "87e96ec0-fd3d-4cb4-9084-d3271958ad64",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:36.572018Z",
          "iopub.execute_input": "2024-07-26T17:31:36.572435Z",
          "iopub.status.idle": "2024-07-26T17:31:45.277535Z",
          "shell.execute_reply.started": "2024-07-26T17:31:36.572401Z",
          "shell.execute_reply": "2024-07-26T17:31:45.276301Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 90420582.12it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n",
            "CIFAR-10 labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos de aprendizaje\n",
        "\n",
        "Definimos dos modelos de Machine Learning, uno para cada uno de los dos datasets que hemos definido anteriormente. Para ambos problemas, usaremos como función de pérdida o _criterion_ la función de entropía cruzada o _Cross Entropy Loss_ cuya definición [1] la define como (para una muestra $n$):\n",
        "\n",
        "$$l_n = -w_{y_n} \\cdot \\log \\frac{\\exp (x_n, y_n)}{\\sum_{c=0}^C \\exp (x_n, c)} =-w_{y_n}\\cdot \\log (\\text{Softmax} (x_n, y_n))$$\n",
        "\n",
        "En ambos problemas utilizaremos Adam [2] como optimizador o algoritmo de aprendizaje. Utilizaremos los mismos parámetros que en [2] al haber hecho los experimentos sobre modelos similares y con los mismos datasets que estamos usando y que han demostrando buenos resultados: $\\eta = 0.01$ y $\\beta_1 = 0.9,\\beta_2 = 0.999$. Estos parámetros están definidos por defecto en la librería de PyTorch [3].\n",
        "\n",
        "> [1] https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        ">\n",
        "> [2] https://arxiv.org/abs/1412.6980\n",
        ">\n",
        "> [3] https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJLc-rB4gfzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptrón Multicapa (MNIST)\n",
        "\n",
        "El primero, que lo usaremos para ajustar MNIST, será un Perceptrón Multicapa (MLP) de dos capas ocultas de $128$ parámetros la primera y $10$ la segunda (correspondiente al número de clases), y usando ReLU [1] como función de activación.\n",
        "\n",
        "El modelo inicialmente lo tendrá el servidor y será copiado a cada uno de los clientes. En FLEX usamos el decorador `@init_model_server` para inicializar el modelo en el servidor, donde también podemos además de nuestra arquitectura del modelo, el optimizador y la función de pérdida a usar.\n",
        "\n",
        "> [1] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
      ],
      "metadata": {
        "id": "vYPizwuhm28X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer Perceptron classifier with two hidden layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features: Tuple[int, int], hidden_features: int, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "\n",
        "        width, height = in_features\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(width * height, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wro1kxtkw1w0",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.279203Z",
          "iopub.execute_input": "2024-07-26T17:31:45.279670Z",
          "iopub.status.idle": "2024-07-26T17:31:45.289624Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.279627Z",
          "shell.execute_reply": "2024-07-26T17:31:45.288132Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Red Neuronal Convolucional (CNN) - CIFAR-10\n",
        "\n",
        "Para nuestro segundo problema, que consta de ajustar un modelo para clasificar CIFAR-10, utilizaremos un red neuronal convolucional o CNN. A diferencia de MNIST, CIFAR-10 tiene unos datos de entrada con una dimensionalidad más compleja, por lo que utilizar una CNN reduciría la cantidad de parámetros necesarios a entrenar.\n",
        "\n",
        "Usaremos la red neuronal convolucional utilizada en los experimentos de el algoritmo GreedyFed [1], debido a que es una arquitectura con pocos parámetros a entrenar, lo que nos permite obtener los resultados deseados en poco tiempo. Concretamente, la arquitectura de la red a implementar será la de una CNN estándar que comprende dos capas convolucionales (CONV) $4\\times 4$:\n",
        "- Ambas con $8$ canales de salida.\n",
        "- Cada uno de ellos activados por ReLU [2].\n",
        "- Y aplicando _Max Pooling_ $2\\times 2$ con _stride_ de $2$ en la salida de cada capa convolucional para reducir la dimensionalidad del los mapas de activación.\n",
        "\n",
        "Seguido de $1$ capa _Fully-Connected_ de $10$ unidades activada por Softmax para la capa de final de salida.\n",
        "\n",
        "> [1] https://arxiv.org/abs/2312.09108\n",
        ">\n",
        "> [2] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
      ],
      "metadata": {
        "id": "-AOdn5HT0iNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolucional Neural Net classifier for CIFAR-10 image recognition (32x32 images).\n",
        "\n",
        "    Model architecture based on GreedyFed experimental setting:\n",
        "    - https://github.com/pringlesinghal/GreedyFed/blob/b2d928670bcd014035830c531bea9f76c57b4b70/model.py#L46\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, in_width: int = 32, in_height: int = 32, output_dim: int = 10):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.input_w = in_width\n",
        "        self.input_h = in_height\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # First CONV layer\n",
        "        out_channels_conv_1 = 8\n",
        "        kernel_conv_1_size = 4\n",
        "        padding_conv_1 = 1\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels=out_channels_conv_1,\n",
        "            kernel_size=kernel_conv_1_size,\n",
        "            padding=padding_conv_1,\n",
        "        )\n",
        "\n",
        "        output_w_1, output_h_1 = self.__conv_output_dims(\n",
        "            self.input_w, self.input_h, kernel_conv_1_size, padding_conv_1\n",
        "        )\n",
        "\n",
        "        # First MAXPOOL layer\n",
        "        kernel_pool_size = 2\n",
        "        stride_pool = 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=kernel_pool_size, stride=stride_pool)\n",
        "\n",
        "        output_w_2, output_h_2 = self.__pool_output_dims(\n",
        "            output_w_1, output_h_1, kernel_pool_size, stride_pool\n",
        "        )\n",
        "\n",
        "        # Second CONV layer\n",
        "        in_channels_conv_2 = out_channels_conv_1\n",
        "        out_channels_conv_2 = 8\n",
        "        kernel_conv_2_size = 4\n",
        "        padding_conv_2 = 1\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels_conv_2,\n",
        "            out_channels=out_channels_conv_2,\n",
        "            kernel_size=kernel_conv_2_size,\n",
        "            padding=padding_conv_2\n",
        "        )\n",
        "\n",
        "        output_w_3, output_h_3 = self.__conv_output_dims(\n",
        "            output_w_2, output_h_2, kernel_conv_2_size, padding_conv_2\n",
        "        )\n",
        "\n",
        "        # after second MAXPOOL layer, compute final out width and height\n",
        "        output_w_4, output_h_4 = self.__pool_output_dims(\n",
        "            output_w_3, output_h_3, kernel_pool_size, stride_pool\n",
        "        )\n",
        "\n",
        "        # Fully-Connected layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        input_size_4 = int(output_w_4 * output_h_4 * out_channels_conv_2)\n",
        "        self.fc = nn.Linear(input_size_4, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        # x = self.flatten(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def __conv_output_dims(self, width: int, height: int, kernel_size: int, padding: int = 1):\n",
        "        output_w = width - kernel_size + 2 * padding + 1\n",
        "        output_h = height - kernel_size + 2 * padding + 1\n",
        "        return output_w, output_h\n",
        "\n",
        "    def __pool_output_dims(self, width: int, height: int, kernel_size: int, stride: int = 1):\n",
        "        output_w = np.floor((width - kernel_size) / stride) + 1\n",
        "        output_h = np.floor((height - kernel_size) / stride) + 1\n",
        "\n",
        "        return output_w, output_h"
      ],
      "metadata": {
        "id": "5iHpTXr6CbRw",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.291164Z",
          "iopub.execute_input": "2024-07-26T17:31:45.291583Z",
          "iopub.status.idle": "2024-07-26T17:31:45.311817Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.291551Z",
          "shell.execute_reply": "2024-07-26T17:31:45.310403Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración básica de escenario de FL\n",
        "\n",
        "Configuraremos un escenario de aprendizaje federado centralizado (CFL) usando la librería FLEXible. FLEXible [1], o simplemente FLEX, es una librería de Python que proporciona un framework para la construcción de entornos de aprendizaje federado para fines de investigación y simulación. FLEX pretende dar flexibilidad en cuanto la gran variedad de escenarios y necesidades que se pueden llegar a plantear para experimentar en entornos federados.\n",
        "\n",
        "Uno de nuestros objetivos es _integrar_ un método o técnica de selección de clientes en un escenario inicial de FL. La flexibilidad de FLEX nos permitirá conseguir este objetivo gracias a su flujo de mensajes entre entidades separadas por roles, y por la arquitectura modular de estas que nos permiten almacenar información de manera conveniente [1].\n",
        "\n",
        "> [1] https://arxiv.org/abs/2404.06127\n",
        "---"
      ],
      "metadata": {
        "id": "C3LoDmezyvud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicialización del modelo del servidor\n",
        "\n",
        "El primer paso de nuestro bucle de entrenamiento en aprendizaje federado es inicializar el modelo FLEX del servidor (que posteriormente se distribuirá a los clientes).\n",
        "\n",
        "Con FLEX podemos hacer uso del decorador `init_server_model` para facilitarnos esta tarea. Esta función entonces debe de instanciar y devolver un modelo de FLEX en donde además definimos el modelo de aprendizaje $^1$, la función de pérdida, el optimizador y cualquier otra información que se usará en las demás fases de la ronda de entrenamiento.\n",
        "\n",
        "Definimos dos funciones que realizan esta tarea, dos para cada una de nuestras arquitecturas, el Perceptrón Multicapa y la Red Neuronal Convolucional. Como habíamos especificado en el planteamiento del problema de ML a resolver, utilizaremos el optimizador SGD-Adam con sus parámetros por defecto, y la Cross-Entropy como función de pérdida.\n",
        "\n",
        "> $^1$ No se debe confundir el modelo de tipo `FlexModel` con un modelo de ML. El primero implementa el bloque fundamental que define una entidad en el escenario de FL (donde almacenamos información, sus datos locales, su modelo de aprendizaje, ...). El segundo es un modelo aprendizaje como lo puede ser un módulo de PyTorch o Tensorflow que realiza el aprendizaje automático y la predicción sobre los datos. Se dejará claro la diferencia entre los dos tipos de modelos.\n"
      ],
      "metadata": {
        "id": "wOLo8WYFBnwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from flex.pool import init_server_model\n",
        "from flex.pool import FlexPool\n",
        "from flex.model import FlexModel\n",
        "\n",
        "mnist_in_features = (28, 28)\n",
        "mnist_hidden_features = 128\n",
        "\n",
        "@init_server_model\n",
        "def build_server_model_mlp():\n",
        "    server_flex_model = FlexModel()\n",
        "\n",
        "    server_flex_model[\"model\"] = MLP(mnist_in_features, mnist_hidden_features)\n",
        "\n",
        "    # Required to store this for later stages of the FL training process\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
        "    return server_flex_model\n",
        "\n",
        "cifar_in_channels = 3\n",
        "cifar_num_classes = 10\n",
        "\n",
        "@init_server_model\n",
        "def build_server_model_cnn():\n",
        "    server_flex_model = FlexModel()\n",
        "\n",
        "    server_flex_model[\"model\"] = CNN(in_channels=cifar_in_channels, output_dim=cifar_num_classes)\n",
        "\n",
        "    # Required to store this for later stages of the FL training process\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
        "    return server_flex_model"
      ],
      "metadata": {
        "id": "3zAxWjwavQJ0",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.313473Z",
          "iopub.execute_input": "2024-07-26T17:31:45.313934Z",
          "iopub.status.idle": "2024-07-26T17:31:45.328356Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.313892Z",
          "shell.execute_reply": "2024-07-26T17:31:45.327262Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Distribuir modelo del servidor\n",
        "\n",
        "El siguiente paso a realizar en un flujo de entrenamiento de aprendizaje federado es la distribución del modelo del servidor a los cientes. Con FLEX, podemos utilizar el decorador `@deploy_server_model` para distribuir el modelo del servidor a los clientes, definiendo una función que devuelva el modelo a almacenar en cada cliente.\n",
        "\n",
        "En este caso, realizamos una copia profunda del modelo del servidor que será asignado a cada cliente.\n"
      ],
      "metadata": {
        "id": "R5LzYqxUzIaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.pool import deploy_server_model\n",
        "import copy\n",
        "\n",
        "\n",
        "@deploy_server_model\n",
        "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
        "    print(server_flex_model.keys())\n",
        "    return copy.deepcopy(server_flex_model)"
      ],
      "metadata": {
        "id": "UwxXBCoCytZh",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.329927Z",
          "iopub.execute_input": "2024-07-26T17:31:45.330688Z",
          "iopub.status.idle": "2024-07-26T17:31:45.343750Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.330647Z",
          "shell.execute_reply": "2024-07-26T17:31:45.342617Z"
        },
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actualización del modelo del lado del cliente\n",
        "\n",
        "Definimos la función encargada de realizar el entrenamiento del modelo sobre los datos locales del cliente. A esta función, es conveniente pasarle como parámetros los hiperparámetros de entrenamiento de un modelo de ML convencional como el número de épocas $E$ y el tamaño de _batch_ $B$"
      ],
      "metadata": {
        "id": "U1S01aEFzevA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train(client_flex_model: FlexModel, client_data: Dataset, **kwargs):\n",
        "    # parse kwargs\n",
        "    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 5\n",
        "    batch_size = kwargs[\"batch_size\"] if \"batch_size\" in kwargs else 20\n",
        "    transforms = kwargs[\"transforms\"] if \"transforms\" in kwargs else None\n",
        "\n",
        "    # get client data as a torchvision object\n",
        "    train_dataset = client_data.to_torchvision_dataset(transform=transforms)\n",
        "    client_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    # get model\n",
        "    model = client_flex_model[\"model\"]\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    model = model.train()\n",
        "    model = model.to(device)\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "\n",
        "    # train model\n",
        "    for _ in range(epochs):\n",
        "        for imgs, labels in client_dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(imgs)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "id": "UvjgKEJ7zdzd",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.345360Z",
          "iopub.execute_input": "2024-07-26T17:31:45.345819Z",
          "iopub.status.idle": "2024-07-26T17:31:45.358403Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.345776Z",
          "shell.execute_reply": "2024-07-26T17:31:45.357210Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtener los parámetros de los clientes\n",
        "\n",
        "Ahora implementamos la función que hace que el servidor (con rol de agregador) recupere los nuevos parámetros actualizados de los clientes. Con el decorador `@collect_clients_weights` recuperamos los pesos de PyTorch de cada cliente seleccionado para esa ronda. En el caso de PyTorch, el modelo devuelve los pesos en forma de un diccionario con `state_dict` para el que cada nombre representa una capa de la red y sus parámetros, lo que hacemos será devolver una lista con los valores de ese diccionario correspondientes a los pesos de la red entera."
      ],
      "metadata": {
        "id": "dMSabAvG0u2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.pool import collect_clients_weights\n",
        "\n",
        "\n",
        "@collect_clients_weights\n",
        "def get_clients_weights(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    return [weight_dict[name] for name in weight_dict]"
      ],
      "metadata": {
        "id": "ggbUniMr0oGy",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.360233Z",
          "iopub.execute_input": "2024-07-26T17:31:45.360933Z",
          "iopub.status.idle": "2024-07-26T17:31:45.375819Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.360851Z",
          "shell.execute_reply": "2024-07-26T17:31:45.374504Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agregación de los parámetros\n",
        "\n",
        "El servidor/agregador agrega entonces estos nuevos parámetros para conseguir el nuevo modelo global. Utilizamos el decorador `@aggregate_weights` para poder agregar los pesos que hemos recuperado de los clientes en la fase anterior computando la media de los pesos de manera uniforme, conocido como agregador FedAvg [1], donde realizamos la media por columnas para cada capa de pesos.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1602.05629"
      ],
      "metadata": {
        "id": "1DT76cJ41uFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.pool import aggregate_weights\n",
        "import tensorly as tl\n",
        "\n",
        "tl.set_backend(\"pytorch\")\n",
        "\n",
        "\n",
        "@aggregate_weights\n",
        "def aggregate_with_fedavg(list_of_weights: list):\n",
        "    agg_weights = []\n",
        "    for layer_index in range(len(list_of_weights[0])):\n",
        "        weights_per_layer = [weights[layer_index] for weights in list_of_weights]\n",
        "        weights_per_layer = tl.stack(weights_per_layer)\n",
        "        agg_layer = tl.mean(weights_per_layer, axis=0)\n",
        "        agg_weights.append(agg_layer)\n",
        "    return agg_weights"
      ],
      "metadata": {
        "id": "YR27tyd51cfq",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.377513Z",
          "iopub.execute_input": "2024-07-26T17:31:45.377851Z",
          "iopub.status.idle": "2024-07-26T17:31:45.391035Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.377822Z",
          "shell.execute_reply": "2024-07-26T17:31:45.389824Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, agregamos los pesos al modelo de nuestro servidor/agregador. Sencillamente, para cada capa de nuestro modelo, realizamo una copia del nuevo que hemos agregado en la fase anterior."
      ],
      "metadata": {
        "id": "YQtCzF0K2i92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.pool import set_aggregated_weights\n",
        "\n",
        "\n",
        "@set_aggregated_weights\n",
        "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
        "    with torch.no_grad():\n",
        "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
        "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
        "            weight_dict[layer_key].copy_(new)"
      ],
      "metadata": {
        "id": "6Fk-VxE52ZNH",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.396908Z",
          "iopub.execute_input": "2024-07-26T17:31:45.397947Z",
          "iopub.status.idle": "2024-07-26T17:31:45.404766Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.397909Z",
          "shell.execute_reply": "2024-07-26T17:31:45.403621Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación del modelo global\n",
        "\n",
        "Podemos evaluar el modelo del servidor sobre el dataset de test que hemos definido anteriormente que residía en el mismo servidor. Para ello, definimos una función `evaluate_global_model` que obtenga las predicciones del modelo con el dataset de test y devuelva las metricas resultantes, que en este caso son simplemente la pérdida y la _accuracy_."
      ],
      "metadata": {
        "id": "NB5N8y2a27JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_global_model(server_flex_model: FlexModel, test_data: Dataset):\n",
        "    model = server_flex_model[\"model\"]\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    total_count = 0\n",
        "    model = model.to(device)\n",
        "    criterion = server_flex_model[\"criterion\"]\n",
        "    # get test data as a torchvision object\n",
        "    test_dataset = test_data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=256, shuffle=True, pin_memory=False\n",
        "    )\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_dataloader:\n",
        "            total_count += target.size(0)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            losses.append(criterion(output, target).item())\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
        "\n",
        "    test_loss = sum(losses) / len(losses)\n",
        "    test_acc /= total_count\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "AFZA3_Sn20tn",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.406151Z",
          "iopub.execute_input": "2024-07-26T17:31:45.406487Z",
          "iopub.status.idle": "2024-07-26T17:31:45.421410Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.406459Z",
          "shell.execute_reply": "2024-07-26T17:31:45.420270Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "\n",
        "A continuación se muestra el bucle de entrenamiento aplicando todas las fases que hemos implementado anteriormente. En esta versión básica se implementa una selección aleatoria (RandomSampling), similar a la propuesta en [1] donde se selecciona de manera aleatoria y uniforme $M$ clientes para la ronda actual. Este proceso se repite de forma iterativa un número determinado de rondas."
      ],
      "metadata": {
        "id": "4p233PYL9Lum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "Problem = Literal[\"mnist\", \"cifar\"]\n",
        "\n",
        "def train_n_rounds(pool: FlexPool, n_rounds: int, clients_per_round=20, problem: Problem = \"mnist\"):\n",
        "    \"\"\"\n",
        "    FL training loop for a certain number of rounds and clients selected.\n",
        "    \"\"\"\n",
        "    # select transformations depending on problem to solve\n",
        "    transforms = mnist_transforms if problem == \"mnist\" else cifar_transforms\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    for i in range(n_rounds):\n",
        "        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n",
        "        selected_clients_pool = pool.clients.select(clients_per_round)\n",
        "        selected_clients = selected_clients_pool.clients\n",
        "        print(f\"Selected clients for this round: {len(selected_clients)}\")\n",
        "        # Deploy the server model to the selected clients\n",
        "        pool.servers.map(copy_server_model_to_clients, selected_clients)\n",
        "        # Each selected client trains her model\n",
        "        selected_clients.map(train, transforms=transforms)\n",
        "        # The aggregador collects weights from the selected clients and aggregates them\n",
        "        pool.aggregators.map(get_clients_weights, selected_clients)\n",
        "        pool.aggregators.map(aggregate_with_fedavg)\n",
        "        # The aggregator send its aggregated weights to the server\n",
        "        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n",
        "        metrics = pool.servers.map(evaluate_global_model)\n",
        "        loss, acc = metrics[0]\n",
        "        losses.append(loss)\n",
        "        accuracies.append(acc)\n",
        "        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}\")\n",
        "\n",
        "    return losses, accuracies"
      ],
      "metadata": {
        "id": "OK5TLa219FLP",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.422725Z",
          "iopub.execute_input": "2024-07-26T17:31:45.423091Z",
          "iopub.status.idle": "2024-07-26T17:31:45.435967Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.423060Z",
          "shell.execute_reply": "2024-07-26T17:31:45.434852Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selección de clientes con HybridFL\n",
        "\n",
        "Ahora vamos a adaptar nuestro entorno de aprendizaje federado para utilizar el método de selección de clientes HybridFL [1].\n",
        "\n",
        "HybridFL, así como otros algoritmos basados en recursos como FedCS [2] (del cual éste y muchos otros se basan), necesitan ser testeados bajo un entorno experimental que simule de forma realista y adecuada los requisitos de tiempo de reloj de cada una de las fases en aprendizaje federado. Tanto HybridFL como FedCS simulan cada uno de estos tiempos en un entorno dedicado de _Mobile Edge Computing_ (MEC) en el que hay un servidor central, denominado _Parameter Server_ o PS, una estación base o BS (que sirve de servidor en el eje o _edge server_), y una red de dispositivos móviles. En nuestro caso, podemos simular este entorno simplemente haciendo que las fases de entrenamiento como de subida de datos sean \"agendadas\" (_scheduled_), de manera que cumplan un cierto tiempo de reloj determinado antes de completarse. De esta manera, se verá reflejado el impacto que tendrá la estrategia de selección mediante la estimación de estos recursos.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WcHHP4AZVcNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federar MNIST manualmente\n",
        "\n",
        "En primer lugar, vamos a federar de forma manual el dataset de MNIST, del que recordamos que una forma de federar este dataset es mediante la separación por _autores_ de los ejemplos (i.e. cada persona que escribió los ejemplos), y de esta separación obtenemos el __número de clientes__ del que sale un total de $>3000$ clientes. Sin embargo, en nuestro caso usaremos un número de clientes más bajo, en concreto $K=1000$, ya que existen fases de la simulación que serán temporizadas en tiempos mayores de los normales, en especial las fases de actualización y subida del modelo agendadas, esto por los motivos previamente explicados. Para poder cambiar este parámetro es necesario federar manualmente nuestro dataset justo como hemos hecho con CIFAR-10 en la sección `Cargar datasets - MNIST, CIFAR-10`."
      ],
      "metadata": {
        "id": "zDvpb0tDE9fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from flex.data import Dataset, FedDatasetConfig, FedDataDistribution\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root='.',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=None  # we apply them later in training process\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root='.',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=None  # we apply them later in training process\n",
        ")\n",
        "\n",
        "MNIST_LABELS = torch.unique(train_data.targets).tolist()\n",
        "print(f\"MNIST Labels: {MNIST_LABELS}\")\n",
        "\n",
        "config = FedDatasetConfig(seed=33)\n",
        "config.replacement = False\n",
        "config.n_nodes = 1000\n",
        "\n",
        "flex_dataset_mnist = FedDataDistribution.from_config(\n",
        "    centralized_data=Dataset.from_torchvision_dataset(train_data), config=config\n",
        ")\n",
        "\n",
        "# assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_mnist[server_id] = Dataset.from_torchvision_dataset(test_data)"
      ],
      "metadata": {
        "id": "Ga_F2xKqpMY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d41574c-21b4-4373-ebd4-b94927b3b3d3",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:45.437562Z",
          "iopub.execute_input": "2024-07-26T17:31:45.437928Z",
          "iopub.status.idle": "2024-07-26T17:31:57.835529Z",
          "shell.execute_reply.started": "2024-07-26T17:31:45.437885Z",
          "shell.execute_reply": "2024-07-26T17:31:57.834418Z"
        },
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 36076874.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1181511.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9852960.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3491665.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "MNIST Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicialización de los recursos de los clientes\n",
        "\n",
        "El siguiente paso para construir este entorno de simulación es determinar los recursos iniciales que tendrán los clientes. Los clientes son en esencia dispositivos (e.g. móviles, coches inteligentes, SmartTVs, ...) que tienen ciertas especificaciones que definen los recursos que posee. HybridFL así como FedCS, consideran dos tipos de recursos en los dispositivos:\n",
        "\n",
        "- Recursos de comunicación inalámbrica: que influye directamente en la comunicación entre el servidor y los clientes (e.g ancho de banda en red, congestión de la red, _Resource Blocks_ $^1$). Podemos determinar la capacidad de comunicación de los clientes con el servidor en __cuánta cantidad de información es capaz de enviar en un segundo__, a lo que también se le conoce como _throughput_, y utilizamos como unidades estándar Mbit/s.\n",
        "\n",
        "- Recursos computacionales: que influye en las actualizaciones del modelo en los clientes. HybridFL implementa el concepto de entrenamiento centralizado en el servidor; por tanto, como una suposición estándar en FL, suponemos que el servidor central tiene suficiente potencia computacional como para sobrepasar a los clientes en velocidad por la existencia de _stragglers_ [4]. Por lo que el tiempo de actualización de reloj del servidor la suponemos __nula o despreciable__. Podemos determinar la capacidad computacional de los clientes como __el número de ejemplos que puede procesar en un segundo__ actualizar el modelo [1].\n",
        "\n",
        "- Permiso de subida de datos: este tipo de recurso, propio de HybridFL, es básicamente el permiso del cliente de subir sus datos al servidor, cuyo número de clientes se asume muy pequeño (menos del $\\%1$ como mínimo [1]).\n",
        "\n",
        "Por tanto, para poder muestrear en cada ronda de FL los recursos de los clientes, inicializamos primero las capacidades computacionales y de comunicación medias de cada cliente, de manera que en cada ronda se puedan samplear con una distribución Gaussiana truncada [5] como lo hacen los autores en [1]. Concretamente, el _throughput_ medio de los clientes se ha extraído de los experimentos hechos en un entorno simulado de comunicaciones inalámbricas de [1], en el que concluyeron que los clientes presentaban un _throughput_ de $\\theta^{\\text{avg}}_{k} = 1.4$ Mbit/s de media y un máximo de hasta $8.6$ Mbit/s, que resultan valores realistas en una red LTE. En cuanto a la capacidad computacional, se muestrea el número de datos procesados por segundo en entrenamiento de una distribución uniforme $\\gamma^{\\text{avg}}_{k} \\sim U(10, 100)$. Finalmente, se elige de forma aleatoria y uniforme si el cliente $k$ permite su subida de datos en un ratio de $r_{\\text{UL}} = 0.01$.\n",
        "\n",
        "> $^1$ Aunque el número de bloques de recursos o RBs (la unidad mínima de ancho de banda en LTE [3]) puede influir significativamente en el tiempo de comunicación entre el servidor y los clientes, seguiremos la suposición de [2] por el que los RBs asignados a cada cliente son los mismos para todos, de esta manera solo nos preocupan factores más fluctuantes en casos reales como el ancho de banda.\n",
        ">\n",
        "> [1] https://arxiv.org/abs/1905.07210\n",
        ">\n",
        "> [2] https://arxiv.org/abs/1804.08333\n",
        ">\n",
        "> [3] _S. Sesia, M. Baker, and I. Toufik, LTE-the UMTS Long Term Evolution: From Theory to Practice. John Wiley & Sons, 2011_\n",
        ">\n",
        "> [4] G. Wang, C. Zhao, Q. Qi, R. Han, L. Bai and J. Choi, \"Efficient Federated Learning via Joint Communication and Computation Optimization,\" in IEEE Transactions on Vehicular Technology, doi: 10.1109/TVT.2024.3379742.\n",
        ">\n",
        "> [5] https://en.wikipedia.org/wiki/Truncated_normal_distribution"
      ],
      "metadata": {
        "id": "GS_QNYeT7tuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# ratio of clients that permit their data to be uploaded\n",
        "DEFAULT_R_UPLOAD = 0.05\n",
        "\n",
        "# realistic client average throughput in Mbit/s\n",
        "AVG_THROUGHPUT = 1.4\n",
        "\n",
        "def init_client_resources(client_flex_model: FlexModel, client_data: Dataset, **kwargs):\n",
        "    # parse arguments\n",
        "    r_upload = kwargs[\"r_upload\"] if \"r_upload\" in kwargs else DEFAULT_R_UPLOAD\n",
        "\n",
        "    ## Computation capability - data samples per second\n",
        "    avg_comp = np.random.uniform(10, 100)\n",
        "\n",
        "    ## Communication capability - Mbit per second\n",
        "    avg_thr = AVG_THROUGHPUT\n",
        "\n",
        "    # client permits data upload\n",
        "    data_upload = np.random.rand() < r_upload\n",
        "\n",
        "    client_flex_model[\"avg_thr\"] = avg_thr\n",
        "    client_flex_model[\"avg_comp\"] = avg_comp\n",
        "    client_flex_model[\"data_upload\"] = data_upload\n",
        "\n",
        "    # save Nk\n",
        "    client_flex_model[\"n_samples\"] = len(client_data)\n",
        "\n",
        "    # save amount of data for each class\n",
        "    labels = client_data.to_numpy()[1]\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    client_flex_model[\"n_samples_per_class\"] = dict(zip(unique, counts))\n",
        "\n",
        "    # if client permit data to be uploaded, make their dataset public\n",
        "    if data_upload:\n",
        "        client_flex_model[\"public_data\"] = client_data"
      ],
      "metadata": {
        "id": "3pk-955syZgo",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:57.836979Z",
          "iopub.execute_input": "2024-07-26T17:31:57.837323Z",
          "iopub.status.idle": "2024-07-26T17:31:57.847305Z",
          "shell.execute_reply.started": "2024-07-26T17:31:57.837293Z",
          "shell.execute_reply": "2024-07-26T17:31:57.846041Z"
        },
        "trusted": true
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Petición de recursos\n",
        "\n",
        "A continuación, implementamos la función `resource_request` en la que el servidor/agregador comunica a $\\lceil K \\times C \\rceil$ clientes (siendo $K$ el total de clientes y $C$ la fracción de clientes a seleccionar por ronda) seleccionados aleatoriamente, a notificarle de los recursos que poseen para la ronda actual, así como notificarle si permiten su subida de datos al servidor. Esta tarea se realizará de la siguiente manera:\n",
        "\n",
        "1. El servidor inicializará en su modelo de FLEX, un diccionario para cada recurso e información adicional, en el que las claves de los mismos será el identificador del cliente en cuestión.\n",
        "2. Para cada cliente, se recuperará los valores medios de las capacidades computacionales (el número de ejemplos procesados por unidad de tiempo) y de comunicación (_throughput_), así como también si permiten su subida de los datos.\n",
        "    - Con esta información, para el cliente en cuestión, se muestrea la capacidad computacional y de comunicación para la ronda actual de una distribución normal truncada (usamos la librería de `scipy` [1] para muestrear aleatoriamente de esta distribución) con los valores medios recuperados de los clientes $^1$. Para el muestreo de la distribución normal truncada se usa una variancia para ambos tipos de recursos de $r_{\\text{var}} = 0.2$.\n",
        "    - Finalmente, con los recursos muestreados para la ronda actual, se pueden calcular los tiempos estimados de actualización y subida: $t^{\\text{UL}}_k$ y $t^{\\text{UD}}_k$ respectivamente. Que se usarán tanto para la selección de clientes y de datos como para el entrenamiento y subida de modelos agendados.\n",
        "\n",
        "Definimos además una función `get_model_size` que calcula el tamaño en bytes de un modelo de aprendizaje. Esto nos servirá para estimar el tiempo que tarda un cliente con cierto _throughput_ en subir un modelo mediante la siguiente formula:\n",
        "$$t_{k}^{\\text{UL}} = D_m / \\theta_k^{(t)}$$\n",
        "\n",
        "siendo $D_m$ el tamaño en Mbit del modelo, y $\\theta_k^{(t)}$ el _throughput_ del cliente $k$ en la ronda $t$.\n",
        "\n",
        "El tiempo estimado en actualizar un modelo en un cliente $k$ (i.e. entrenar un modelo) se calcula con la siguiente formula:\n",
        "$$t_k^{\\text{UD}} = \\frac{E \\times N_k}{\\gamma_k^{(t)}}$$\n",
        "\n",
        "siendo $E$ el número de épocas de entrenamiento, $N_k$ el número de ejemplos que posee el cliente $k$ y $\\gamma_k^{(t)}$ el número de ejemplo procesados por unidad de tiempo para entrenar el modelo en el cliente $k$ en la ronda $t$.\n",
        "\n",
        "> $^1$ Nótese que conceptualmente, este muestreo de la capacidad de comunicación y computacional del cliente es lo que este último realmente notifica al servidor y no sus valores medios.\n",
        ">\n",
        "> [1] https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.truncnorm.html"
      ],
      "metadata": {
        "id": "sJR_M4pHppB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_model_size(model: nn.Module) -> int:\n",
        "    \"\"\"\n",
        "    Get the size in bytes of a PyTorch learning model.\n",
        "    \"\"\"\n",
        "    total_size = 0\n",
        "    for param in model.parameters():\n",
        "        total_size += param.nelement() * param.element_size()\n",
        "\n",
        "    return int(total_size)"
      ],
      "metadata": {
        "id": "HDL2SEX_vkMP",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:57.848636Z",
          "iopub.execute_input": "2024-07-26T17:31:57.848995Z",
          "iopub.status.idle": "2024-07-26T17:31:57.866216Z",
          "shell.execute_reply.started": "2024-07-26T17:31:57.848954Z",
          "shell.execute_reply": "2024-07-26T17:31:57.865003Z"
        },
        "trusted": true
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "# loose for simulated capabilities\n",
        "r_var = 0.2\n",
        "\n",
        "def resource_request(agg_model: FlexModel, client_flex_models: List[FlexModel], **kwargs):\n",
        "    \"\"\"\n",
        "    Requests resource capabilities and whether or not they permit their data to\n",
        "    be uploaded for M selected clients randomly.\n",
        "    \"\"\"\n",
        "    # parse arguments\n",
        "    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 5\n",
        "\n",
        "    # get model size in bytes\n",
        "    if \"model_size\" in agg_model.keys():\n",
        "        model_size = agg_model[\"model_size\"]\n",
        "    else:\n",
        "        model_size = get_model_size(agg_model[\"model\"])\n",
        "\n",
        "        # store model size in server model\n",
        "        agg_model[\"model_size\"] = model_size\n",
        "\n",
        "\n",
        "    # indexed resources for each selected client saved in the aggregator\n",
        "    agg_model[\"comp_resources\"] = {}\n",
        "    agg_model[\"comm_resources\"] = {}\n",
        "    agg_model[\"client_t_update\"] = {}\n",
        "    agg_model[\"client_t_upload\"] = {}\n",
        "    agg_model[\"permit_data_upload\"] = {}\n",
        "    agg_model[\"client_data\"] = {}\n",
        "    agg_model[\"client_n_samples_per_class\"] = {}\n",
        "\n",
        "    for k in client_flex_models:\n",
        "        # get client average computational and communications capabilities\n",
        "        avg_comp_cap = client_flex_models[k][\"avg_comp\"]\n",
        "        avg_thr = client_flex_models[k][\"avg_thr\"]\n",
        "        permit_data_upload = client_flex_models[k][\"data_upload\"]\n",
        "\n",
        "        ## Computational capability - data samples processed per second to train\n",
        "        std_dev_comp = 0.1 * avg_comp_cap\n",
        "\n",
        "        # compute lower and upper bounds\n",
        "        a, b = (1 - r_var) * avg_comp_cap, (1 + r_var) * avg_comp_cap\n",
        "        a = (a - avg_comp_cap) / std_dev_comp\n",
        "        b = (b - avg_comp_cap) / std_dev_comp\n",
        "\n",
        "        comp_cap = truncnorm.rvs(a, b, loc=avg_comp_cap, scale=std_dev_comp, size=1)[0]\n",
        "\n",
        "        # compute estimated update time\n",
        "        n_samples = client_flex_models[k][\"n_samples\"]\n",
        "        t_update = (epochs * n_samples) / comp_cap\n",
        "\n",
        "        ## Communication capability - Mbit per second\n",
        "        std_dev_thr = 0.1 * avg_thr\n",
        "\n",
        "        # compute lower and upper bounds\n",
        "        a, b = (1 - r_var) * avg_thr, (1 + r_var) * avg_thr\n",
        "        a = (a - avg_thr) / std_dev_thr\n",
        "        b = (b - avg_thr) / std_dev_thr\n",
        "\n",
        "        comm_cap = truncnorm.rvs(a, b, loc=avg_thr, scale=std_dev_thr, size=1)[0]\n",
        "\n",
        "        # compute estimated upload time in seconds\n",
        "        t_upload = (model_size * 8) / (comm_cap * 1_000_000)\n",
        "\n",
        "        # save resource capabilities for client k in current round\n",
        "        agg_model[\"comp_resources\"][k] = comp_cap\n",
        "        agg_model[\"comm_resources\"][k] = comm_cap\n",
        "\n",
        "        # save estimated update and upload times\n",
        "        agg_model[\"client_t_update\"][k] = t_update\n",
        "        agg_model[\"client_t_upload\"][k] = t_upload\n",
        "\n",
        "        # also save estimated times in clients to simulate scheduled training\n",
        "        client_flex_models[k][\"t_update\"] = t_update\n",
        "        # client_flex_models[k][\"t_upload\"] = t_upload\n",
        "\n",
        "        ## Client Data\n",
        "        if permit_data_upload:\n",
        "            agg_model[\"client_data\"][k] = client_flex_models[k][\"public_data\"]\n",
        "\n",
        "        # set data upload permit\n",
        "        agg_model[\"permit_data_upload\"][k] = permit_data_upload\n",
        "\n",
        "        ## Amount of data for each class\n",
        "        agg_model[\"client_n_samples_per_class\"][k] = client_flex_models[k][\"n_samples_per_class\"]"
      ],
      "metadata": {
        "id": "9Yr27vPgrsiJ",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:57.867832Z",
          "iopub.execute_input": "2024-07-26T17:31:57.868308Z",
          "iopub.status.idle": "2024-07-26T17:31:57.887510Z",
          "shell.execute_reply.started": "2024-07-26T17:31:57.868266Z",
          "shell.execute_reply": "2024-07-26T17:31:57.886225Z"
        },
        "trusted": true
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección de clientes\n",
        "\n",
        "La selección de los clientes es un algoritmo de tipo _greedy_ en el que se busca maximizar el número de clientes seleccionados $𝕊$ sujeto a un cierto _deadline_ de tiempo por ronda $T_\\text{round}$:\n",
        "$$\n",
        "\\max |𝕊| \\\\\n",
        "\\text{s.t.} \\ T_\\text{round} \\ge T_\\text{cs} + T_{𝕊}^{\\text d} + \\Theta_{|𝕊|} + T_\\text{agg}\n",
        "$$\n",
        "siendo $T_\\text{cs}$ el tiempo de selección de clientes, $T_{𝕊}^{\\text d}$ el tiempo de distribución del modelo global a los clientes seleccionados, $\\Theta_{|𝕊|}$ el tiempo transcurrido desde el inicio de la actualización de los modelos hasta que el $k_i$-ésimo cliente haya actualizado y subido sus parámetros al servidor (más información en como se calcula este tiempo en [1]) y $T_\\text{agg}$ el tiempo que toma al servidor agregar los parámetros de los clientes.\n",
        "\n",
        "Como en [1] y [2], nosotros suponemos que el servidor tiene vastos recursos y que por tanto los tiempos de selección y agregación son despreciables: $T_\\text{cs} = T_\\text{agg} = 0$.\n",
        "\n",
        "El algoritmo de selección en HybridFL [2] varía del de FedCS [1] en cuanto a que en el primero se elige el cliente con tiempo estimado mínimo mientras que en [1] se elige el que tenga la inversa del tiempo máximo. Como podemos darnos cuenta las dos formas son equivalentes. En nuestro caso optamos por la minimización de la función (i.e. elegir el cliente con menor tiempo estimado agregado). En HybridFL [2], esta función de tiempo transcurrido si se incluye el cliente $k$ al conjunto de seleccionados se le denomina $T_{\\text{inc}}(𝕊, k)$.\n",
        "\n",
        "En [2] además, se elige el cliente con $T_{\\text{inc}}$ menor multiplicado por un coeficiente de variación $CV(N_r)$ que penaliza la función a minimizar si los datos de los clientes tienen un alto nivel de desbalance entre clases, de manera que se pueda sanitizar el problema de datos No-IID en los clientes en esa ronda de selección. Mas información sobre como calcular este coeficiente en [2].\n",
        "\n",
        "> [1] https://arxiv.org/abs/1905.07210\n",
        ">\n",
        "> [2] https://arxiv.org/abs/1804.08333"
      ],
      "metadata": {
        "id": "OyVAxgGCzlgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Para poder implementar la selección de los clientes, tenemos primero que definir:\n",
        "1. Por un lado, una función que calcule el tiempo de distribución del modelo a cada cliente.\n",
        "2. Una función para calcular el tiempo estimado transcurrido si incluimos un cliente $k$ al conjunto de clientes seleccionados $𝕊$, $T_{\\text{inc}}(𝕊, k)$.\n",
        "3. Y por último, una función para calcular el coeficiente de variación $CV(N_r)$.\n",
        "\n",
        "Definimos $T_\\text{round}$ como una variable global para el que se establece como límite de tiempo por ronda de unos $3$ minutos. Se ha elegido este tiempo por mostrar buenos resultados tanto en HybridFL como en FedCS además de ser lo suficientemente corto como para garantizar no agotar todos los clientes, sin embargo, en [1] se deja como una vía de investigación a mejorar el de utilizar un $T_\\text{round}$ dinámico."
      ],
      "metadata": {
        "id": "NVzOaUan3XGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def T_model_dist(model_size: int, throughputs: list):\n",
        "    \"\"\"\n",
        "    Computes the estimated time it takes to distribute the model to all clients synchronically.\n",
        "\n",
        "    If throughputs is empty, then time is 0 (no clients to distribute to).\n",
        "    \"\"\"\n",
        "    if not throughputs:\n",
        "        return 0\n",
        "\n",
        "    return (model_size * 8) / (min(throughputs) * 1_000_000)"
      ],
      "metadata": {
        "id": "wLr_waY8dtm-",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:57.888922Z",
          "iopub.execute_input": "2024-07-26T17:31:57.889359Z",
          "iopub.status.idle": "2024-07-26T17:31:57.903440Z",
          "shell.execute_reply.started": "2024-07-26T17:31:57.889326Z",
          "shell.execute_reply": "2024-07-26T17:31:57.902009Z"
        },
        "trusted": true
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def T_inc(agg_model: FlexModel, S: set, k: int, elapsed_time: int):\n",
        "    \"\"\"\n",
        "    Computes estimate elapsed time when client k is included to selected set of clients S.\n",
        "    \"\"\"\n",
        "    t_upload = agg_model[\"client_t_upload\"][k]\n",
        "    t_update = agg_model[\"client_t_update\"][k]\n",
        "\n",
        "    # get model size to estimate dist time\n",
        "    model_size = agg_model[\"model_size\"]\n",
        "\n",
        "    # throughputs of clients in S\n",
        "    S_thr = [agg_model[\"comm_resources\"][client] for client in agg_model[\"comm_resources\"] if client in S]\n",
        "\n",
        "    # throughputs of clients in S + {k}\n",
        "    S_plus_k_thr = [agg_model[\"comm_resources\"][client] for client in agg_model[\"comm_resources\"] if client in S or client == k]\n",
        "\n",
        "    return T_model_dist(model_size, S_plus_k_thr) - T_model_dist(model_size, S_thr) + t_upload + max(0, t_update - elapsed_time)"
      ],
      "metadata": {
        "id": "pt_uHGutH0FF",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:57.905080Z",
          "iopub.execute_input": "2024-07-26T17:31:57.905442Z",
          "iopub.status.idle": "2024-07-26T17:31:57.915471Z",
          "shell.execute_reply.started": "2024-07-26T17:31:57.905410Z",
          "shell.execute_reply": "2024-07-26T17:31:57.914266Z"
        },
        "trusted": true
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CV(agg_model: FlexModel, S: set, classes: list):\n",
        "    \"\"\"\n",
        "    Computes the coefficient of variation of data samples per class in clients in set S.\n",
        "    \"\"\"\n",
        "    # get total number of data samples per class\n",
        "    Nr = {}\n",
        "    for k in agg_model[\"client_n_samples_per_class\"]:\n",
        "        if k not in S:\n",
        "            continue\n",
        "\n",
        "        k_counts = agg_model[\"client_n_samples_per_class\"][k]\n",
        "        Nr = {key: Nr.get(key, 0) + k_counts.get(key, 0) for key in set(Nr) | set(k_counts)}\n",
        "\n",
        "    # fill up for all classes\n",
        "    Nr = {key: Nr.get(key, 0) for key in classes}\n",
        "\n",
        "    L = len(classes)\n",
        "    n_mean = sum(Nr.values()) / L\n",
        "\n",
        "    # compute the variance of the amount of data per class\n",
        "    variance = sum((n_l - n_mean)**2 for n_l in Nr.values()) / L\n",
        "\n",
        "    # return the coefficient of variance with respect to the mean\n",
        "    return variance / n_mean"
      ],
      "metadata": {
        "id": "CjbRsXxgjhr3",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:57.917020Z",
          "iopub.execute_input": "2024-07-26T17:31:57.917469Z",
          "iopub.status.idle": "2024-07-26T17:31:57.930082Z",
          "shell.execute_reply.started": "2024-07-26T17:31:57.917436Z",
          "shell.execute_reply": "2024-07-26T17:31:57.928894Z"
        },
        "trusted": true
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.data import Dataset\n",
        "\n",
        "T_round = 3 * 60    # 3 minutes\n",
        "\n",
        "def client_selection(agg_model: FlexModel, agg_data: Dataset):\n",
        "    # get classes of the ML task\n",
        "    classes = agg_model[\"classes\"]\n",
        "\n",
        "    # get client indices\n",
        "    Kp = set(agg_model[\"comp_resources\"].keys())\n",
        "\n",
        "    # initialize set of selected clients to train locally\n",
        "    S = set()\n",
        "\n",
        "    # initialize time budget\n",
        "    t = 0\n",
        "\n",
        "    # function to minimize\n",
        "    def f(x):\n",
        "        return T_inc(agg_model, S, x, t)\n",
        "\n",
        "    while len(Kp) > 0:\n",
        "        x = min(Kp, key=lambda k: f(k) + CV(agg_model, S | {k}, classes))\n",
        "\n",
        "        # remove selected from Kp\n",
        "        Kp.remove(x)\n",
        "\n",
        "        # check time deadline\n",
        "        tp = t + T_inc(agg_model, S, x, t)\n",
        "        if tp < T_round:\n",
        "            # add client if time deadline is met\n",
        "            t = tp\n",
        "            S.add(x)\n",
        "\n",
        "    return S"
      ],
      "metadata": {
        "id": "chBafb6YDncA",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:57.931528Z",
          "iopub.execute_input": "2024-07-26T17:31:57.931886Z",
          "iopub.status.idle": "2024-07-26T17:31:57.942032Z",
          "shell.execute_reply.started": "2024-07-26T17:31:57.931857Z",
          "shell.execute_reply": "2024-07-26T17:31:57.940920Z"
        },
        "trusted": true
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección de datos\n",
        "\n",
        "En esta fase, el servidor recupera todos los datos de los clientes que permitieron la subida de sus datos $D_u$, y el servidor entonces selecciona los ejemplos según un criterio de selección que depende del _deadline_ $t^\\text{UD}$, que es el tiempo decidido para el que cuentan los clientes en subir sus datos antes de que el primer cliente en actualizar el modelo suba sus parámetros. En HybridFL [1], se proponen dos formas de seleccionar los datos que conforman el conjunto de datos final $D^\\text{UL}$ para el que el servidor actualiza el modelo global:\n",
        "1. El primero es seleccionar de manera _greedy_ los ejemplos del cliente con mayor _throughput_ en orden.\n",
        "2. En el segundo, el servidor intenta construir un conjunto de datos IID de la misma forma que en el primero solo que en vez de hacerlo en orden, lo hace por clase, de manera que se distribuyan los datos de manera idéntica. El dataset $D^\\text{UL}$ ya cuenta con la propiedad de ser uniformemente distribuído por haber seleccionado los clientes de forma aleatoria en la fase de __Petición de Recursos__.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1905.07210"
      ],
      "metadata": {
        "id": "Rv2kD9k6AD6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from flex.data import Dataset\n",
        "\n",
        "def data_selection(agg_model: FlexModel, agg_data: Dataset, **kwargs):\n",
        "    S = kwargs[\"S\"]\n",
        "\n",
        "    # get clients that permit their data to be uploaded and they are not selected to train\n",
        "    U = [u for u in agg_model[\"permit_data_upload\"] if agg_model[\"permit_data_upload\"][u] and u not in S]\n",
        "\n",
        "    print(f\"Number of clients in U: {len(U)}\")\n",
        "\n",
        "    t_upload_deadline = min(agg_model[\"client_t_update\"].values())\n",
        "\n",
        "    classes = agg_model[\"classes\"]\n",
        "\n",
        "    # masks to skip samples indices from client data\n",
        "    removal_masks = {u: [] for u in agg_model[\"client_data\"]}\n",
        "\n",
        "    def client_has_class(k: int, label: int):\n",
        "        labels = agg_model[\"client_data\"][k].to_numpy()[1]\n",
        "\n",
        "        # remove already taken samples\n",
        "        removal_mask = removal_masks[k]\n",
        "        filtered_classes = np.delete(labels, removal_mask, axis=0)\n",
        "\n",
        "        res = filtered_classes[filtered_classes == label].size > 0\n",
        "\n",
        "        return res\n",
        "\n",
        "    def empty_client_data():\n",
        "        samples = []\n",
        "        for u in U:\n",
        "            d_x, d_y = agg_model[\"client_data\"][u].to_numpy()\n",
        "            filtered_d_x = np.delete(d_x, removal_masks[u], axis=0)\n",
        "            filtered_d_y = np.delete(d_y, removal_masks[u], axis=0)\n",
        "            samples.append(filtered_d_x.size)\n",
        "\n",
        "        return sum(samples) == 0\n",
        "\n",
        "    D_ul_X = []\n",
        "    D_ul_y = []\n",
        "    flag = True\n",
        "    t_data_upload = 0\n",
        "\n",
        "    while flag:\n",
        "        # free up memory\n",
        "        gc.collect()\n",
        "\n",
        "        for l in classes:\n",
        "            # get client with maximum throughput with class l samples\n",
        "            x = max([u for u in U if client_has_class(u, l)], key=lambda k: agg_model[\"comm_resources\"][k], default=None)\n",
        "\n",
        "            # if there's no client with l class, just continue with the other class\n",
        "            if not x:\n",
        "                # check if there's data before continuing to avoid infinite loop\n",
        "                flag = not empty_client_data()\n",
        "                continue\n",
        "\n",
        "            d_x, d_y = agg_model[\"client_data\"][x].to_numpy()\n",
        "\n",
        "            # filter data with label l\n",
        "            mask = np.where(d_y == l)[0].tolist()\n",
        "\n",
        "            # filter already taken samples\n",
        "            mask = [i for i in mask if i not in removal_masks[x]]\n",
        "\n",
        "            # take next first sample of class l\n",
        "            d = d_x[mask][0]\n",
        "\n",
        "            # compute data upload time\n",
        "            u_thr = agg_model[\"comm_resources\"][x]\n",
        "            D_sample = d.nbytes + np.array(l).nbytes   # size of sample\n",
        "\n",
        "            t_u = (D_sample * 8) / (u_thr * 1_000_000)\n",
        "\n",
        "            if (t_data_upload + t_u) <= t_upload_deadline:\n",
        "                # add d to D_ul\n",
        "                D_ul_X.append(d)\n",
        "                D_ul_y.append(l)\n",
        "\n",
        "                # remove d from D_ul\n",
        "                removal_masks[x].append(mask[0])\n",
        "\n",
        "            # check deadline\n",
        "            if (t_data_upload + t_u) > t_upload_deadline:\n",
        "                flag = False\n",
        "\n",
        "            # finish if clients data are empty\n",
        "            if empty_client_data():\n",
        "                flag = False\n",
        "                break\n",
        "\n",
        "            # update data upload time\n",
        "            t_data_upload += t_u\n",
        "\n",
        "    return np.array(D_ul_X), np.array(D_ul_y)"
      ],
      "metadata": {
        "id": "kLj7YqUTqz8q",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:31:57.943779Z",
          "iopub.execute_input": "2024-07-26T17:31:57.944784Z",
          "iopub.status.idle": "2024-07-26T17:31:57.965702Z",
          "shell.execute_reply.started": "2024-07-26T17:31:57.944750Z",
          "shell.execute_reply": "2024-07-26T17:31:57.964569Z"
        },
        "trusted": true
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribución del modelo global\n",
        "\n",
        "Redefinimos la subrutina de `copy_server_model_to_clients` visto en la configuración básica, para copiar solo la información que deben de compartir los clientes con el servidor, es decir, no debemos distribuir la información almacenada propia del servidor a los clientes y solo la que nos interesa. Esto viene a darse porque FLEX por defecto copia el modelo del servidor entero, y por tanto se copiaría los datos de los clientes $D_u$, información de los recursos de todos los clientes, etc.\n",
        "\n",
        "Para solo distribuir la información que nos interesa a los clientes, distribuimos toda la información del servidor para las claves que estén en la lista de claves de distribución (_white list_)."
      ],
      "metadata": {
        "id": "1viI2fAYJNVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.pool import deploy_server_model\n",
        "import copy\n",
        "\n",
        "white_list = [\n",
        "    \"model\",\n",
        "    \"criterion\",\n",
        "    \"optimizer_func\",\n",
        "    \"optimizer_kwargs\"\n",
        "]\n",
        "\n",
        "@deploy_server_model\n",
        "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
        "    white_listed = {key: value for key, value in server_flex_model.items() if key in white_list}\n",
        "    return copy.deepcopy(white_listed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-26T17:32:34.488668Z",
          "iopub.execute_input": "2024-07-26T17:32:34.489222Z",
          "iopub.status.idle": "2024-07-26T17:32:34.497928Z",
          "shell.execute_reply.started": "2024-07-26T17:32:34.489178Z",
          "shell.execute_reply": "2024-07-26T17:32:34.496398Z"
        },
        "trusted": true,
        "id": "tntdkpO2JNVs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actualización del modelo agendado\n",
        "\n",
        "Como hemos explicado anteriormente, para simular el tiempo que se estima en la actualización del modelo en los clientes, definimos la función de entrenamiento `scheduled_train` el cual entrena de la manera convencional que hemos visto en la configuración básica del entorno FL, pero esta vez, el tiempo transcurrido de esta función está sujeto a cumplir el tiempo de actualización para ese cliente como mínimo. De manera que se refleje el impacto de la selección de tal cliente en la duración total del entrenamiento del modelo global."
      ],
      "metadata": {
        "id": "dK0fuEjrCf4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def scheduled_train(client_flex_model: FlexModel, client_data: Dataset, **kwargs):\n",
        "    # parse arguments\n",
        "    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 5\n",
        "    batch_size = kwargs[\"batch_size\"] if \"batch_size\" in kwargs else 20\n",
        "    transforms = kwargs[\"transforms\"] if \"transforms\" in kwargs else None\n",
        "\n",
        "    # get scheduled update time\n",
        "    t_update = client_flex_model[\"t_update\"]\n",
        "\n",
        "    # start timer\n",
        "    start = time.time()\n",
        "\n",
        "    train_dataset = client_data.to_torchvision_dataset(transform=transforms)\n",
        "    client_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "    model = client_flex_model[\"model\"]\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    model = model.train()\n",
        "    model = model.to(device)\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "    for _ in range(epochs):\n",
        "        for imgs, labels in client_dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(imgs)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # stop timer and get effective training time\n",
        "    end = time.time()\n",
        "    training_time = end - start\n",
        "\n",
        "    # simulate scheduled updating time by sleeping\n",
        "    time.sleep(max(0, t_update - training_time))"
      ],
      "metadata": {
        "id": "RNKALJ1GFHdo",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:32:34.500659Z",
          "iopub.execute_input": "2024-07-26T17:32:34.501145Z",
          "iopub.status.idle": "2024-07-26T17:32:34.515876Z",
          "shell.execute_reply.started": "2024-07-26T17:32:34.501086Z",
          "shell.execute_reply": "2024-07-26T17:32:34.514401Z"
        },
        "trusted": true
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cuanto a la actualización del modelo de forma centralizada en el servidor con la función `centralized_train`, donde realizamos el mismo bucle de entrenamiento pero en este caso _sin agendar_, debido a que hemos supuesto que el tiempo de entrenamiento del servidor era despreciable comparado con los cliente, y que por tanto no tendría ningún impacto en el tiempo transcurrido de entrenamiento federado. También, ante la posibilidad de que no se haya podido recuperar datos del cliente (e.g. $U = 0$), indicamos con un campo booleano si el servidor ha podido actualizar el modelo o no; para de esta manera saber _a posteriori_ si agregamos los pesos del servidor si y solo si son nuevos parámetros."
      ],
      "metadata": {
        "id": "9gdQFIIQExY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def centralized_train(server_model: FlexModel, _test_data: Dataset, **kwargs):\n",
        "    # parse arguments\n",
        "    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 5\n",
        "    batch_size = kwargs[\"batch_size\"] if \"batch_size\" in kwargs else 20\n",
        "    transforms = kwargs[\"transforms\"] if \"transforms\" in kwargs else None\n",
        "\n",
        "    # get client data\n",
        "    X, y = kwargs[\"client_data\"]\n",
        "\n",
        "    if y.size == 0:\n",
        "        # model has not been updated by server\n",
        "        server_model[\"trained\"] = False\n",
        "        return\n",
        "\n",
        "    client_data = Dataset.from_array(X, y)\n",
        "\n",
        "    train_dataset = client_data.to_torchvision_dataset(transform=transforms)\n",
        "    client_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "    model = server_model[\"model\"]\n",
        "    optimizer = server_model[\"optimizer_func\"](\n",
        "        model.parameters(), **server_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    model = model.train()\n",
        "    model = model.to(device)\n",
        "    criterion = server_model[\"criterion\"]\n",
        "    for _ in range(epochs):\n",
        "        for imgs, labels in client_dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(imgs)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # server has updated the model\n",
        "    server_model[\"trained\"] = True"
      ],
      "metadata": {
        "id": "3OC4fi-JHQF-",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:32:34.517384Z",
          "iopub.execute_input": "2024-07-26T17:32:34.517767Z",
          "iopub.status.idle": "2024-07-26T17:32:34.537296Z",
          "shell.execute_reply.started": "2024-07-26T17:32:34.517737Z",
          "shell.execute_reply": "2024-07-26T17:32:34.535574Z"
        },
        "trusted": true
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subida de las actualizaciones de los modelos\n",
        "\n",
        "Por último, agendamos de la misma manera que hicimos en las actualizaciones del modelo pero esta vez con la subida de los parámetros de las actualizaciones de los clientes. La subida se estos parámetros se realiza de manera síncrona, por lo que el tiempo total de subida $t^\\text{UL}$ es el tiempo acumulado de todos los $\\sum_{k \\in 𝕊}{t_k^\\text{UL}}$. Resulta evidente que el tiempo de subida del servidor $t^\\text{UL}_\\text{server} = 0$ dado que el modelo \"_ya se encuentra_\" en el servidor."
      ],
      "metadata": {
        "id": "SWt4c9utGAtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def collect_server_and_clients_weights(aggregator_flex_model: FlexModel, clients_flex_models: List[FlexModel]):\n",
        "\n",
        "    def get_model_weights(client_flex_model: FlexModel):\n",
        "        \"\"\"\n",
        "        Returns the weights of a Flex model per layer.\n",
        "        \"\"\"\n",
        "        weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "        return [weight_dict[name] for name in weight_dict]\n",
        "\n",
        "    if \"weights\" not in aggregator_flex_model:\n",
        "        aggregator_flex_model[\"weights\"] = []\n",
        "\n",
        "    # collect clients weights\n",
        "    upload_time = 0\n",
        "    for k in clients_flex_models:\n",
        "        # time upload\n",
        "        start = time.time()\n",
        "\n",
        "        client_weights = get_model_weights(clients_flex_models[k])\n",
        "        aggregator_flex_model[\"weights\"].append(client_weights)\n",
        "\n",
        "        end = time.time()\n",
        "        upload_time += end - start\n",
        "\n",
        "    # simulate scheduled uploading time by sleeping\n",
        "    estimated_upload_time = sum(aggregator_flex_model[\"client_t_upload\"].values())\n",
        "    time_to_sleep = max(0, estimated_upload_time - upload_time)\n",
        "\n",
        "    with tqdm(total=time_to_sleep, desc=\"Uploading weights\") as pbar:\n",
        "        interval = 0.1\n",
        "        for _ in range(int(time_to_sleep / interval)):\n",
        "            time.sleep(interval)\n",
        "            pbar.update(interval)\n",
        "\n",
        "    # finally collect server weights if server has trained with client data\n",
        "    if aggregator_flex_model[\"trained\"]:\n",
        "        server_weights = get_model_weights(aggregator_flex_model)\n",
        "        aggregator_flex_model[\"weights\"].append(server_weights)"
      ],
      "metadata": {
        "id": "cp5DB6uWM51W",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:35:16.824782Z",
          "iopub.execute_input": "2024-07-26T17:35:16.825326Z",
          "iopub.status.idle": "2024-07-26T17:35:16.839042Z",
          "shell.execute_reply.started": "2024-07-26T17:35:16.825285Z",
          "shell.execute_reply": "2024-07-26T17:35:16.837677Z"
        },
        "trusted": true
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento paralelo de los modelos\n",
        "\n",
        "Cuando no tenemos restricciones de tiempo ni fases agendadas, es viable realizar las actualizaciones locales de los clientes de forma _iterativa_, i.e. una actualización tras de otra. Sin embargo, para poder simular de la manera más cercana a la realidad el entrenamiento en cada ronda y que los resultados sean acordes a partir de los datos y recursos de los clientes, necesitamos hacer que _al menos_ las actualizaciones de los modelos locales en los clientes se realicen de forma paralela. Esto no solo se necesita para obtener resultados coherentes con la configuración establecida, sino que también es necesario para que el entrenamiento sea viable computacionalmente; esto debido a que algunos clientes pueden tomar hasta $> 30$ segundos en actualizar un modelo, algo que puede retrasar el entrenamiento de otro en el caso de hacerse de forma iterativa y síncrona.\n",
        "\n",
        "Solo necesitaremos de paralelizar las actualizaciones en los clientes. Las demás fases como la subida de los parámetros locales, la distribución del modelo, la agregación de las actualizaciones, etc. Si se han de realizar de forma síncrona. En [1] muestran resultados del estado del arte en aprendizaje distribuído utilizando una aproximación síncrona.\n",
        "\n",
        "Para entonces poder paralelizar este entrenamiento, creamos una clase `FlexParallelPool` que hereda de la clase de FLEX `FlexPool`. Con esto, definimos un método `parallel_map` que realice la misma tarea que `map` (que es aplicar una función a un conjunto de actores destino), pero en este caso de forma paralela, creando para cada actor destino un hilo de ejecución. De esta manera lograremos paralelizar la función `scheduled_train` de los clientes y así poder simular el entrenamiento en paralelo de una forma más realista.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1604.00981"
      ],
      "metadata": {
        "id": "RuNyjTdSE0qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "from typing import Callable\n",
        "\n",
        "\n",
        "class FlexParallelPool(FlexPool):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def parallel_map(self, func: Callable, dst_pool: FlexPool = None, **kwargs):\n",
        "        if dst_pool is None:\n",
        "            threads = []\n",
        "            for i in self._actors:\n",
        "                t = threading.Thread(target=func, args=(self._models.get(i), self._data.get(i)), kwargs=kwargs)\n",
        "                threads.append(t)\n",
        "                t.start()\n",
        "\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            return None\n",
        "        elif FlexPool.check_compatibility(self, dst_pool):\n",
        "            threads = []\n",
        "            for i in self._actors:\n",
        "                t = threading.Thread(target=func, args=(self._models.get(i), dst_pool._models), kwargs=kwargs)\n",
        "                threads.append(t)\n",
        "                t.start()\n",
        "\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            return None\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Source and destination pools are not allowed to comunicate, ensure that their actors can communicate.\"\n",
        "            )"
      ],
      "metadata": {
        "id": "_EmPboRGUXLq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop final\n",
        "\n",
        "Finalmente, implementamos la función de entrenamiento en aprendizaje federado en el que aplicamos todas las funciones que realizan la selección de clientes con HybridFL. Este bucle de entrenamiento recibe un límite de tiempo $T_\\text{final}$ para el cual el bucle termina cuando haya transcurrido esta cantidad de tiempo en segundos."
      ],
      "metadata": {
        "id": "OAekDaR1JLSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import gc\n",
        "\n",
        "from flex.pool import FlexPool\n",
        "\n",
        "def train_n_rounds_hybrid(pool: FlexParallelPool, T_final: int, clients_per_round: int = 100, problem: Problem = \"mnist\"):\n",
        "    \"\"\"\n",
        "    FL training loop for a certain deadline time in seconds.\n",
        "    \"\"\"\n",
        "    # collect garbage\n",
        "    gc.collect()\n",
        "\n",
        "    # save list of classes to server model to use them in client and data selection\n",
        "    def set_classes_to_server(flex_model: FlexModel, _):\n",
        "        flex_model[\"classes\"] = MNIST_LABELS if problem == \"mnist\" else CIFAR_LABELS\n",
        "\n",
        "    pool.servers.map(set_classes_to_server)\n",
        "\n",
        "    # select transformations depending on problem to solve and assign to server\n",
        "    transforms = mnist_transforms if problem == \"mnist\" else cifar_transforms\n",
        "\n",
        "    # initialize clients simulated resources\n",
        "    pool.clients.map(init_client_resources)\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    start = time.time()\n",
        "    end = start\n",
        "    i = 0\n",
        "    while (end - start) < T_final:\n",
        "        print(f\"\\nRunning round: {i+1} - Time elapsed: {end - start} seconds\")\n",
        "\n",
        "        # Resource Request: ask M random clients to participate in the current round\n",
        "        # and clients notify the server about their resources and whether or not the permit\n",
        "        # their data to be uploaded (set U of clients)\n",
        "        print(\"Server requesting clients resources...\")\n",
        "        random_clients_pool = pool.clients.select(clients_per_round)\n",
        "        random_clients = random_clients_pool.clients\n",
        "\n",
        "        pool.aggregators.map(resource_request, random_clients)\n",
        "\n",
        "        # Client selection from Kp\n",
        "        print(\"Server selecting clients to train locally...\")\n",
        "        S = pool.aggregators.map(client_selection)[0]\n",
        "        print(f\"Selected clients (S): {len(S)} out of {clients_per_round}\")\n",
        "\n",
        "        # Data selection from U clients\n",
        "        print(\"Server selecting data to train centralized...\")\n",
        "        client_data = pool.aggregators.map(data_selection, S=S)[0]\n",
        "        print(f\"Selected data length (|D_UL|): {client_data[0].size}\")\n",
        "\n",
        "        # get selected clients pool\n",
        "        selected_clients_pool = pool.clients.select(lambda actor_id, actor_roles: actor_id in S)\n",
        "        selected_clients = selected_clients_pool.clients\n",
        "        print(f\"Selected clients for this round: {len(selected_clients)}\")\n",
        "\n",
        "        # Deploy the server model to the selected clients\n",
        "        print(\"Distributing model...\")\n",
        "        pool.servers.map(copy_server_model_to_clients, selected_clients)\n",
        "\n",
        "        # Each selected client trains their model obeying the time restrictions\n",
        "        print(\"Scheduled model update...\")\n",
        "        # selected_clients.map(scheduled_train, transforms=transforms)\n",
        "        FlexParallelPool(selected_clients._data,\n",
        "                         selected_clients._actors,\n",
        "                         selected_clients._models).parallel_map(scheduled_train, transforms=transforms)\n",
        "\n",
        "        # Server updates model from clients uploaded data\n",
        "        print(\"Centralized model update...\")\n",
        "        pool.servers.map(centralized_train, client_data=client_data, transforms=transforms)\n",
        "\n",
        "        # The aggregator collects weights from the selected clients\n",
        "        # (obeying their time restrictions) and aggregates them\n",
        "        print(\"Scheduled model upload...\")\n",
        "        pool.aggregators.map(collect_server_and_clients_weights, selected_clients)\n",
        "\n",
        "        print(\"Aggregating global model...\")\n",
        "        pool.aggregators.map(aggregate_with_fedavg)\n",
        "\n",
        "        # The aggregator send its aggregated weights to the server\n",
        "        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n",
        "        metrics = pool.servers.map(evaluate_global_model)\n",
        "        loss, acc = metrics[0]\n",
        "        losses.append(loss)\n",
        "        accuracies.append(acc)\n",
        "        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}\")\n",
        "\n",
        "        end = time.time()\n",
        "        i += 1\n",
        "\n",
        "    return losses, accuracies"
      ],
      "metadata": {
        "id": "NVPmE4zGxd-X",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:35:16.842130Z",
          "iopub.execute_input": "2024-07-26T17:35:16.842747Z",
          "iopub.status.idle": "2024-07-26T17:35:16.863515Z",
          "shell.execute_reply.started": "2024-07-26T17:35:16.842695Z",
          "shell.execute_reply": "2024-07-26T17:35:16.862186Z"
        },
        "trusted": true
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flex.pool import FlexPool\n",
        "\n",
        "# Setup\n",
        "T_FINAL_MIN = 30    # 30 minutes of FL training\n",
        "C = 0.1              # fraction of participant clients per round\n",
        "\n",
        "pool_mnist = FlexParallelPool.client_server_pool(flex_dataset_mnist, init_func=build_server_model_mlp)\n",
        "pool_cifar = FlexParallelPool.client_server_pool(flex_dataset_cifar, init_func=build_server_model_cnn)\n",
        "\n",
        "print(f\"MNIST config: {len(pool_mnist.servers)} servers and {len(pool_mnist.clients)} clients)\")\n",
        "print(f\"CIFAR-10 config: {len(pool_cifar.servers)} servers and {len(pool_cifar.clients)} clients)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsRkZAp_DzD6",
        "outputId": "1b4b8f7a-b495-42dd-f2d6-024fc033ddb9",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:35:16.865945Z",
          "iopub.execute_input": "2024-07-26T17:35:16.866374Z",
          "iopub.status.idle": "2024-07-26T17:35:16.914506Z",
          "shell.execute_reply.started": "2024-07-26T17:35:16.866337Z",
          "shell.execute_reply": "2024-07-26T17:35:16.913297Z"
        },
        "trusted": true
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST config: 1 servers and 1000 clients)\n",
            "CIFAR-10 config: 1 servers and 1000 clients)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "losses, accuracies = train_n_rounds_hybrid(\n",
        "    pool_mnist,\n",
        "    T_final=T_FINAL_MIN * 60,\n",
        "    clients_per_round=math.ceil(len(pool_mnist.clients) * C),\n",
        "    problem=\"mnist\"\n",
        ")\n",
        "plot_loss_accuracy(losses, accuracies, title=\"Learning curves on MNIST\")"
      ],
      "metadata": {
        "id": "orqf2co6Jk2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8fb3a5-92ed-487d-ee9c-8a1860ed9ca2",
        "execution": {
          "iopub.status.busy": "2024-07-26T17:35:16.916123Z",
          "iopub.execute_input": "2024-07-26T17:35:16.916552Z",
          "iopub.status.idle": "2024-07-26T17:42:49.192764Z",
          "shell.execute_reply.started": "2024-07-26T17:35:16.916514Z",
          "shell.execute_reply": "2024-07-26T17:42:49.190803Z"
        },
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running round: 1 - Time elapsed: 0.0 seconds\n",
            "Server requesting clients resources...\n",
            "Server selecting clients to train locally...\n",
            "Selected clients (S): 77 out of 100\n",
            "Server selecting data to train centralized...\n",
            "Number of clients in U: 2\n",
            "Selected data length (|D_UL|): 94080\n",
            "Selected clients for this round: 77\n",
            "Distributing model...\n",
            "Scheduled model update...\n",
            "Centralized model update...\n",
            "Scheduled model upload...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Uploading weights:  77%|███████▋  | 179.1999999999941/232.99038441781985 [03:02<00:54,  1.02s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "losses, accuracies = train_n_rounds_hybrid(\n",
        "    pool_cifar,\n",
        "    T_final=T_FINAL_MIN * 60,\n",
        "    clients_per_round=math.ceil(len(pool_cifar.clients) * C),\n",
        "    problem=\"cifar\"\n",
        ")\n",
        "plot_loss_accuracy(losses, accuracies, title=\"Learning curves on CIFAR-10\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-26T17:35:16.916123Z",
          "iopub.execute_input": "2024-07-26T17:35:16.916552Z",
          "iopub.status.idle": "2024-07-26T17:42:49.192764Z",
          "shell.execute_reply.started": "2024-07-26T17:35:16.916514Z",
          "shell.execute_reply": "2024-07-26T17:42:49.190803Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "hseMqLTyUQm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UwGxIeRpdJd0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}