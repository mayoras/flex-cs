{"metadata":{"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9168646,"sourceType":"datasetVersion","datasetId":5538124},{"sourceId":9174307,"sourceType":"datasetVersion","datasetId":5544443},{"sourceId":9190946,"sourceType":"datasetVersion","datasetId":5554075}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Selección de clientes GreedyFed - Método basado en rendimiento del modelo\n\nEn este notebook vamos a entrenar un modelo de ML utilizando el paradigma de Aprendizaje Federado [1] para los problemas de clasificación de dígitos manuscritos MNIST [2] y clasificación de imágenes a color CIFAR-10 [3]. Usaremos estos dos problemas de visión por computador para la implementación y análisis de rendimiento del método GreedyFed [4] para la selección de clientes.\n\n> [1] https://arxiv.org/abs/1602.05629\n>\n> [2] http://yann.lecun.com/exdb/mnist\n>\n> [3] https://www.cs.toronto.edu/~kriz/cifar.html\n>\n> [4] https://arxiv.org/abs/2312.09108","metadata":{"id":"QzFVvanFrrwe"}},{"cell_type":"code","source":"# install FLEXible framework if not installed\ntry:\n    import flex\n    print(\"FLEXible is installed.\")\nexcept:\n    print(\"FLEXible is not installed.\\nInstalling dependency flexible-fl...\")\n    !pip install flexible-fl","metadata":{"id":"O4BfI6bDrg3o","execution":{"iopub.status.busy":"2024-08-26T16:08:21.365501Z","iopub.execute_input":"2024-08-26T16:08:21.365851Z","iopub.status.idle":"2024-08-26T16:08:21.374439Z","shell.execute_reply.started":"2024-08-26T16:08:21.365823Z","shell.execute_reply":"2024-08-26T16:08:21.373506Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"FLEXible is installed.\n","output_type":"stream"}]},{"cell_type":"code","source":"# install Torchvision if not installed\ntry:\n    import torchvision\n    print(\"Torchvision is installed.\")\nexcept:\n    print(\"FLEXible is not installed.\\nInstalling dependency torchvision...\")\n    !pip install torchvision","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:08:21.381526Z","iopub.execute_input":"2024-08-26T16:08:21.382381Z","iopub.status.idle":"2024-08-26T16:08:21.393343Z","shell.execute_reply.started":"2024-08-26T16:08:21.382344Z","shell.execute_reply":"2024-08-26T16:08:21.392467Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Torchvision is installed.\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_loss_accuracy(loss, accuracy, title=\"Learning Curves\"):\n    # Example data\n    epochs = range(1, len(loss) + 1)\n\n    # Plot loss\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, 'b', label='Loss')\n    plt.title('Loss over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, 'g', label='Accuracy')\n    plt.title(\"Accuracy over Epochs\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.suptitle(title)\n\n    # Show the plots\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"fTScJ4UAS-mB","execution":{"iopub.status.busy":"2024-08-26T16:08:21.396995Z","iopub.execute_input":"2024-08-26T16:08:21.397297Z","iopub.status.idle":"2024-08-26T16:08:21.408363Z","shell.execute_reply.started":"2024-08-26T16:08:21.397272Z","shell.execute_reply":"2024-08-26T16:08:21.407502Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# select device\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n)\ndevice","metadata":{"id":"5jdIu00Ftc1u","execution":{"iopub.status.busy":"2024-08-26T16:08:21.413427Z","iopub.execute_input":"2024-08-26T16:08:21.413779Z","iopub.status.idle":"2024-08-26T16:08:21.429108Z","shell.execute_reply.started":"2024-08-26T16:08:21.413748Z","shell.execute_reply":"2024-08-26T16:08:21.428207Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Cargar datasets - MNIST, CIFAR-10\n","metadata":{"id":"EtC5Foj2FDZy"}},{"cell_type":"markdown","source":"## Distribución de los datos\n\nPara la distribución de los datos preparamos una federación para ambos datasets en el que simulamos clientes __heterogéneos__ en cuestión tanto de clases como del tamaño de sus conjuntos de datos. Nos basamos en la federación que realizan en [1].\n\nPara poder crear un escenario de clientes heterogéneos, suponemos un dataset de $10$ clases que puede ser cualquiera de los que estamos utilizando (MNIST o CIFAR-10). Luego realizamos una federación con la siguiente descripción de [1]: _\"muestrear dos/diez clases para cada cliente para CIFAR-10/CIFAR-100$^1$; Luego, para cada cliente $i$ y una clase seleccionada $c$, muestreamos $\\alpha_{i,c} \\sim U(.4, .6)$, y lo asignamos con $\\frac{\\alpha_{i,c}}{\\sum_j{\\alpha_{j,c}}}$ de los ejemplos para esta clase. Repetimos lo anterior para $10, 50$ y $100$ clientes.\"_ De esta forma, creamos una distribución de datos en el que cada cliente tendrán un número de clases diferente al resto y con diferentes tamaños, formando una distribución no-IID de datos.\n\n> $^1$ En nuestro caso solo será para CIFAR-10 y MNIST, es decir, suponemos siempre $10$ clases.\n>\n> [1] https://arxiv.org/abs/2103.04628.","metadata":{}},{"cell_type":"markdown","source":"## MNIST\nEl primero será MNIST (Modified National Institute of Standards and Technology database), que consiste en imágenes de $28$ pixeles de ancho y alto en escala de grises de dígitos manuscritos del $0$ al $9$. Formalmente, definimos el problema de ML como un problema de clasificación multiclase, donde definimos una imagen de entrada $X_n \\in \\mathcal X$ se define como una matriz $X_{28\\times 28}, x_i \\in \\{1, ..., 255\\}$, donde $x_i$ es un pixel de la imagen $X$. Cada imágen de entrada $X_n\\in \\mathcal X$ es etiquetada con una clase $y_n \\in \\mathcal Y, y = \\{0, 1, ..., 9\\}$ que representa el dígito al que corresponde la imagen. El conjunto de entrenamiento consta de $N=240,000$ imágenes.\n\nEl dataset descargado será el de _Extended MNIST_ [1], que comprende una versión extendida del concepto original de MNIST para proporcionar dígitos y letras manuscritas, una cantidad más grande de datos, diferentes formas de separar los datos (solo dígitos, letras, por clase, ...), etc. Nosotros solo utilizaremos los dígitos para entrenar el clásico MNIST, del cual su versión extendida consta de $N = 280,000$ imágenes de dígitos manuscritos.\n\nPara poder federar EMNIST, debemos de crear una configuración con `FedDatasetConfig` para poder especificar como queremos distribuir los datos y a cuántos nodos queremos federarlo y luego utilizar la clase `FedDataDistribution` para poder crear el dataset federado. Definimos también las transformaciones a realizar a los datos que son simplemente normalizar cada pixel a valores de $[0,1]$ (función `ToTensor()`) y normalizar los valores de cada pixel con una media y desviación estándar de $0.5$.\n\n> [1] https://www.nist.gov/itl/products-and-services/emnist-dataset","metadata":{"id":"kIIPVTjuSl00"}},{"cell_type":"code","source":"import math\nimport numpy as np\n\nfrom copy import deepcopy\nfrom torchvision import datasets, transforms\nfrom flex.data import Dataset, FedDatasetConfig, FedDataDistribution\n\nK_mnist = 300\n\ntrain_data = datasets.MNIST(\n    root='.',\n    train=True,\n    download=True,\n    transform=None  # we apply them later in training process\n)\n\ntest_data = datasets.MNIST(\n    root='.',\n    train=False,\n    download=True,\n    transform=None  # we apply them later in training process\n)\n\nconfig = FedDatasetConfig(seed=33)\nconfig.replacement = False\nconfig.n_nodes = K_mnist\n\n# assign a sample proportion for each node-class pair (https://arxiv.org/abs/2404.06127)\nnum_classes = 10\nalphas = np.random.uniform(low=0.4, high=0.6, size=(config.n_nodes, num_classes))\nalphas = alphas / np.sum(alphas, axis=0)\nconfig.weights_per_label = alphas\n\n# create Federated data distribution of MNIST\nflex_dataset_mnist = FedDataDistribution.from_config(\n    centralized_data=Dataset.from_torchvision_dataset(train_data), config=config\n)\n\n# assign test data to server_id\nserver_id = \"server\"\nflex_dataset_mnist[server_id] = Dataset.from_torchvision_dataset(test_data)\n\n# apply transforms\nmnist_transforms = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n)","metadata":{"id":"CYOy-Bmsu0_J","scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T16:08:21.478091Z","iopub.execute_input":"2024-08-26T16:08:21.478914Z","iopub.status.idle":"2024-08-26T16:08:29.756134Z","shell.execute_reply.started":"2024-08-26T16:08:21.478877Z","shell.execute_reply":"2024-08-26T16:08:29.752345Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m config\u001b[38;5;241m.\u001b[39mweights_per_label \u001b[38;5;241m=\u001b[39m alphas\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# create Federated data distribution of MNIST\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m flex_dataset_mnist \u001b[38;5;241m=\u001b[39m \u001b[43mFedDataDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentralized_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_torchvision_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# assign test data to server_id\u001b[39;00m\n\u001b[1;32m     40\u001b[0m server_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/flex/data/fed_data_distribution.py:250\u001b[0m, in \u001b[0;36mFedDataDistribution.from_config\u001b[0;34m(cls, centralized_data, config)\u001b[0m\n\u001b[1;32m    242\u001b[0m     rng\u001b[38;5;241m.\u001b[39mshuffle(remaining_data_indices)\n\u001b[1;32m    243\u001b[0m keep_y_data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    244\u001b[0m     centralized_data\u001b[38;5;241m.\u001b[39my_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m config_\u001b[38;5;241m.\u001b[39mkeep_labels[i]\n\u001b[1;32m    245\u001b[0m )\n\u001b[1;32m    246\u001b[0m (\n\u001b[1;32m    247\u001b[0m     sub_data_indices,\n\u001b[1;32m    248\u001b[0m     sub_features_indices,\n\u001b[1;32m    249\u001b[0m     remaining_data_indices,\n\u001b[0;32m--> 250\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining_data_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentralized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m X_data \u001b[38;5;241m=\u001b[39m centralized_data\u001b[38;5;241m.\u001b[39mX_data[sub_data_indices]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfeatures_per_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/flex/data/fed_data_distribution.py:344\u001b[0m, in \u001b[0;36mFedDataDistribution.__sample\u001b[0;34m(cls, rng, data_indices, data, labels, config, node_i)\u001b[0m\n\u001b[1;32m    342\u001b[0m sub_features_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__sample_features(rng, data, config, node_i)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Sample data indices\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m sub_data_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sample_with_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_i\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Update remaining data indices\u001b[39;00m\n\u001b[1;32m    348\u001b[0m remaining_data_indices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    349\u001b[0m     data_indices\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mreplacement\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(data_indices) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(sub_data_indices)))\n\u001b[1;32m    352\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/flex/data/fed_data_distribution.py:400\u001b[0m, in \u001b[0;36mFedDataDistribution.__sample_with_weights\u001b[0;34m(cls, rng, data_indices, labels, config, node_i)\u001b[0m\n\u001b[1;32m    398\u001b[0m proportion_per_label \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_labels):\n\u001b[0;32m--> 400\u001b[0m     available_class_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     proportion_per_label[label] \u001b[38;5;241m=\u001b[39m floor(\n\u001b[1;32m    402\u001b[0m         available_class_indices \u001b[38;5;241m*\u001b[39m config\u001b[38;5;241m.\u001b[39mweights_per_label[node_i][j]\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m proportion_per_label:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## CIFAR-10\n\nEl segundo dataset es CIFAR-10 (_Canadian Institute for Advanced Research_)[1], el cual es otro de los datasets junto con MNIST, más utilizados en el campo del Deep Learning y Visión por Computador. CIFAR-10 consiste en una colección de imágenes de $32$ pixeles de altura y de ancho a color ($3$ canales RGB) representando $10$ objetos reales:\n- Avión (airplane)\n- Automóvil (automobile)\n- Pájaro (bird)\n- Gato (cat)\n- Ciervo (deer)\n- Perro (dog)\n- Rana (frog)\n- Caballo (horse)\n- Barco (ship)\n- Camión (truck)\n\nFormalmente, definimos el problema de ML como un problema de clasificación multiclase, donde definimos una imagen de entrada $X_n \\in \\mathcal X$ se define como una matriz $X_{32\\times 32 \\times 3}, x_i \\in \\{1, ..., 255\\}$, donde $x_i^c$ es un pixel de la imagen $X$ en el canal $c$. Cada imágen de entrada $X_n\\in \\mathcal X$ es etiquetada con una clase $y_n \\in \\mathcal Y, y = \\{0, 1, ..., 9\\}$ tal que indexa la lista de clases $C=(c_0=\\text{plane}, c_1=\\text{automobile}, ..., c_9=\\text{truck})$. El conjunto de entrenamiento consta de $N=50,000$ datos de entrada.\n\nRealizamos la misma serie de operaciones que con EMNIST, pero esta vez adaptamos a un número de nodos de $K = 100$ (sin contar el servidor). Definimos también las transformaciones para escalar los valores a $[0,1]$ y normalizar con medias y desviaciones estándar específicas de CIFAR-10 [2].\n\n> [1] https://www.cs.toronto.edu/~kriz/cifar.html\n>\n> [2] https://github.com/kuangliu/pytorch-cifar/issues/19","metadata":{"id":"CW6KW3xiYW3e"}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nfrom flex.data import FedDataDistribution, FedDatasetConfig, Dataset\n\nK_cifar = 100\n\ntrain_data = datasets.CIFAR10(\n    root=\".\",\n    train=True,\n    download=True,\n    transform=None,  # we apply them later in training process\n)\n\ntest_data = datasets.CIFAR10(\n    root=\".\",\n    train=False,\n    download=True,\n    transform=None,  # we apply them later in training process\n)\n\nconfig = FedDatasetConfig(seed=33)\nconfig.replacement = False\nconfig.n_nodes = K_cifar\n\nnum_classes = 10\n\n# assign a sample proportion for each node-class pair\nalphas = np.random.uniform(low=0.4, high=0.6, size=(config.n_nodes, num_classes))\nalphas = alphas / np.sum(alphas, axis=0)\nconfig.weights_per_label = alphas\n\n# create Federated data distribution of CIFAR-10\nflex_dataset_cifar = FedDataDistribution.from_config(\n    centralized_data=Dataset.from_torchvision_dataset(train_data),\n    config=config\n)\n\n# assign test data to server_id\nserver_id = \"server\"\nflex_dataset_cifar[server_id] = Dataset.from_torchvision_dataset(test_data)\n\n# apply transforms\ncifar_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalizar con las medias y desviaciones estándar específicas de CIFAR-10\n])","metadata":{"id":"ErtVS1zPd-Zb","execution":{"iopub.status.busy":"2024-08-26T16:08:29.756850Z","iopub.status.idle":"2024-08-26T16:08:29.757234Z","shell.execute_reply.started":"2024-08-26T16:08:29.757029Z","shell.execute_reply":"2024-08-26T16:08:29.757044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelos de aprendizaje\n\nDefinimos dos modelos de Machine Learning, uno para cada uno de los dos datasets que hemos definido anteriormente. Para ambos problemas, usaremos como función de pérdida o _criterion_ la función de entropía cruzada o _Cross Entropy Loss_ cuya definición [1] la define como (para una muestra $n$):\n\n$$l_n = -w_{y_n} \\cdot \\log \\frac{\\exp (x_n, y_n)}{\\sum_{c=0}^C \\exp (x_n, c)} =-w_{y_n}\\cdot \\log (\\text{Softmax} (x_n, y_n))$$\n\nEn ambos problemas utilizaremos Adam [2] como optimizador o algoritmo de aprendizaje. Utilizaremos los mismos parámetros que en [2] al haber hecho los experimentos sobre modelos similares y con los mismos datasets que estamos usando y que han demostrando buenos resultados: $\\eta = 0.01$ y $\\beta_1 = 0.9,\\beta_2 = 0.999$. Estos parámetros están definidos por defecto en la librería de PyTorch [3].\n\n> [1] https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n>\n> [2] https://arxiv.org/abs/1412.6980\n>\n> [3] https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n\n---\n\n\n","metadata":{"id":"RJLc-rB4gfzS"}},{"cell_type":"markdown","source":"## Perceptrón Multicapa (MNIST)\n\nEl primero, que lo usaremos para ajustar MNIST, será un Perceptrón Multicapa (MLP) de dos capas ocultas de $128$ unidades la primera y $10$ la segunda (correspondiente al número de clases), y usando ReLU [1] como función de activación.\n\nEl modelo inicialmente lo tendrá el servidor y será copiado a cada uno de los clientes. En FLEX usamos el decorador `@init_model_server` para inicializar el modelo en el servidor, donde también podemos además de nuestra arquitectura del modelo, el optimizador y la función de pérdida a usar.\n\n> [1] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)","metadata":{"id":"vYPizwuhm28X"}},{"cell_type":"code","source":"from typing import Tuple\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MLP(nn.Module):\n    \"\"\"\n    Multi-layer Perceptron classifier with two hidden layers.\n    \"\"\"\n    def __init__(self, in_features: Tuple[int, int], hidden_features: int, num_classes: int = 10):\n        super().__init__()\n\n        width, height = in_features\n\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(width * height, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, num_classes)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n\n        return x","metadata":{"id":"wro1kxtkw1w0","execution":{"iopub.status.busy":"2024-08-26T16:08:29.758954Z","iopub.status.idle":"2024-08-26T16:08:29.759327Z","shell.execute_reply.started":"2024-08-26T16:08:29.759134Z","shell.execute_reply":"2024-08-26T16:08:29.759166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Red Neuronal Convolucional (CNN) - CIFAR-10\n\nPara nuestro segundo problema, que consta de ajustar un modelo para clasificar CIFAR-10, utilizaremos un red neuronal convolucional o CNN. A diferencia de MNIST, CIFAR-10 tiene unos datos de entrada con una dimensionalidad más compleja, por lo que utilizar una CNN reduciría la cantidad de parámetros necesarios a entrenar.\n\nUsaremos la red neuronal convolucional utilizada en los experimentos de el algoritmo GreedyFed [1], debido a que es una arquitectura con pocos parámetros a entrenar, lo que nos permite obtener los resultados deseados en poco tiempo. Concretamente, la arquitectura de la red a implementar será la de una CNN estándar que comprende dos capas convolucionales (CONV) $4\\times 4$:\n- Ambas con $8$ canales de salida.\n- Cada uno de ellos activados por ReLU [2].\n- Y aplicando _Max Pooling_ $2\\times 2$ con _stride_ de $2$ en la salida de cada capa convolucional para reducir la dimensionalidad del los mapas de activación.\n\nSeguido de $1$ capa _Fully-Connected_ de $10$ unidades activada por Softmax para la capa de final de salida.\n\n> [1] https://arxiv.org/abs/2312.09108\n>\n> [2] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)","metadata":{"id":"hTQhX95DUm1C"}},{"cell_type":"code","source":"import numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNN(nn.Module):\n    \"\"\"\n    Convolucional Neural Net classifier for CIFAR-10 image recognition (32x32 images).\n\n    Model architecture based on GreedyFed experimental setting:\n    - https://github.com/pringlesinghal/GreedyFed/blob/b2d928670bcd014035830c531bea9f76c57b4b70/model.py#L46\n    \"\"\"\n    def __init__(self, in_channels: int, in_width: int = 32, in_height: int = 32, output_dim: int = 10):\n        super(CNN, self).__init__()\n\n        self.in_channels = in_channels\n        self.input_w = in_width\n        self.input_h = in_height\n        self.output_dim = output_dim\n\n        # First CONV layer\n        out_channels_conv_1 = 8\n        kernel_conv_1_size = 4\n        padding_conv_1 = 1\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels=out_channels_conv_1,\n            kernel_size=kernel_conv_1_size,\n            padding=padding_conv_1,\n        )\n\n        output_w_1, output_h_1 = self.__conv_output_dims(\n            self.input_w, self.input_h, kernel_conv_1_size, padding_conv_1\n        )\n\n        # First MAXPOOL layer\n        kernel_pool_size = 2\n        stride_pool = 2\n        self.pool = nn.MaxPool2d(kernel_size=kernel_pool_size, stride=stride_pool)\n\n        output_w_2, output_h_2 = self.__pool_output_dims(\n            output_w_1, output_h_1, kernel_pool_size, stride_pool\n        )\n\n        # Second CONV layer\n        in_channels_conv_2 = out_channels_conv_1\n        out_channels_conv_2 = 8\n        kernel_conv_2_size = 4\n        padding_conv_2 = 1\n        self.conv2 = nn.Conv2d(\n            in_channels_conv_2,\n            out_channels=out_channels_conv_2,\n            kernel_size=kernel_conv_2_size,\n            padding=padding_conv_2\n        )\n\n        output_w_3, output_h_3 = self.__conv_output_dims(\n            output_w_2, output_h_2, kernel_conv_2_size, padding_conv_2\n        )\n\n        # after second MAXPOOL layer, compute final out width and height\n        output_w_4, output_h_4 = self.__pool_output_dims(\n            output_w_3, output_h_3, kernel_pool_size, stride_pool\n        )\n\n        # Fully-Connected layer\n        self.flatten = nn.Flatten()\n        input_size_4 = int(output_w_4 * output_h_4 * out_channels_conv_2)\n        self.fc = nn.Linear(input_size_4, output_dim)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        # x = self.flatten(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n    def __conv_output_dims(self, width: int, height: int, kernel_size: int, padding: int = 1):\n        output_w = width - kernel_size + 2 * padding + 1\n        output_h = height - kernel_size + 2 * padding + 1\n        return output_w, output_h\n\n    def __pool_output_dims(self, width: int, height: int, kernel_size: int, stride: int = 1):\n        output_w = np.floor((width - kernel_size) / stride) + 1\n        output_h = np.floor((height - kernel_size) / stride) + 1\n\n        return output_w, output_h","metadata":{"id":"5FpzmNDvUp-X","execution":{"iopub.status.busy":"2024-08-26T16:08:29.761065Z","iopub.status.idle":"2024-08-26T16:08:29.761430Z","shell.execute_reply.started":"2024-08-26T16:08:29.761268Z","shell.execute_reply":"2024-08-26T16:08:29.761282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuración básica de escenario de FL\n\nConfiguraremos un escenario de aprendizaje federado centralizado (CFL) usando la librería FLEXible. FLEXible [1], o simplemente FLEX, es una librería de Python que proporciona un framework para la construcción de entornos de aprendizaje federado para fines de investigación y simulación. FLEX pretende dar flexibilidad en cuanto la gran variedad de escenarios y necesidades que se pueden llegar a plantear para experimentar en entornos federados.\n\nUno de nuestros objetivos es _integrar_ un método o técnica de selección de clientes en un escenario inicial de FL. La flexibilidad de FLEX nos permitirá conseguir este objetivo gracias a su flujo de mensajes entre entidades separadas por roles, y por la arquitectura modular de estas que nos permiten almacenar información de manera conveniente [1].\n\n> [1] https://arxiv.org/abs/2404.06127\n---","metadata":{"id":"C3LoDmezyvud"}},{"cell_type":"markdown","source":"### Inicialización del modelo del servidor\n\nEl primer paso de nuestro bucle de entrenamiento en aprendizaje federado es inicializar el modelo FLEX del servidor (que posteriormente se distribuirá a los clientes).\n\nCon FLEX podemos hacer uso del decorador `init_server_model` para facilitarnos esta tarea. Esta función entonces debe de instanciar y devolver un modelo de FLEX en donde además definimos el modelo de aprendizaje $^1$, la función de pérdida, el optimizador y cualquier otra información que se usará en las demás fases de la ronda de entrenamiento.\n\nDefinimos dos funciones que realizan esta tarea, dos para cada una de nuestras arquitecturas, el Perceptrón Multicapa y la Red Neuronal Convolucional. Como habíamos especificado en el planteamiento del problema de ML a resolver, utilizaremos el optimizador SGD-Adam con sus parámetros por defecto, y la Cross-Entropy como función de pérdida.\n\n> $^1$ No se debe confundir el modelo de tipo `FlexModel` con un modelo de ML. El primero implementa el bloque fundamental que define una entidad en el escenario de FL (donde almacenamos información, sus datos locales, su modelo de aprendizaje, ...). El segundo es un modelo aprendizaje como lo puede ser un módulo de PyTorch o Tensorflow que realiza el aprendizaje automático y la predicción sobre los datos. Se dejará claro la diferencia entre los dos tipos de modelos.\n","metadata":{"id":"wOLo8WYFBnwR"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom flex.pool import init_server_model\nfrom flex.pool import FlexPool\nfrom flex.model import FlexModel\n\nmnist_in_features = (28, 28)\nmnist_hidden_features = 128\n\n@init_server_model\ndef build_server_model_mlp():\n    server_flex_model = FlexModel()\n\n    server_flex_model[\"model\"] = MLP(mnist_in_features, mnist_hidden_features)\n\n    # Required to store this for later stages of the FL training process\n    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n    server_flex_model[\"optimizer_kwargs\"] = {}\n    return server_flex_model\n\ncifar_in_channels = 3\ncifar_num_classes = 10\n\n@init_server_model\ndef build_server_model_cnn():\n    server_flex_model = FlexModel()\n\n    server_flex_model[\"model\"] = CNN(in_channels=cifar_in_channels, output_dim=cifar_num_classes)\n\n    # Required to store this for later stages of the FL training process\n    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n    server_flex_model[\"optimizer_kwargs\"] = {}\n    return server_flex_model","metadata":{"id":"C9SWwbzBU9hD","execution":{"iopub.status.busy":"2024-08-26T16:08:29.763128Z","iopub.status.idle":"2024-08-26T16:08:29.763565Z","shell.execute_reply.started":"2024-08-26T16:08:29.763376Z","shell.execute_reply":"2024-08-26T16:08:29.763393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### Distribuir modelo del servidor\n\nEl siguiente paso a realizar en un flujo de entrenamiento de aprendizaje federado es la distribución del modelo del servidor a los cientes. Con FLEX, podemos utilizar el decorador `@deploy_server_model` para distribuir el modelo del servidor a los clientes, definiendo una función que devuelva el modelo a almacenar en cada cliente.\n\nEn este caso, realizamos una copia profunda del modelo del servidor que será asignado a cada cliente.\n","metadata":{"id":"R5LzYqxUzIaU"}},{"cell_type":"code","source":"from flex.pool import deploy_server_model\nimport copy\n\n\n@deploy_server_model\ndef copy_server_model_to_clients(server_flex_model: FlexModel):\n    return copy.deepcopy(server_flex_model)","metadata":{"id":"UwxXBCoCytZh","execution":{"iopub.status.busy":"2024-08-26T16:08:29.765033Z","iopub.status.idle":"2024-08-26T16:08:29.765376Z","shell.execute_reply.started":"2024-08-26T16:08:29.765210Z","shell.execute_reply":"2024-08-26T16:08:29.765229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Actualización del modelo del lado del cliente\n\nDefinimos la función encargada de realizar el entrenamiento del modelo sobre los datos locales del cliente. A esta función, es conveniente pasarle como parámetros los hiperparámetros de entrenamiento de un modelo de ML convencional como el número de épocas $E$ y el tamaño de _batch_ $B$","metadata":{"id":"U1S01aEFzevA"}},{"cell_type":"code","source":"from flex.data import Dataset\nfrom torch.utils.data import DataLoader\n\n\ndef train(client_flex_model: FlexModel, client_data: Dataset, **kwargs):\n    # parse kwargs\n    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 5\n    batch_size = kwargs[\"batch_size\"] if \"batch_size\" in kwargs else 20\n    transforms = kwargs[\"transforms\"] if \"transforms\" in kwargs else None\n\n    # get client data as a torchvision object\n    train_dataset = client_data.to_torchvision_dataset(transform=transforms)\n    client_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n\n    # get model\n    model = client_flex_model[\"model\"]\n    optimizer = client_flex_model[\"optimizer_func\"](\n        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n    )\n    model = model.train()\n    model = model.to(device)\n    criterion = client_flex_model[\"criterion\"]\n\n    # train model\n    for _ in range(epochs):\n        for imgs, labels in client_dataloader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            pred = model(imgs)\n            loss = criterion(pred, labels)\n            loss.backward()\n            optimizer.step()","metadata":{"id":"CrWryr7HVNKt","execution":{"iopub.status.busy":"2024-08-26T16:08:29.766568Z","iopub.status.idle":"2024-08-26T16:08:29.766895Z","shell.execute_reply.started":"2024-08-26T16:08:29.766737Z","shell.execute_reply":"2024-08-26T16:08:29.766751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Obtener los parámetros de los clientes\n\nAhora implementamos la función que hace que el servidor (con rol de agregador) recupere los nuevos parámetros actualizados de los clientes. Con el decorador `@collect_clients_weights` recuperamos los pesos de PyTorch de cada cliente seleccionado para esa ronda. En el caso de PyTorch, el modelo devuelve los pesos en forma de un diccionario con `state_dict` para el que cada nombre representa una capa de la red y sus parámetros, lo que hacemos será devolver una lista con los valores de ese diccionario correspondientes a los pesos de la red entera.","metadata":{"id":"dMSabAvG0u2J"}},{"cell_type":"code","source":"from flex.pool import collect_clients_weights\n\n\n@collect_clients_weights\ndef get_clients_weights(client_flex_model: FlexModel):\n    weight_dict = client_flex_model[\"model\"].state_dict()\n    return [weight_dict[name] for name in weight_dict]","metadata":{"id":"ggbUniMr0oGy","execution":{"iopub.status.busy":"2024-08-26T16:08:29.768043Z","iopub.status.idle":"2024-08-26T16:08:29.768383Z","shell.execute_reply.started":"2024-08-26T16:08:29.768217Z","shell.execute_reply":"2024-08-26T16:08:29.768237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Agregación de los parámetros\n\nEl servidor/agregador agrega entonces estos nuevos parámetros para conseguir el nuevo modelo global. Utilizamos el decorador `@aggregate_weights` para poder agregar los pesos que hemos recuperado de los clientes en la fase anterior computando la media de los pesos de manera uniforme, conocido como agregador FedAvg [1], donde realizamos la media por columnas para cada capa de pesos.\n\n> [1] https://arxiv.org/abs/1602.05629","metadata":{"id":"1DT76cJ41uFY"}},{"cell_type":"code","source":"from flex.pool import aggregate_weights\nimport tensorly as tl\n\ntl.set_backend(\"pytorch\")\n\n\n@aggregate_weights\ndef aggregate_with_fedavg(list_of_weights: list):\n    agg_weights = []\n    for layer_index in range(len(list_of_weights[0])):\n        weights_per_layer = [weights[layer_index] for weights in list_of_weights]\n        weights_per_layer = tl.stack(weights_per_layer)\n        agg_layer = tl.mean(weights_per_layer, axis=0)\n        agg_weights.append(agg_layer)\n    return agg_weights","metadata":{"id":"YR27tyd51cfq","execution":{"iopub.status.busy":"2024-08-26T16:08:29.769663Z","iopub.status.idle":"2024-08-26T16:08:29.769986Z","shell.execute_reply.started":"2024-08-26T16:08:29.769827Z","shell.execute_reply":"2024-08-26T16:08:29.769841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finalmente, agregamos los pesos al modelo de nuestro servidor/agregador. Sencillamente, para cada capa de nuestro modelo, realizamo una copia del nuevo que hemos agregado en la fase anterior.","metadata":{"id":"YQtCzF0K2i92"}},{"cell_type":"code","source":"from flex.pool import set_aggregated_weights\n\n\n@set_aggregated_weights\ndef set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n    with torch.no_grad():\n        weight_dict = server_flex_model[\"model\"].state_dict()\n        for layer_key, new in zip(weight_dict, aggregated_weights):\n            weight_dict[layer_key].copy_(new)","metadata":{"id":"6Fk-VxE52ZNH","execution":{"iopub.status.busy":"2024-08-26T16:08:29.771342Z","iopub.status.idle":"2024-08-26T16:08:29.771712Z","shell.execute_reply.started":"2024-08-26T16:08:29.771522Z","shell.execute_reply":"2024-08-26T16:08:29.771537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluación del modelo global\n\nPodemos evaluar el modelo del servidor sobre el dataset de test que hemos definido anteriormente que residía en el mismo servidor. Para ello, definimos una función `evaluate_global_model` que obtenga las predicciones del modelo con el dataset de test y devuelva las metricas resultantes, que en este caso son simplemente la pérdida y la _accuracy_.","metadata":{"id":"NB5N8y2a27JU"}},{"cell_type":"code","source":"def evaluate_global_model(server_flex_model: FlexModel, test_data: Dataset):\n    model = server_flex_model[\"model\"]\n    model.eval()\n    test_loss = 0\n    test_acc = 0\n    total_count = 0\n    model = model.to(device)\n    criterion = server_flex_model[\"criterion\"]\n    # get test data as a torchvision object\n    test_dataset = test_data.to_torchvision_dataset(transform=mnist_transforms)\n    test_dataloader = DataLoader(\n        test_dataset, batch_size=256, shuffle=True, pin_memory=False\n    )\n    losses = []\n    with torch.no_grad():\n        for data, target in test_dataloader:\n            total_count += target.size(0)\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            losses.append(criterion(output, target).item())\n            pred = output.data.max(1, keepdim=True)[1]\n            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n\n    test_loss = sum(losses) / len(losses)\n    test_acc /= total_count\n    return test_loss, test_acc","metadata":{"id":"AFZA3_Sn20tn","execution":{"iopub.status.busy":"2024-08-26T16:08:29.773060Z","iopub.status.idle":"2024-08-26T16:08:29.773479Z","shell.execute_reply.started":"2024-08-26T16:08:29.773296Z","shell.execute_reply":"2024-08-26T16:08:29.773312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training loop\n\nA continuación se muestra el bucle de entrenamiento aplicando todas las fases que hemos implementado anteriormente. En esta versión básica se implementa una selección aleatoria (RandomSampling _baseline_), similar a la propuesta en [1] donde se selecciona de manera aleatoria y uniforme $M$ clientes para la ronda actual. Este proceso se repite de forma iterativa un número determinado de rondas.","metadata":{"id":"4p233PYL9Lum"}},{"cell_type":"code","source":"from typing import Literal\n\nProblem = Literal[\"mnist\", \"cifar\"]\n\ndef train_n_rounds(pool: FlexPool, n_rounds: int, clients_per_round=20, problem: Problem = \"mnist\"):\n    \"\"\"\n    FL training loop for a certain number of rounds and clients selected.\n    \"\"\"\n    # select transformations depending on problem to solve\n    transforms = mnist_transforms if problem == \"mnist\" else cifar_transforms\n\n    losses = []\n    accuracies = []\n    for i in range(n_rounds):\n        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n        selected_clients_pool = pool.clients.select(clients_per_round)\n        selected_clients = selected_clients_pool.clients\n        print(f\"Selected clients for this round: {len(selected_clients)}\")\n        # Deploy the server model to the selected clients\n        pool.servers.map(copy_server_model_to_clients, selected_clients)\n        # Each selected client trains her model\n        selected_clients.map(train, transforms=transforms)\n        # The aggregador collects weights from the selected clients and aggregates them\n        pool.aggregators.map(get_clients_weights, selected_clients)\n        pool.aggregators.map(aggregate_with_fedavg)\n        # The aggregator send its aggregated weights to the server\n        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n        metrics = pool.servers.map(evaluate_global_model)\n        loss, acc = metrics[0]\n        losses.append(loss)\n        accuracies.append(acc)\n        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}\")\n\n    return losses, accuracies","metadata":{"id":"YVv7hlVaVg5y","execution":{"iopub.status.busy":"2024-08-26T16:08:29.774985Z","iopub.status.idle":"2024-08-26T16:08:29.775379Z","shell.execute_reply.started":"2024-08-26T16:08:29.775188Z","shell.execute_reply":"2024-08-26T16:08:29.775205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seleccionando clientes con GreedyFed\n\nAhora adaptamos nuestro entorno de Aprendizaje Federado para seleccionar clientes con GreedyFed utilizando las funciones de FLEX. Para ello, vamos a preparar el entorno antes de pasar directamente con el algoritmo de selección.\n\nEn primer lugar, GreedyFed [1] define un conjunto de datos de validación que utiliza el algoritmo de aproximación de valores de Shapley GTG-Shapley [2] para poder calcular la contribución marginal de los $M$ clientes seleccionados en la ronda actual. Para esto, el algoritmo GTG-Shapley necesita de un conjunto de validación $\\mathcal D_{\\text{val}}$ para poder calcular esta contribución. Para ello, separaremos el dataset de entrenamiento en dos subconjuntos de datos: uno de entrenamieto y otro de validación, siendo el tamaño del de validación un $10\\%$ de los datos totales de entrenamiento aleatoriamente separados, para lo cual contaríamos con $6000$ ejemplos en MNIST y $5000$ ejemplo en CIFAR-10.\n\n$$\n\\mathcal D_\\text{T} = \\mathcal D_\\text{train} \\cup \\mathcal D_\\text{val}\\\\\n\\begin{cases}\nN &= |\\mathcal D_\\text{T}|,\\\\\nN_\\text{val} &= \\lfloor 0.1 \\times N\\rfloor,\\\\\nN_\\text{train} &= N-N_\\text{val}\n\\end{cases}\n$$\n\nEl algoritmo GTG-Shapley es un algoritmo de naturaleza combinatoria, donde en cada ronda sufre una complejidad en tiempo de $\\mathcal O(M \\log M)$ donde $M$ es el número de clientes seleccionados por ronda (i.e. el _presupuesto_ de clientes seleccionados). Por tanto, la distribución de los datos en este esquema de selección tendrá un número total de clientes $K$ más pequeño que en otros métodos, por lo que establecemos un número de $K=300$ nodos sin contar el servidor central para MNIST y $K=100$ para CIFAR-10 que son valores muy similares usados por los autores de [1].\n\n### Separar conjunto de validación $D_\\text{val}$\n\nDicho lo anterior, federamos de nuevo MNIST para poder definir los parámetros comentados así como en el caso de CIFAR-10.\n\n> [1] https://arxiv.org/abs/2312.09108\n>\n> [2] https://arxiv.org/abs/2109.02053","metadata":{"id":"4L2wrteiiFb_"}},{"cell_type":"code","source":"import math\nfrom copy import deepcopy\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import random_split\n\nfrom flex.data import Dataset, FedDatasetConfig, FedDataDistribution\n\nK_mnist = 300\n\ntrain_data = datasets.MNIST(\n    root='.',\n    train=True,\n    download=True,\n    transform=None  # we apply them later in training process\n)\n\ntest_data = datasets.MNIST(\n    root='.',\n    train=False,\n    download=True,\n    transform=None  # we apply them later in training process\n)\n\n# split test data into training and validation datasets (val data ~ 10% of train data)\nval_len = math.floor(0.1 * len(train_data))\ntrain_len = len(train_data) - val_len\n\ntrain_data, val_data = random_split(train_data, [train_len, val_len])\n\nnum_classes = 10\n\nconfig = FedDatasetConfig(seed=33)\nconfig.replacement = False\nconfig.n_nodes = K_mnist\n\n# assign a sample proportion for each node-class pair\nalphas = np.random.uniform(low=0.4, high=0.6, size=(config.n_nodes, num_classes))\nalphas = alphas / np.sum(alphas, axis=0)\nconfig.weights_per_label = alphas\n\nflex_dataset_mnist = FedDataDistribution.from_config(\n    centralized_data=Dataset.from_torchvision_dataset(train_data), config=config\n)\n\n# assign test data to server_id\nserver_id = \"server\"\nflex_dataset_mnist[server_id] = Dataset.from_torchvision_dataset(test_data)\nmnist_val_data = deepcopy(val_data)","metadata":{"id":"Ga_F2xKqpMY7","scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T16:08:29.777406Z","iopub.status.idle":"2024-08-26T16:08:29.777776Z","shell.execute_reply.started":"2024-08-26T16:08:29.777594Z","shell.execute_reply":"2024-08-26T16:08:29.777611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K_cifar = 100\n\ntrain_data = datasets.CIFAR10(\n    root='.',\n    train=True,\n    download=True,\n    transform=None  # we apply them later in training process\n)\n\ntest_data = datasets.CIFAR10(\n    root='.',\n    train=False,\n    download=True,\n    transform=None  # we apply them later in training process\n)\n\nnum_classes = 10\n\n# split train data into training and validation datasets (val data ~ 10% of train data)\nval_len = math.floor(0.1 * len(train_data))\ntrain_len = len(train_data) - val_len\n\ntrain_data, val_data = random_split(train_data, [train_len, val_len])\n\nconfig = FedDatasetConfig(seed=33)\nconfig.replacement = False\nconfig.n_nodes = K_cifar\n\n# assign a sample proportion for each node-class pair\nalphas = np.random.uniform(low=0.4, high=0.6, size=(config.n_nodes, num_classes))\nalphas = alphas / np.sum(alphas, axis=0)\nconfig.weights_per_label = alphas\n\nflex_dataset_cifar = FedDataDistribution.from_config(\n    centralized_data=Dataset.from_torchvision_dataset(train_data), config=config\n)\n\n# assign test data to server_id\nserver_id = \"server\"\nflex_dataset_cifar[server_id] = Dataset.from_torchvision_dataset(test_data)\ncifar_val_data = deepcopy(val_data)","metadata":{"id":"4CUa-A4piFcA","execution":{"iopub.status.busy":"2024-08-26T16:08:29.781337Z","iopub.status.idle":"2024-08-26T16:08:29.781729Z","shell.execute_reply.started":"2024-08-26T16:08:29.781547Z","shell.execute_reply":"2024-08-26T16:08:29.781563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Asignar $\\mathcal D_\\text{val}$ a los agregadores","metadata":{"id":"EVQR6hq6iFcA"}},{"cell_type":"markdown","source":"Hemos podido separar el dataset de entrenamiento en uno de training y otro de validación. Ahora debemos de crear la función que almacene este conjunto en el modelo del servidor $^1$ para posteriormente usarlo al ejecutar el algoritmo GTG-Shapley. Creamos, por tanto, una función `save_validation_data` que guardará el conjunto de validación que hemos creado en los modelos de los agregadores.\n\n> $^1$ Recordemos que en la arquitectura en la que trabajamos, los servidores tienen los roles de _servers_ y _aggregators_ por lo que se usan ambos nombres indistintamente para referirse al servidor.","metadata":{"id":"DT8GofR6iFcA"}},{"cell_type":"code","source":"from copy import deepcopy\n\ndef save_validation_data(agg_flex_model: FlexModel, _test_data: Dataset, **kwargs):\n    agg_flex_model[\"val_data\"] = deepcopy(kwargs[\"val_data\"])","metadata":{"id":"BFO7Bgj0iFcB","execution":{"iopub.status.busy":"2024-08-26T16:08:29.783523Z","iopub.status.idle":"2024-08-26T16:08:29.783909Z","shell.execute_reply.started":"2024-08-26T16:08:29.783721Z","shell.execute_reply":"2024-08-26T16:08:29.783737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Función de utilidad $\\mathcal U(w)$\n\nEl algoritmo de aproximación de valores Shapley GTG-Shapley define una función $\\mathcal U(\\text w) := -\\mathcal L(\\text w;\\mathcal D_\\text{val})$ que estima la utilidad de un modelo $\\text w$ sobre el conjunto de validación $\\mathcal D_\\text{val}$ negando el valor de pérdida sobre este último. De esta manera, se puede dar una valuación a un modelo agregado que es creciente cuanto mejor predice el conjunto de validación, lo que será útil para tener una métrica que mide que tan positiva es la contribución marginal de un cliente concreto $k \\in S_t$.\n\nEn este caso, esta función de utilidad la usará el algoritmo GTG-Shapley, por lo que definimos una función `utility` al cual le pasamos como parámetro un modelo de aprendizaje (como lo es un módulo de NN de PyTorch), el dataset de validación y la función de pérdida.","metadata":{"id":"3Pe6p0qsiFcB"}},{"cell_type":"code","source":"from torchvision.transforms import Compose\nfrom typing import Callable, Optional\n\ndef utility(model: nn.Module, val_data: Dataset, criterion: Callable, transforms: Optional[Compose]):\n    \"\"\"\n    Computes the negative loss of a model over a validation dataset.\n\n    Parameters:\n    ----------\n        - model: Model to calculate utility.\n        - val_data: Dataset on which utility is going to be computed.\n        - criterion: loss function of the problem.\n        - transforms: data transform functions.\n\n    Returns:\n        - Utility of the model over the validation dataset as the negative loss.\n    \"\"\"\n    model.eval()\n    val_loss = 0\n    val_acc = 0\n\n    model = model.to(device)\n\n    # get test data as a torchvision object\n    val_dataset = val_data.to_torchvision_dataset(transform=transforms)\n    val_dataloader = DataLoader(\n        val_dataset, batch_size=256, shuffle=True, pin_memory=True\n    )\n\n    losses = []\n    with torch.no_grad():\n        for data, target in val_dataloader:\n            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n            output = model(data)\n\n            losses.append(criterion(output, target).item())\n\n    # compute average loss\n    val_loss = sum(losses) / len(losses) if losses else 0\n\n    return 1 - val_loss","metadata":{"id":"A1Y47s2ZiFcB","execution":{"iopub.status.busy":"2024-08-26T16:08:29.785425Z","iopub.status.idle":"2024-08-26T16:08:29.786004Z","shell.execute_reply.started":"2024-08-26T16:08:29.785627Z","shell.execute_reply":"2024-08-26T16:08:29.785643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test de convergencia de valores SV\n\nEn el algoritmo GTG-Shapley, se computan los valores de Shapley (contribución marginal media) para cada cliente $k \\in S_t$ sobre un número de determinado de iteraciones o hasta que los valores converjan computando el cambio medio en un número de iteraciones determinado. Definimos por tanto una función para comprobar si los valores de Shapley de un cliente han convergido en un número de mínimo $20$ iteraciones y para un umbral inferior de $1\\%$ [1].\n\n> [1] Código extraído del código hecho por los autores originales: [https://github.com/pringlesinghal/GreedyFed/utils.py](https://github.com/pringlesinghal/GreedyFed/blob/b2d928670bcd014035830c531bea9f76c57b4b70/utils.py#L7).","metadata":{"id":"rAPCBjV0iFcB"}},{"cell_type":"code","source":"def convergenceTest(values):\n    \"\"\"\n    Compute average change in last 20 iterations and if it less that 1% it has converged\n    \"\"\"\n    if len(values) < 20:\n        return False\n    else:\n        last_vals = torch.Tensor(values[-20:]).to(device)\n        last_val = torch.Tensor([values[-1]]).to(device)\n\n        # avoid division by zero\n        if last_val < 1e-6:\n            return False\n\n        return (\n            torch.mean(\n                torch.abs(last_vals - last_val)\n            )\n            / torch.abs(last_val)\n        ) < 0.01","metadata":{"id":"eQ64zRChiFcC","execution":{"iopub.status.busy":"2024-08-26T16:08:29.787486Z","iopub.status.idle":"2024-08-26T16:08:29.787862Z","shell.execute_reply.started":"2024-08-26T16:08:29.787678Z","shell.execute_reply":"2024-08-26T16:08:29.787694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Algoritmo GTG-Shapley\n\nImplementamos el algoritmo de aproximación de valores de Shapley GTG-Shapley [1]. Este algoritmo depende de la función de agregación $F$ del servidor, para lo que definimos además una función que obtenga el modelo agregado dado una lista de modelos de los clientes.\n\nLos hiperparámetros de este algoritmo son por un lado el máximo de iteraciones $T$ para cada cliente y el umbral de error $ϵ$ para considerar la contribución al modelo agregado (si la contribución es menor que $\\epsilon$ los valores de Shapley $SV_k$ son directamente $0$). En nuestro caso, daremos $T=20 \\times M$ iteraciones ($20$ por cliente seleccionado) y un umbral $\\epsilon=10^{-4}$.\n\n> [1] https://arxiv.org/abs/2109.02053.","metadata":{"id":"wPrq0iRsiFcC"}},{"cell_type":"code","source":"import torch.nn as nn\nfrom copy import deepcopy\nfrom typing import List\n\ndef _aggregate(weights: List):\n    \"\"\"Aggregate with FedAvg given a list of weights.\"\"\"\n    agg_weights = []\n    for layer_index in range(len(weights[0])):\n        weights_per_layer = [weights[layer_index] for weights in weights]\n        weights_per_layer = tl.stack(weights_per_layer)\n        agg_layer = tl.mean(weights_per_layer, axis=0)\n        agg_weights.append(agg_layer)\n    return agg_weights\n\ndef get_aggregated_model(server_model: nn.Module, client_models: List[nn.Module]):\n    client_model_weights =  []\n    for k in client_models:\n        weight_dict = k.state_dict()\n        client_model_weights.append([weight_dict[name] for name in weight_dict])\n\n    # get aggregated weights\n    aggregated_weights = _aggregate(client_model_weights)\n\n    # copy original model\n    agg_model = deepcopy(server_model)\n\n    # set aggregated weights\n    with torch.no_grad():\n        weight_dict = agg_model.state_dict()\n        for layer_key, new in zip(weight_dict, aggregated_weights):\n            weight_dict[layer_key].copy_(new)\n\n    return agg_model","metadata":{"id":"ELLvELWwiFcC","execution":{"iopub.status.busy":"2024-08-26T16:08:29.790315Z","iopub.status.idle":"2024-08-26T16:08:29.790706Z","shell.execute_reply.started":"2024-08-26T16:08:29.790520Z","shell.execute_reply":"2024-08-26T16:08:29.790536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\n\nfrom typing import List\n\n# experimental settings of GTG-Shapley: https://arxiv.org/abs/2312.09108\nMAX_ITERATIONS = 30\nEPSILON = 1e-4\n\ndef shapley_values_gtg(agg_model: FlexModel, client_flex_models: List[FlexModel]):\n    \"\"\"\n    Computes shapley values for the client updates on validation dataset.\n    \"\"\"\n    start = time.time()\n    # get arguments\n    criterion = agg_model[\"criterion\"]\n    d_val = agg_model[\"val_data\"]\n    current_model = agg_model[\"model\"]\n    transforms = agg_model[\"transforms\"]\n\n    num_clients = len(client_flex_models)\n\n    shapley_values = [[0] for i in range(num_clients)]\n    converged = False\n\n    T = MAX_ITERATIONS * num_clients\n    t = 0\n    threshold = EPSILON\n\n    # initial server model t loss\n    v_init = utility(current_model, d_val, criterion, transforms)\n\n    # final model t+1 loss\n    model_final = get_aggregated_model(\n        current_model,\n        [client_flex_models[k][\"model\"] for k in client_flex_models]\n    )\n    v_final = utility(model_final, d_val, criterion, transforms)\n\n    # if difference in loss on aggregated model is lower than threshold, SVs are 0 (contribution is negligible)\n    if np.abs(v_final - v_init) < threshold:\n        epsilon = 1e-9\n        return [epsilon for i in range(num_clients)]\n\n    # id-independent list of client models\n    client_models = list(client_flex_models.values())\n\n    while not converged and t < T:\n        for k in range(num_clients):\n            t += 1\n\n            # get all possible combinations of selected clients having k in idx=0\n            client_permutation = np.concatenate(\n                (\n                    np.array([k]),\n                    np.random.permutation(\n                        [i for i in range(num_clients) if i != k]\n                    )\n                )\n            ).astype(int)\n\n            v_j = v_init\n\n            for j in range(num_clients):\n                if np.abs(v_final - v_j) < threshold:\n                    v_jp1 = v_j\n                else:\n                    subset = client_permutation[: (j + 1)]\n                    client_models_subset = [client_models[i][\"model\"] for i in subset]\n\n                    # aggregate the subset-clients\n                    model_subset = get_aggregated_model(current_model, client_models_subset)\n                    v_jp1 = utility(model_subset, d_val, criterion, transforms)\n\n                phi_old = shapley_values[client_permutation[j]][-1]\n                phi_new = ((t - 1) * phi_old + (v_jp1 - v_j)) / t\n                shapley_values[client_permutation[j]].append(phi_new)\n                v_j = v_jp1\n\n        # check if SV values converged\n        shapley_avg = np.mean(shapley_values, axis=0)\n        converged = convergenceTest(shapley_avg)\n\n    if not converged:\n        print(\"not converged in SV GTG\")\n\n    final_shapley_values = [shapley_values[i][-1] for i in range(num_clients)]\n\n    end = time.time()\n    print(f\"GTG-Shapley lasted: {end - start} seconds to complete\")\n\n    return final_shapley_values","metadata":{"id":"nd0Z2G66iFcC","execution":{"iopub.status.busy":"2024-08-26T16:08:29.792244Z","iopub.status.idle":"2024-08-26T16:08:29.792698Z","shell.execute_reply.started":"2024-08-26T16:08:29.792504Z","shell.execute_reply":"2024-08-26T16:08:29.792526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop final\n\nFinalmente, redefinimos la función de bucle iterativo de aprendizaje donde se siguen los siguientes pasos definidos en [1]:\n\n1. Asigna el conjunto de validación al servidor.\n2. Se seleccionan los clientes por Round-Robin (RR) para las primeras épocas de inicialización de valores Shapley, o seleccionando por criterio Greedy (elegir los clientes con SV más grandes).\n3. Se actualiza el modelo por los clientes seleccionados.\n4. Se calculan los valores de Shapley de cada cliente $k\\in S_t$.\n5. Se computa los valores de Shapley cumulativos de cada cliente $k\\in S_t$ (valores UCB).\n\n> [1] https://arxiv.org/abs/2312.09108","metadata":{"id":"xVTNG0iwiFcD"}},{"cell_type":"code","source":"import time\nfrom torch import topk\n\nfrom flex.data import Dataset\n\ndef train_n_rounds(pool: FlexModel, n_rounds: int, clients_per_round=20, problem: Problem = \"mnist\"):\n    \"\"\"\n    FL training loop for a certain number of rounds and clients selected.\n    \"\"\"\n    # select transformations depending on problem to solve and assign to server\n    transforms = mnist_transforms if problem == \"mnist\" else cifar_transforms\n\n    def set_transforms(server_model, _):\n        server_model[\"transforms\"] = transforms\n    pool.servers.map(set_transforms)\n\n    # assign validation dataset to server\n    val_data = mnist_val_data if problem == \"mnist\" else cifar_val_data\n    flex_val_data = Dataset.from_torchvision_dataset(val_data)\n    pool.servers.map(save_validation_data, val_data=flex_val_data)\n\n    num_clients = len(pool.clients)\n\n    UCB = [0 for i in range(num_clients)]\n    SV = [0 for i in range(num_clients)]\n    SV_curr = [0 for i in range(num_clients)]\n    N_t = [0 for i in range(num_clients)]\n\n    losses = []\n    accuracies = []\n    elapsed_round_times = []\n    for i in range(n_rounds):\n        start = time.time()\n        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n\n        # select clients to transmit weights to\n        # initially sample every client atleast once (RR)\n        selected = [False for _ in range(num_clients)]\n        if i < np.floor(num_clients / clients_per_round):\n            print(\"RR round\")\n            for idx in range(i * clients_per_round, (i + 1) * clients_per_round):\n                selected[idx] = True\n                N_t[idx] += 1\n        elif i == np.floor(num_clients / clients_per_round):\n            print(\"Last RR round\")\n            for idx in range(i * clients_per_round, num_clients):\n                selected[idx] = True\n                N_t[idx] += 1\n\n            remaining_selections = clients_per_round * (i + 1) - num_clients\n            if remaining_selections > 0:\n                unselected_indices = list(range(0, i * clients_per_round))\n                selected_indices_subset = np.random.choice(\n                    unselected_indices, size=remaining_selections, replace=False\n                )\n\n                for idx in selected_indices_subset:\n                    selected[idx] = True\n                    N_t[idx] += 1\n        else:\n            # UCB Greedy Selection (select clients with highest SV)\n            _, selected_indices = topk(torch.Tensor(UCB), clients_per_round)\n            print(\"Best selected: \", selected_indices)\n            for idx in selected_indices.tolist():\n                selected[idx] = True\n                N_t[idx] += 1\n\n        # take selected clients\n        selected_indices = [index for index, value in enumerate(selected) if value]\n        selected_clients_pool = pool.clients.select(lambda actor_id, actor_roles: actor_id in selected_indices)\n        selected_clients = selected_clients_pool.clients\n\n        print(f\"Selected clients for this round: {len(selected_clients)}\")\n\n        # Deploy the server model to the selected clients\n        pool.servers.map(copy_server_model_to_clients, selected_clients)\n\n        # Each selected client trains its model\n        selected_clients.map(train, transforms=transforms)\n\n        # The aggregador collects weights from the selected clients and aggregates them\n        pool.aggregators.map(get_clients_weights, selected_clients)\n\n        # The aggregator aggregates client updates with FedAveraging\n        pool.aggregators.map(aggregate_with_fedavg)\n\n        # compute SV values for each selected client in the server\n        shapley_values = pool.aggregators.map(shapley_values_gtg, selected_clients)[0]\n\n        print(\"GTG-Shapley values: \", shapley_values)\n\n        # The aggregator send its aggregated weights to the server\n        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n\n        # compute the cumulative SV for each client\n        counter = 0\n        for k in range(num_clients):\n            if selected[k]:\n                SV_curr[k] = shapley_values[counter]\n\n                # previous num selections weight/penalty\n                prev_wt = (N_t[k] - 1) / N_t[k]\n                # current num selections weight/penalty\n                curr_wt = 1 - prev_wt   # 1 / N_t[k]\n\n                # previous cumulative SV\n                prev_sv = SV[k]\n\n                # t-round SV\n                curr_sv = SV_curr[k]\n\n                # compute new cumulative SV\n                SV[k] = prev_wt * prev_sv + curr_wt * curr_sv\n\n                counter += 1\n            else:\n                SV_curr[k] = 0\n\n            UCB[k] = SV[k]\n\n        metrics = pool.servers.map(evaluate_global_model)\n        loss, acc = metrics[0]\n\n        end = time.time()\n\n        losses.append(loss)\n        accuracies.append(acc)\n        elapsed_round_times.append(end - start)\n        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}, round time: {end - start} sec\")\n\n    return losses, accuracies, elapsed_round_times","metadata":{"id":"GhuOh3p9iFcD","execution":{"iopub.status.busy":"2024-08-26T16:08:29.794759Z","iopub.status.idle":"2024-08-26T16:08:29.795133Z","shell.execute_reply.started":"2024-08-26T16:08:29.794951Z","shell.execute_reply":"2024-08-26T16:08:29.794967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport pickle\n\ndef save_data(pool, losses, accuracies, elapsed, filename):\n    df = pd.DataFrame({\n        \"loss\": losses,\n        \"accuracy\": accuracies,\n        \"elapsed_time\": elapsed\n    })\n\n    # save data for MNIST\n    df.to_csv(f\"{filename}.csv\", index=False)\n\n    display(df)\n\n    # save model\n    with open(f\"{filename}_model.pkl\", \"wb\") as out:\n        pickle.dump(pool, out)","metadata":{"id":"Ad-STpCNYaJM","execution":{"iopub.status.busy":"2024-08-26T16:08:29.796700Z","iopub.status.idle":"2024-08-26T16:08:29.797074Z","shell.execute_reply.started":"2024-08-26T16:08:29.796892Z","shell.execute_reply":"2024-08-26T16:08:29.796908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiate pools and constants\nMNIST_ROUNDS = 150\nCIFAR_ROUNDS = 100\n\npool_mnist = FlexPool.client_server_pool(flex_dataset_mnist, init_func=build_server_model_mlp)\npool_cifar = FlexPool.client_server_pool(flex_dataset_cifar, init_func=build_server_model_cnn)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:08:29.798470Z","iopub.status.idle":"2024-08-26T16:08:29.798852Z","shell.execute_reply.started":"2024-08-26T16:08:29.798670Z","shell.execute_reply":"2024-08-26T16:08:29.798686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimento 1: MNIST\n\nEntrenamos el modelo para MNIST utilizando GreedyFed con un número de clientes seleccionados por ronda $M=3$ para $T=150$ rondas. De la ejecución obtendremos los valores de pérdida del modelo global por ronda, la _accuracy_ y el tiempo transcurrido por ronda.","metadata":{}},{"cell_type":"code","source":"import time\n\nstart = time.time()\nlosses, accuracies, elapsed = train_n_rounds(pool_mnist, n_rounds=MNIST_ROUNDS, clients_per_round=3, problem=\"mnist\")\nend = time.time()\n\nsave_data(pool_mnist, losses, accuracies, elapsed, f\"greedy_mnist_{MNIST_ROUNDS}\")\n\nprint(f\"Training time: {end - start} sec\")\nplot_loss_accuracy(losses, accuracies, title=\"Learning curves on MNIST\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"enfdsr6OWefZ","outputId":"3eb9a229-db2a-4977-d2f9-a4c6d920eeae","scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T16:08:29.800445Z","iopub.status.idle":"2024-08-26T16:08:29.800825Z","shell.execute_reply.started":"2024-08-26T16:08:29.800643Z","shell.execute_reply":"2024-08-26T16:08:29.800659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimento 1: CIFAR-10\n\nAhora vamos a realizar el entrenamiento del modelo de CNN con CIFAR-10 para el que tenemos la siguiente configuración:\n- $T=100$\n- $M=6$\n\nY vamos a obtener las mismas métricas que con MNIST.\n\nAdemás de esto, vamos a redefinir la función de entrenamiento para que podamos entrenar por _checkpoints_, es decir, guardar una memoria para poder correr el algoritmo desde un punto de partida. Para ello, hacemos que un diccionario se pase como parámetro de manera que reinicialice los valores de estado, y que devuelva además de las métricas, la memoria modificada con los valores de finalización de la ejecución. Lo hacemos de esta manera para poder realizar varias ejecuciones en caso de que se agoten las cuotas de tiempo de Colab.","metadata":{}},{"cell_type":"code","source":"import time\nfrom typing import Optional\nfrom torch import topk\n\nfrom flex.data import Dataset\n\ndef train_n_rounds_with_memory(pool: FlexModel, n_rounds: int, clients_per_round=20, problem: Problem = \"mnist\", memo: Optional[dict] = None):\n    \"\"\"\n    FL training loop for a certain number of rounds and clients selected.\n    \"\"\"\n    # select transformations depending on problem to solve and assign to server\n    transforms = mnist_transforms if problem == \"mnist\" else cifar_transforms\n\n    def set_transforms(server_model, _):\n        server_model[\"transforms\"] = transforms\n    pool.servers.map(set_transforms)\n\n    # assign validation dataset to server\n    val_data = mnist_val_data if problem == \"mnist\" else cifar_val_data\n    flex_val_data = Dataset.from_torchvision_dataset(val_data)\n    pool.servers.map(save_validation_data, val_data=flex_val_data)\n\n    num_clients = len(pool.clients)\n    \n    if memo:\n        UCB = memo[\"UCB\"]\n        SV = memo[\"SV\"]\n        SV_curr = memo[\"SV_curr\"]\n        N_t = memo[\"N_t\"]\n        losses = memo[\"losses\"]\n        accuracies = memo[\"accuracies\"]\n        elapsed_round_times = memo[\"elapsed_round_times\"]\n        i = memo[\"i\"]\n    else:\n        UCB = [0 for i in range(num_clients)]\n        SV = [0 for i in range(num_clients)]\n        SV_curr = [0 for i in range(num_clients)]\n        N_t = [0 for i in range(num_clients)]\n        losses = []\n        accuracies = []\n        elapsed_round_times = []\n        i = 0\n        \n    while i < n_rounds:\n        start = time.time()\n        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n\n        # select clients to transmit weights to\n        # initially sample every client atleast once (RR)\n        selected = [False for _ in range(num_clients)]\n        if i < np.floor(num_clients / clients_per_round):\n            print(\"RR round\")\n            for idx in range(i * clients_per_round, (i + 1) * clients_per_round):\n                selected[idx] = True\n                N_t[idx] += 1\n        elif i == np.floor(num_clients / clients_per_round):\n            print(\"Last RR round\")\n            for idx in range(i * clients_per_round, num_clients):\n                selected[idx] = True\n                N_t[idx] += 1\n\n            remaining_selections = clients_per_round * (i + 1) - num_clients\n            if remaining_selections > 0:\n                unselected_indices = list(range(0, i * clients_per_round))\n                selected_indices_subset = np.random.choice(\n                    unselected_indices, size=remaining_selections, replace=False\n                )\n\n                for idx in selected_indices_subset:\n                    selected[idx] = True\n                    N_t[idx] += 1\n        else:\n            # UCB Greedy Selection (select clients with highest SV)\n            _, selected_indices = topk(torch.Tensor(UCB), clients_per_round)\n            print(\"Best selected: \", selected_indices)\n            for idx in selected_indices.tolist():\n                selected[idx] = True\n                N_t[idx] += 1\n\n        # take selected clients\n        selected_indices = [index for index, value in enumerate(selected) if value]\n        selected_clients_pool = pool.clients.select(lambda actor_id, actor_roles: actor_id in selected_indices)\n        selected_clients = selected_clients_pool.clients\n\n        print(f\"Selected clients for this round: {len(selected_clients)}\")\n\n        # Deploy the server model to the selected clients\n        pool.servers.map(copy_server_model_to_clients, selected_clients)\n\n        # Each selected client trains its model\n        selected_clients.map(train, transforms=transforms)\n\n        # The aggregador collects weights from the selected clients and aggregates them\n        pool.aggregators.map(get_clients_weights, selected_clients)\n\n        # The aggregator aggregates client updates with FedAveraging\n        pool.aggregators.map(aggregate_with_fedavg)\n\n        # compute SV values for each selected client in the server\n        shapley_values = pool.aggregators.map(shapley_values_gtg, selected_clients)[0]\n\n        print(\"GTG-Shapley values: \", shapley_values)\n\n        # The aggregator send its aggregated weights to the server\n        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n\n        # compute the cumulative SV for each client\n        counter = 0\n        for k in range(num_clients):\n            if selected[k]:\n                SV_curr[k] = shapley_values[counter]\n\n                # previous num selections weight/penalty\n                prev_wt = (N_t[k] - 1) / N_t[k]\n                # current num selections weight/penalty\n                curr_wt = 1 - prev_wt   # 1 / N_t[k]\n\n                # previous cumulative SV\n                prev_sv = SV[k]\n\n                # t-round SV\n                curr_sv = SV_curr[k]\n\n                # compute new cumulative SV\n                SV[k] = prev_wt * prev_sv + curr_wt * curr_sv\n\n                counter += 1\n            else:\n                SV_curr[k] = 0\n\n            UCB[k] = SV[k]\n\n        metrics = pool.servers.map(evaluate_global_model)\n        loss, acc = metrics[0]\n\n        end = time.time()\n\n        losses.append(loss)\n        accuracies.append(acc)\n        elapsed_round_times.append(end - start)\n        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}, round time: {end - start} sec\")\n        \n        i += 1\n        \n    memo = {\n        \"UCB\": UCB,\n        \"SV\": SV,\n        \"SV_curr\": SV_curr,\n        \"N_t\": N_t,\n        \"losses\": losses,\n        \"accuracies\": accuracies,\n        \"elapsed_round_times\": elapsed_round_times,\n        \"i\": i\n    }\n\n    return losses, accuracies, elapsed_round_times, memo","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:08:29.802373Z","iopub.status.idle":"2024-08-26T16:08:29.802779Z","shell.execute_reply.started":"2024-08-26T16:08:29.802574Z","shell.execute_reply":"2024-08-26T16:08:29.802590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Redefinimos la función `save_data` para guardar el estado del modelo de PyTorch así como la memoria de la ejecución para poder marcar un punto de guardado sobre diferentes ejecuciones.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport pickle\n\ndef save_data(model, losses, accuracies, elapsed, memo, filename):\n    df = pd.DataFrame({\n        \"loss\": losses,\n        \"accuracy\": accuracies,\n        \"elapsed_time\": elapsed\n    })\n\n    # save data for MNIST\n    df.to_csv(f\"{filename}.csv\", index=False)\n\n    display(df)\n    \n    # save memory\n    with open(f\"{filename}_memo.pkl\", \"wb\") as file:\n        pickle.dump(memo, file)\n\n    # save model\n    torch.save(model.state_dict(), f\"{filename}.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:08:29.804249Z","iopub.status.idle":"2024-08-26T16:08:29.804637Z","shell.execute_reply.started":"2024-08-26T16:08:29.804445Z","shell.execute_reply":"2024-08-26T16:08:29.804462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definimos una función para obtener el modelo a partir de la _pool_ de servidores.","metadata":{}},{"cell_type":"code","source":"from flex.model import FlexModel\nfrom flex.data import Dataset\n\ndef get_model(server: FlexModel, test_data: Dataset):\n    return server[\"model\"]\n\nmodel = pool_cifar.servers.map(get_model)[0]\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:08:29.805983Z","iopub.status.idle":"2024-08-26T16:08:29.806414Z","shell.execute_reply.started":"2024-08-26T16:08:29.806216Z","shell.execute_reply":"2024-08-26T16:08:29.806239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definimos una función para cargar el estado de un modelo de PyTorch en el servidor.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom flex.model import FlexModel\nfrom flex.data import Dataset\n\ndef load_model_state(server: FlexModel, test_data: Dataset, **kwargs):\n    model_filename = kwargs[\"model_filename\"]\n    server[\"model\"].load_state_dict(torch.load(model_filename))","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:08:29.808078Z","iopub.status.idle":"2024-08-26T16:08:29.808507Z","shell.execute_reply.started":"2024-08-26T16:08:29.808311Z","shell.execute_reply":"2024-08-26T16:08:29.808328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ejecutamos el entrenamiento en ejecuciones de $20$ rondas, lo que nos daría un total de $\\lceil \\frac{T}{20}\\rceil = 5$ ejecuciones en total. Antes de cada ejecución (a excepción de la primera) le vamos a pasar la memoria o estado de ejecución de la anterior, y cargamos en la _pool_ el modelo guardado, hasta completar las cinco ejecuciones.","metadata":{}},{"cell_type":"code","source":"import os\n\n# CHANGE FOR THE RUN CHECKPOINT TO USE\nroot_path = \"/kaggle/input/greedy-run\"","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:08:29.809655Z","iopub.status.idle":"2024-08-26T16:08:29.810035Z","shell.execute_reply.started":"2024-08-26T16:08:29.809853Z","shell.execute_reply":"2024-08-26T16:08:29.809870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round_interval = CIFAR_ROUNDS // 5\n\nstart = time.time()\nlosses, accuracies, elapsed, memo = train_n_rounds_with_memory(\n    pool_cifar,\n    n_rounds=round_interval,\n    clients_per_round=6,\n    problem=\"cifar\"\n)\nend = time.time()\n\n# get and save model state\nmodel = pool_cifar.servers.map(get_model)[0]\nsave_data(model, losses, accuracies, elapsed, memo, f\"greedy_cifar_{CIFAR_ROUNDS}_1\")\n\nprint(f\"Training time: {end - start} sec\")","metadata":{"id":"_kSzrr4gjFbS","scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T16:08:29.811830Z","iopub.status.idle":"2024-08-26T16:08:29.812241Z","shell.execute_reply.started":"2024-08-26T16:08:29.812026Z","shell.execute_reply":"2024-08-26T16:08:29.812042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round_interval = CIFAR_ROUNDS // 5\n\n# load model\npool_cifar.servers.map(load_model_state, model_filename=f\"{os.path.join(root_path, f'greedy_cifar_{CIFAR_ROUNDS}_1.pth')}\")\n# load memory\nwith open(f\"{os.path.join(root_path, f'greedy_cifar_{CIFAR_ROUNDS}_1_memo.pkl')}\", \"rb\") as file:\n    memo = pickle.load(file)\n\nstart = time.time()\nlosses, accuracies, elapsed, memo = train_n_rounds_with_memory(\n    pool_cifar,\n    n_rounds=round_interval * 2,\n    clients_per_round=6,\n    problem=\"cifar\",\n    memo=memo\n)\nend = time.time()\n\n# get and save model state\nmodel = pool_cifar.servers.map(get_model)[0]\nsave_data(model, losses, accuracies, elapsed, memo, f\"greedy_cifar_{CIFAR_ROUNDS}_2\")\n\nprint(f\"Training time: {end - start} sec\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T16:08:29.813613Z","iopub.status.idle":"2024-08-26T16:08:29.813991Z","shell.execute_reply.started":"2024-08-26T16:08:29.813807Z","shell.execute_reply":"2024-08-26T16:08:29.813823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round_interval = CIFAR_ROUNDS // 5\n\n# load model\npool_cifar.servers.map(load_model_state, model_filename=f\"{os.path.join(root_path, f'greedy_cifar_{CIFAR_ROUNDS}_2.pth')}\")\n# load memory\nwith open(f\"{os.path.join(root_path, f'greedy_cifar_{CIFAR_ROUNDS}_2_memo.pkl')}\", \"rb\") as file:\n    memo = pickle.load(file)\n\nstart = time.time()\nlosses, accuracies, elapsed, memo = train_n_rounds_with_memory(\n    pool_cifar,\n    n_rounds=round_interval * 3,\n    clients_per_round=6,\n    problem=\"cifar\",\n    memo=memo\n)\nend = time.time()\n\n# get and save model state\nmodel = pool_cifar.servers.map(get_model)[0]\nsave_data(model, losses, accuracies, elapsed, memo, f\"greedy_cifar_{CIFAR_ROUNDS}_3\")\n\nprint(f\"Training time: {end - start} sec\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T16:08:29.815573Z","iopub.status.idle":"2024-08-26T16:08:29.815955Z","shell.execute_reply.started":"2024-08-26T16:08:29.815772Z","shell.execute_reply":"2024-08-26T16:08:29.815788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round_interval = CIFAR_ROUNDS // 5\n\n# load model\npool_cifar.servers.map(load_model_state, model_filename=f\"{os.path.join(root_path, f'greedy_cifar_{CIFAR_ROUNDS}_3.pth')}\")\n# load memory\nwith open(f\"{os.path.join(root_path, f'greedy_cifar_{CIFAR_ROUNDS}_3_memo.pkl')}\", \"rb\") as file:\n    memo = pickle.load(file)\n\nstart = time.time()\nlosses, accuracies, elapsed, memo = train_n_rounds_with_memory(\n    pool_cifar,\n    n_rounds=round_interval * 4,\n    clients_per_round=6,\n    problem=\"cifar\",\n    memo=memo\n)\nend = time.time()\n\n# get and save model state\nmodel = pool_cifar.servers.map(get_model)[0]\nsave_data(model, losses, accuracies, elapsed, memo, f\"greedy_cifar_{CIFAR_ROUNDS}_4\")\n\nprint(f\"Training time: {end - start} sec\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T16:08:29.817448Z","iopub.status.idle":"2024-08-26T16:08:29.817830Z","shell.execute_reply.started":"2024-08-26T16:08:29.817648Z","shell.execute_reply":"2024-08-26T16:08:29.817664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round_interval = CIFAR_ROUNDS // 5\n\n# load model\npool_cifar.servers.map(load_model_state, model_filename=f\"{os.path.join(root_path, f'greedy_cifar_{CIFAR_ROUNDS}_4.pth')}\")\n# load memory\nwith open(f\"{os.path.join(root_path, f'greedy_cifar_{CIFAR_ROUNDS}_4_memo.pkl')}\", \"rb\") as file:\n    memo = pickle.load(file)\n\nstart = time.time()\nlosses, accuracies, elapsed, memo = train_n_rounds_with_memory(\n    pool_cifar,\n    n_rounds=round_interval * 5,\n    clients_per_round=6,\n    problem=\"cifar\",\n    memo=memo\n)\nend = time.time()\n\n# get and save model state\nmodel = pool_cifar.servers.map(get_model)[0]\nsave_data(model, losses, accuracies, elapsed, memo, f\"greedy_cifar_{CIFAR_ROUNDS}_5\")\n\nprint(f\"Training time: {end - start} sec\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T16:08:29.819302Z","iopub.status.idle":"2024-08-26T16:08:29.819762Z","shell.execute_reply.started":"2024-08-26T16:08:29.819508Z","shell.execute_reply":"2024-08-26T16:08:29.819525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss_accuracy(losses, accuracies, title=\"Learning curves on CIFAR-10\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:08:29.821329Z","iopub.status.idle":"2024-08-26T16:08:29.821707Z","shell.execute_reply.started":"2024-08-26T16:08:29.821524Z","shell.execute_reply":"2024-08-26T16:08:29.821541Z"},"trusted":true},"execution_count":null,"outputs":[]}]}