{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzFVvanFrrwe"
      },
      "source": [
        "# Selección de clientes GreedyFed - Método basado en rendimiento del modelo\n",
        "\n",
        "En este notebook vamos a entrenar un modelo de ML utilizando el paradigma de Aprendizaje Federado [1] para los problemas de clasificación de dígitos manuscritos MNIST [2] y clasificación de imágenes a color CIFAR-10 [3]. Usaremos estos dos problemas de visión por computador para la implementación y análisis de rendimiento del método GreedyFed [4] para la selección de clientes.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1602.05629\n",
        ">\n",
        "> [2] http://yann.lecun.com/exdb/mnist\n",
        ">\n",
        "> [3] https://www.cs.toronto.edu/~kriz/cifar.html\n",
        ">\n",
        "> [4] https://arxiv.org/abs/2312.09108"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4BfI6bDrg3o",
        "outputId": "d0a6239c-3103-433b-884f-a0680c693071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLEXible is not installed.\n",
            "Installing dependency flexible-fl...\n",
            "Collecting flexible-fl\n",
            "  Downloading flexible_fl-0.6.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m728.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.25.2)\n",
            "Collecting multiprocess (from flexible-fl)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.2.2)\n",
            "Collecting cardinality (from flexible-fl)\n",
            "  Downloading cardinality-0.1.1.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sultan (from flexible-fl)\n",
            "  Downloading sultan-0.9.1-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (4.66.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (1.11.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from flexible-fl) (5.1.0)\n",
            "Collecting tensorly (from flexible-fl)\n",
            "  Downloading tensorly-0.8.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->flexible-fl) (2.31.0)\n",
            "Collecting dill>=0.3.8 (from multiprocess->flexible-fl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->flexible-fl) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->flexible-fl) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->flexible-fl) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->flexible-fl) (1.7.1)\n",
            "Building wheels for collected packages: cardinality\n",
            "  Building wheel for cardinality (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cardinality: filename=cardinality-0.1.1-py3-none-any.whl size=2587 sha256=9c11409135565da03bc24478609b97c88c8c5c80925d0eaf21b3289c98d6a925\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/19/d1/2665c004b583a7d1880fa59055a3e462d6e35841a01b57010b\n",
            "Successfully built cardinality\n",
            "Installing collected packages: sultan, cardinality, dill, tensorly, multiprocess, flexible-fl\n",
            "Successfully installed cardinality-0.1.1 dill-0.3.8 flexible-fl-0.6.1 multiprocess-0.70.16 sultan-0.9.1 tensorly-0.8.1\n"
          ]
        }
      ],
      "source": [
        "# install FLEXible framework if not installed\n",
        "try:\n",
        "    import flex\n",
        "    print(\"FLEXible is installed.\")\n",
        "except:\n",
        "    print(\"FLEXible is not installed.\\nInstalling dependency flexible-fl...\")\n",
        "    !pip install flexible-fl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTScJ4UAS-mB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_accuracy(loss, accuracy, title=\"Learning Curves\"):\n",
        "    # Example data\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, 'b', label='Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, 'g', label='Accuracy')\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "\n",
        "    # Show the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5jdIu00Ftc1u",
        "outputId": "d03bda2a-c505-4956-ee39-8021fc44163b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# select device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIIPVTjuSl00"
      },
      "source": [
        "# Cargar datasets - MNIST, CIFAR-10\n",
        "\n",
        "Cargaremos $2$ de los datasets que usaremos para entrenar el modelo de ML.\n",
        "\n",
        "## MNIST\n",
        "El primero será MNIST (Modified National Institute of Standards and Technology database), que consiste en imágenes de $28$ pixeles de ancho y alto en escala de grises de dígitos manuscritos del $0$ al $9$. Formalmente, definimos el problema de ML como un problema de clasificación multiclase, donde definimos una imagen de entrada $X_n \\in \\mathcal X$ se define como una matriz $X_{28\\times 28}, x_i \\in \\{1, ..., 255\\}$, donde $x_i$ es un pixel de la imagen $X$. Cada imágen de entrada $X_n\\in \\mathcal X$ es etiquetada con una clase $y_n \\in \\mathcal Y, y = \\{0, 1, ..., 9\\}$ que representa el dígito al que corresponde la imagen. El conjunto de entrenamiento consta de $N=240,000$ imágenes.\n",
        "\n",
        "El dataset descargado será el de _Extended MNIST_ [1], que comprende una versión extendida del concepto original de MNIST para proporcionar dígitos y letras manuscritas, una cantidad más grande de datos, diferentes formas de separar los datos (solo dígitos, letras, por clase, ...), etc. Nosotros solo utilizaremos los dígitos para entrenar el clásico MNIST, del cual su versión extendida consta de $N = 280,000$ imágenes de dígitos manuscritos.\n",
        "\n",
        "Una característica importante de EMNIST es que cada imagen tendrá asociado un autor (i.e. la persona que ha escrito el dígito o letra), tal que un dígito es escrito por un solo autor pero un autor puede escribir muchos dígitos. Esto nos da la ventaja de poder distribuir los datos de forma No-IID tal que cada cliente tendrá los datos con autoría de un solo autor.\n",
        "\n",
        "FLEX nos da utilidades para cargar EMNIST y federarlo automáticamente con la función `load`, donde además descargamos el dataset de test y solo usaremos el dataset de dígitos clásico de MNIST. Definimos también las transformaciones a realizar a los datos que son simplemente normalizar cada pixel a valores de $[0,1]$ (función `ToTensor()`) y normalizar los valores de cada pixel con una media y desviación estándar de $0.5$.\n",
        "\n",
        "> [1] https://www.nist.gov/itl/products-and-services/emnist-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYOy-Bmsu0_J",
        "outputId": "5c51bf12-44fa-4cbe-f8ee-a569f4b2bfe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fl9fRPPxTUxnC56ACzZ8JiLiew0SMFwt\n",
            "From (redirected): https://drive.google.com/uc?id=1fl9fRPPxTUxnC56ACzZ8JiLiew0SMFwt&confirm=t&uuid=3f45023a-d114-43f5-a6f2-9d0b89b620a9\n",
            "To: /content/emnist-digits.mat\n",
            "100%|██████████| 90.7M/90.7M [00:03<00:00, 30.1MB/s]\n",
            "\u001b[36m[sultan]: md5 -q ./emnist-digits.mat;\u001b[0m\n",
            "DEBUG:sultan:md5 -q ./emnist-digits.mat;\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | NoneType: None\u001b[0m\n",
            "CRITICAL:sultan:| NoneType: None\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: --{ STDERR }-------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ STDERR }-------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | /bin/sh: 1: md5: not found\u001b[0m\n",
            "CRITICAL:sultan:| /bin/sh: 1: md5: not found\n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | Traceback (most recent call last):\u001b[0m\n",
            "CRITICAL:sultan:| Traceback (most recent call last):\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\n",
            "\u001b[01;31m[sultan]: |     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\u001b[0m\n",
            "CRITICAL:sultan:|     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\n",
            "\u001b[01;31m[sultan]: |     self.dump_exception()\u001b[0m\n",
            "CRITICAL:sultan:|     self.dump_exception()\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise self._exception\u001b[0m\n",
            "CRITICAL:sultan:|     raise self._exception\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\u001b[0m\n",
            "CRITICAL:sultan:|     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\n",
            "\u001b[01;31m[sultan]: | subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\u001b[0m\n",
            "CRITICAL:sultan:| subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[36m[sultan]: md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\u001b[0m\n",
            "DEBUG:sultan:md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\n",
            "\u001b[36m[sultan]: md5 -q ./emnist-digits.mat;\u001b[0m\n",
            "DEBUG:sultan:md5 -q ./emnist-digits.mat;\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | NoneType: None\u001b[0m\n",
            "CRITICAL:sultan:| NoneType: None\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: --{ STDERR }-------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ STDERR }-------------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | /bin/sh: 1: md5: not found\u001b[0m\n",
            "CRITICAL:sultan:| /bin/sh: 1: md5: not found\n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[01;31m[sultan]: Unable to run 'md5 -q ./emnist-digits.mat;'\u001b[0m\n",
            "CRITICAL:sultan:Unable to run 'md5 -q ./emnist-digits.mat;'\n",
            "\u001b[01;31m[sultan]: --{ TRACEBACK }----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:--{ TRACEBACK }----------------------------------------------------------------------------------------------------\n",
            "\u001b[01;31m[sultan]: | Traceback (most recent call last):\u001b[0m\n",
            "CRITICAL:sultan:| Traceback (most recent call last):\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/api.py\", line 212, in run\n",
            "\u001b[01;31m[sultan]: |     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\u001b[0m\n",
            "CRITICAL:sultan:|     result = Result(process, commands, self._context, streaming, halt_on_nonzero=halt_on_nonzero)\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 59, in __init__\n",
            "\u001b[01;31m[sultan]: |     self.dump_exception()\u001b[0m\n",
            "CRITICAL:sultan:|     self.dump_exception()\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 114, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise self._exception\u001b[0m\n",
            "CRITICAL:sultan:|     raise self._exception\n",
            "\u001b[01;31m[sultan]: |   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\u001b[0m\n",
            "CRITICAL:sultan:|   File \"/usr/local/lib/python3.10/dist-packages/sultan/result.py\", line 95, in dump_exception\n",
            "\u001b[01;31m[sultan]: |     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\u001b[0m\n",
            "CRITICAL:sultan:|     raise subprocess.CalledProcessError(self.rc, ''.join(self._commands), self.stderr)\n",
            "\u001b[01;31m[sultan]: | subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\u001b[0m\n",
            "CRITICAL:sultan:| subprocess.CalledProcessError: Command 'md5 -q ./emnist-digits.mat;' returned non-zero exit status 127.\n",
            "\u001b[01;31m[sultan]: | \u001b[0m\n",
            "CRITICAL:sultan:| \n",
            "\u001b[01;31m[sultan]: -------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "CRITICAL:sultan:-------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[33m[sultan]: The following are additional information that can be used to debug this exception.\u001b[0m\n",
            "WARNING:sultan:The following are additional information that can be used to debug this exception.\n",
            "\u001b[33m[sultan]: The following is the context used to run:\u001b[0m\n",
            "WARNING:sultan:The following is the context used to run:\n",
            "\u001b[33m[sultan]: \t - cwd: None\u001b[0m\n",
            "WARNING:sultan:\t - cwd: None\n",
            "\u001b[33m[sultan]: \t - sudo: False\u001b[0m\n",
            "WARNING:sultan:\t - sudo: False\n",
            "\u001b[33m[sultan]: \t - user: root\u001b[0m\n",
            "WARNING:sultan:\t - user: root\n",
            "\u001b[33m[sultan]: \t - hostname: None\u001b[0m\n",
            "WARNING:sultan:\t - hostname: None\n",
            "\u001b[33m[sultan]: \t - env: None\u001b[0m\n",
            "WARNING:sultan:\t - env: None\n",
            "\u001b[33m[sultan]: \t - logging: True\u001b[0m\n",
            "WARNING:sultan:\t - logging: True\n",
            "\u001b[33m[sultan]: \t - executable: None\u001b[0m\n",
            "WARNING:sultan:\t - executable: None\n",
            "\u001b[33m[sultan]: \t - ssh_config: \u001b[0m\n",
            "WARNING:sultan:\t - ssh_config: \n",
            "\u001b[33m[sultan]: \t - src: None\u001b[0m\n",
            "WARNING:sultan:\t - src: None\n",
            "\u001b[36m[sultan]: md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\u001b[0m\n",
            "DEBUG:sultan:md5sum ./emnist-digits.mat | cut -f 1 -d \" \";\n"
          ]
        }
      ],
      "source": [
        "from flex.datasets import load\n",
        "from torchvision import transforms\n",
        "\n",
        "flex_dataset_mnist, test_data = load(\"federated_emnist\", return_test=True, split=\"digits\")\n",
        "\n",
        "# Assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_mnist[server_id] = test_data\n",
        "\n",
        "# apply transforms\n",
        "mnist_transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6KW3xiYW3e"
      },
      "source": [
        "## CIFAR-10\n",
        "\n",
        "El segundo dataset es CIFAR-10 (_Canadian Institute for Advanced Research_)[1], el cual es otro de los datasets junto con MNIST, más utilizados en el campo del Deep Learning y Visión por Computador. CIFAR-10 consiste en una colección de imágenes de $32$ pixeles de altura y de ancho a color ($3$ canales RGB) representando $10$ objetos reales:\n",
        "- Avión (airplane)\n",
        "- Automóvil (automobile)\n",
        "- Pájaro (bird)\n",
        "- Gato (cat)\n",
        "- Ciervo (deer)\n",
        "- Perro (dog)\n",
        "- Rana (frog)\n",
        "- Caballo (horse)\n",
        "- Barco (ship)\n",
        "- Camión (truck)\n",
        "\n",
        "Formalmente, definimos el problema de ML como un problema de clasificación multiclase, donde definimos una imagen de entrada $X_n \\in \\mathcal X$ se define como una matriz $X_{32\\times 32 \\times 3}, x_i \\in \\{1, ..., 255\\}$, donde $x_i^c$ es un pixel de la imagen $X$ en el canal $c$. Cada imágen de entrada $X_n\\in \\mathcal X$ es etiquetada con una clase $y_n \\in \\mathcal Y, y = \\{0, 1, ..., 9\\}$ tal que indexa la lista de clases $C=(c_0=\\text{plane}, c_1=\\text{automobile}, ..., c_9=\\text{truck})$. El conjunto de entrenamiento consta de $N=50,000$ datos de entrada.\n",
        "\n",
        "A diferencia de EMNIST, FLEX no carga por defecto CIFAR-10, por lo que deberemos de descargar y federar manualmente el dataset. Utilizaremos el dataset descargado desde `torchvision` de PyTorch y federamos utilizando una configuración por defecto de FLEX con un número de nodos o clientes de $K = 100$ clientes. Definimos también las transformaciones para escalar los valores a $[0,1]$ y normalizar con medias y desviaciones estándar específicas de CIFAR-10 [2].\n",
        "\n",
        "> [1] https://www.cs.toronto.edu/~kriz/cifar.html\n",
        ">\n",
        "> [2] https://github.com/kuangliu/pytorch-cifar/issues/19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErtVS1zPd-Zb",
        "outputId": "ba6a9773-6f1f-4ea4-ec22-20bbfceb8b8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 75639438.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from flex.data import FedDataDistribution, FedDatasetConfig, Dataset\n",
        "\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=\".\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=None,  # we apply them later in training process\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\".\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=None,  # we apply them later in training process\n",
        ")\n",
        "\n",
        "config = FedDatasetConfig(seed=33)\n",
        "config.replacement = False\n",
        "config.n_nodes = 100\n",
        "\n",
        "# create Federated data distribution of CIFAR-10\n",
        "flex_dataset_cifar = FedDataDistribution.from_config(\n",
        "    centralized_data=Dataset.from_torchvision_dataset(train_data),\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_cifar[server_id] = Dataset.from_torchvision_dataset(test_data)\n",
        "\n",
        "# apply transforms\n",
        "cifar_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalizar con las medias y desviaciones estándar específicas de CIFAR-10\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJLc-rB4gfzS"
      },
      "source": [
        "# Modelos de aprendizaje\n",
        "\n",
        "Definimos dos modelos de Machine Learning, uno para cada uno de los dos datasets que hemos definido anteriormente. Para ambos problemas, usaremos como función de pérdida o _criterion_ la función de entropía cruzada o _Cross Entropy Loss_ cuya definición [1] la define como (para una muestra $n$):\n",
        "\n",
        "$$l_n = -w_{y_n} \\cdot \\log \\frac{\\exp (x_n, y_n)}{\\sum_{c=0}^C \\exp (x_n, c)} =-w_{y_n}\\cdot \\log (\\text{Softmax} (x_n, y_n))$$\n",
        "\n",
        "En ambos problemas utilizaremos Adam [2] como optimizador o algoritmo de aprendizaje. Utilizaremos los mismos parámetros que en [2] al haber hecho los experimentos sobre modelos similares y con los mismos datasets que estamos usando y que han demostrando buenos resultados: $\\eta = 0.01$ y $\\beta_1 = 0.9,\\beta_2 = 0.999$. Estos parámetros están definidos por defecto en la librería de PyTorch [3].\n",
        "\n",
        "> [1] https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        ">\n",
        "> [2] https://arxiv.org/abs/1412.6980\n",
        ">\n",
        "> [3] https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPizwuhm28X"
      },
      "source": [
        "## Perceptrón Multicapa (MNIST)\n",
        "\n",
        "El primero, que lo usaremos para ajustar MNIST, será un Perceptrón Multicapa (MLP) de dos capas ocultas de $128$ parámetros la primera y $10$ la segunda (correspondiente al número de clases), y usando ReLU [1] como función de activación.\n",
        "\n",
        "El modelo inicialmente lo tendrá el servidor y será copiado a cada uno de los clientes. En FLEX usamos el decorador `@init_model_server` para inicializar el modelo en el servidor, donde también podemos además de nuestra arquitectura del modelo, el optimizador y la función de pérdida a usar.\n",
        "\n",
        "> [1] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wro1kxtkw1w0"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer Perceptron classifier with two hidden layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features: Tuple[int, int], hidden_features: int, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "\n",
        "        width, height = in_features\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(width * height, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTQhX95DUm1C"
      },
      "source": [
        "## Red Neuronal Convolucional (CNN) - CIFAR-10\n",
        "\n",
        "Para nuestro segundo problema, que consta de ajustar un modelo para clasificar CIFAR-10, utilizaremos un red neuronal convolucional o CNN. A diferencia de MNIST, CIFAR-10 tiene unos datos de entrada con una dimensionalidad más compleja, por lo que utilizar una CNN reduciría la cantidad de parámetros necesarios a entrenar.\n",
        "\n",
        "Usaremos la red neuronal convolucional utilizada en los experimentos de el algoritmo GreedyFed [1], debido a que es una arquitectura con pocos parámetros a entrenar, lo que nos permite obtener los resultados deseados en poco tiempo. Concretamente, la arquitectura de la red a implementar será la de una CNN estándar que comprende dos capas convolucionales (CONV) $4\\times 4$:\n",
        "- Ambas con $8$ canales de salida.\n",
        "- Cada uno de ellos activados por ReLU [2].\n",
        "- Y aplicando _Max Pooling_ $2\\times 2$ con _stride_ de $2$ en la salida de cada capa convolucional para reducir la dimensionalidad del los mapas de activación.\n",
        "\n",
        "Seguido de $1$ capa _Fully-Connected_ de $10$ unidades activada por Softmax para la capa de final de salida.\n",
        "\n",
        "> [1] https://arxiv.org/abs/2312.09108\n",
        ">\n",
        "> [2] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FpzmNDvUp-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolucional Neural Net classifier for CIFAR-10 image recognition (32x32 images).\n",
        "\n",
        "    Model architecture based on GreedyFed experimental setting:\n",
        "    - https://github.com/pringlesinghal/GreedyFed/blob/b2d928670bcd014035830c531bea9f76c57b4b70/model.py#L46\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, in_width: int = 32, in_height: int = 32, output_dim: int = 10):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.input_w = in_width\n",
        "        self.input_h = in_height\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # First CONV layer\n",
        "        out_channels_conv_1 = 8\n",
        "        kernel_conv_1_size = 4\n",
        "        padding_conv_1 = 1\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels=out_channels_conv_1,\n",
        "            kernel_size=kernel_conv_1_size,\n",
        "            padding=padding_conv_1,\n",
        "        )\n",
        "\n",
        "        output_w_1, output_h_1 = self.__conv_output_dims(\n",
        "            self.input_w, self.input_h, kernel_conv_1_size, padding_conv_1\n",
        "        )\n",
        "\n",
        "        # First MAXPOOL layer\n",
        "        kernel_pool_size = 2\n",
        "        stride_pool = 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=kernel_pool_size, stride=stride_pool)\n",
        "\n",
        "        output_w_2, output_h_2 = self.__pool_output_dims(\n",
        "            output_w_1, output_h_1, kernel_pool_size, stride_pool\n",
        "        )\n",
        "\n",
        "        # Second CONV layer\n",
        "        in_channels_conv_2 = out_channels_conv_1\n",
        "        out_channels_conv_2 = 8\n",
        "        kernel_conv_2_size = 4\n",
        "        padding_conv_2 = 1\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels_conv_2,\n",
        "            out_channels=out_channels_conv_2,\n",
        "            kernel_size=kernel_conv_2_size,\n",
        "            padding=padding_conv_2\n",
        "        )\n",
        "\n",
        "        output_w_3, output_h_3 = self.__conv_output_dims(\n",
        "            output_w_2, output_h_2, kernel_conv_2_size, padding_conv_2\n",
        "        )\n",
        "\n",
        "        # after second MAXPOOL layer, compute final out width and height\n",
        "        output_w_4, output_h_4 = self.__pool_output_dims(\n",
        "            output_w_3, output_h_3, kernel_pool_size, stride_pool\n",
        "        )\n",
        "\n",
        "        # Fully-Connected layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        input_size_4 = int(output_w_4 * output_h_4 * out_channels_conv_2)\n",
        "        self.fc = nn.Linear(input_size_4, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        # x = self.flatten(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def __conv_output_dims(self, width: int, height: int, kernel_size: int, padding: int = 1):\n",
        "        output_w = width - kernel_size + 2 * padding + 1\n",
        "        output_h = height - kernel_size + 2 * padding + 1\n",
        "        return output_w, output_h\n",
        "\n",
        "    def __pool_output_dims(self, width: int, height: int, kernel_size: int, stride: int = 1):\n",
        "        output_w = np.floor((width - kernel_size) / stride) + 1\n",
        "        output_h = np.floor((height - kernel_size) / stride) + 1\n",
        "\n",
        "        return output_w, output_h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3LoDmezyvud"
      },
      "source": [
        "## Configuración básica de escenario de FL\n",
        "\n",
        "Configuraremos un escenario de aprendizaje federado centralizado (CFL) usando la librería FLEXible. FLEXible [1], o simplemente FLEX, es una librería de Python que proporciona un framework para la construcción de entornos de aprendizaje federado para fines de investigación y simulación. FLEX pretende dar flexibilidad en cuanto la gran variedad de escenarios y necesidades que se pueden llegar a plantear para experimentar en entornos federados.\n",
        "\n",
        "Uno de nuestros objetivos es _integrar_ un método o técnica de selección de clientes en un escenario inicial de FL. La flexibilidad de FLEX nos permitirá conseguir este objetivo gracias a su flujo de mensajes entre entidades separadas por roles, y por la arquitectura modular de estas que nos permiten almacenar información de manera conveniente [1].\n",
        "\n",
        "> [1] https://arxiv.org/abs/2404.06127\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOLo8WYFBnwR"
      },
      "source": [
        "### Inicialización del modelo del servidor\n",
        "\n",
        "El primer paso de nuestro bucle de entrenamiento en aprendizaje federado es inicializar el modelo FLEX del servidor (que posteriormente se distribuirá a los clientes).\n",
        "\n",
        "Con FLEX podemos hacer uso del decorador `init_server_model` para facilitarnos esta tarea. Esta función entonces debe de instanciar y devolver un modelo de FLEX en donde además definimos el modelo de aprendizaje $^1$, la función de pérdida, el optimizador y cualquier otra información que se usará en las demás fases de la ronda de entrenamiento.\n",
        "\n",
        "Definimos dos funciones que realizan esta tarea, dos para cada una de nuestras arquitecturas, el Perceptrón Multicapa y la Red Neuronal Convolucional. Como habíamos especificado en el planteamiento del problema de ML a resolver, utilizaremos el optimizador SGD-Adam con sus parámetros por defecto, y la Cross-Entropy como función de pérdida.\n",
        "\n",
        "> $^1$ No se debe confundir el modelo de tipo `FlexModel` con un modelo de ML. El primero implementa el bloque fundamental que define una entidad en el escenario de FL (donde almacenamos información, sus datos locales, su modelo de aprendizaje, ...). El segundo es un modelo aprendizaje como lo puede ser un módulo de PyTorch o Tensorflow que realiza el aprendizaje automático y la predicción sobre los datos. Se dejará claro la diferencia entre los dos tipos de modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9SWwbzBU9hD"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from flex.pool import init_server_model\n",
        "from flex.pool import FlexPool\n",
        "from flex.model import FlexModel\n",
        "\n",
        "mnist_in_features = (28, 28)\n",
        "mnist_hidden_features = 128\n",
        "\n",
        "@init_server_model\n",
        "def build_server_model_mlp():\n",
        "    server_flex_model = FlexModel()\n",
        "\n",
        "    server_flex_model[\"model\"] = MLP(mnist_in_features, mnist_hidden_features)\n",
        "\n",
        "    # Required to store this for later stages of the FL training process\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
        "    return server_flex_model\n",
        "\n",
        "cifar_in_channels = 3\n",
        "cifar_num_classes = 10\n",
        "\n",
        "@init_server_model\n",
        "def build_server_model_cnn():\n",
        "    server_flex_model = FlexModel()\n",
        "\n",
        "    server_flex_model[\"model\"] = CNN(in_channels=cifar_in_channels, output_dim=cifar_num_classes)\n",
        "\n",
        "    # Required to store this for later stages of the FL training process\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
        "    return server_flex_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5LzYqxUzIaU"
      },
      "source": [
        "---\n",
        "### Distribuir modelo del servidor\n",
        "\n",
        "El siguiente paso a realizar en un flujo de entrenamiento de aprendizaje federado es la distribución del modelo del servidor a los cientes. Con FLEX, podemos utilizar el decorador `@deploy_server_model` para distribuir el modelo del servidor a los clientes, definiendo una función que devuelva el modelo a almacenar en cada cliente.\n",
        "\n",
        "En este caso, realizamos una copia profunda del modelo del servidor que será asignado a cada cliente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwxXBCoCytZh"
      },
      "outputs": [],
      "source": [
        "from flex.pool import deploy_server_model\n",
        "import copy\n",
        "\n",
        "\n",
        "@deploy_server_model\n",
        "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
        "    return copy.deepcopy(server_flex_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1S01aEFzevA"
      },
      "source": [
        "### Actualización del modelo del lado del cliente\n",
        "\n",
        "Definimos la función encargada de realizar el entrenamiento del modelo sobre los datos locales del cliente. A esta función, es conveniente pasarle como parámetros los hiperparámetros de entrenamiento de un modelo de ML convencional como el número de épocas $E$ y el tamaño de _batch_ $B$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrWryr7HVNKt"
      },
      "outputs": [],
      "source": [
        "from flex.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train(client_flex_model: FlexModel, client_data: Dataset, **kwargs):\n",
        "    # parse kwargs\n",
        "    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 5\n",
        "    batch_size = kwargs[\"batch_size\"] if \"batch_size\" in kwargs else 20\n",
        "    transforms = kwargs[\"transforms\"] if \"transforms\" in kwargs else None\n",
        "\n",
        "    # get client data as a torchvision object\n",
        "    train_dataset = client_data.to_torchvision_dataset(transform=transforms)\n",
        "    client_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    # get model\n",
        "    model = client_flex_model[\"model\"]\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    model = model.train()\n",
        "    model = model.to(device)\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "\n",
        "    # train model\n",
        "    for _ in range(epochs):\n",
        "        for imgs, labels in client_dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(imgs)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMSabAvG0u2J"
      },
      "source": [
        "### Obtener los parámetros de los clientes\n",
        "\n",
        "Ahora implementamos la función que hace que el servidor (con rol de agregador) recupere los nuevos parámetros actualizados de los clientes. Con el decorador `@collect_clients_weights` recuperamos los pesos de PyTorch de cada cliente seleccionado para esa ronda. En el caso de PyTorch, el modelo devuelve los pesos en forma de un diccionario con `state_dict` para el que cada nombre representa una capa de la red y sus parámetros, lo que hacemos será devolver una lista con los valores de ese diccionario correspondientes a los pesos de la red entera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggbUniMr0oGy"
      },
      "outputs": [],
      "source": [
        "from flex.pool import collect_clients_weights\n",
        "\n",
        "\n",
        "@collect_clients_weights\n",
        "def get_clients_weights(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    return [weight_dict[name] for name in weight_dict]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DT76cJ41uFY"
      },
      "source": [
        "### Agregación de los parámetros\n",
        "\n",
        "El servidor/agregador agrega entonces estos nuevos parámetros para conseguir el nuevo modelo global. Utilizamos el decorador `@aggregate_weights` para poder agregar los pesos que hemos recuperado de los clientes en la fase anterior computando la media de los pesos de manera uniforme, conocido como agregador FedAvg [1], donde realizamos la media por columnas para cada capa de pesos.\n",
        "\n",
        "> [1] https://arxiv.org/abs/1602.05629"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR27tyd51cfq"
      },
      "outputs": [],
      "source": [
        "from flex.pool import aggregate_weights\n",
        "import tensorly as tl\n",
        "\n",
        "tl.set_backend(\"pytorch\")\n",
        "\n",
        "\n",
        "@aggregate_weights\n",
        "def aggregate_with_fedavg(list_of_weights: list):\n",
        "    agg_weights = []\n",
        "    for layer_index in range(len(list_of_weights[0])):\n",
        "        weights_per_layer = [weights[layer_index] for weights in list_of_weights]\n",
        "        weights_per_layer = tl.stack(weights_per_layer)\n",
        "        agg_layer = tl.mean(weights_per_layer, axis=0)\n",
        "        agg_weights.append(agg_layer)\n",
        "    return agg_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQtCzF0K2i92"
      },
      "source": [
        "Finalmente, agregamos los pesos al modelo de nuestro servidor/agregador. Sencillamente, para cada capa de nuestro modelo, realizamo una copia del nuevo que hemos agregado en la fase anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fk-VxE52ZNH"
      },
      "outputs": [],
      "source": [
        "from flex.pool import set_aggregated_weights\n",
        "\n",
        "\n",
        "@set_aggregated_weights\n",
        "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
        "    with torch.no_grad():\n",
        "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
        "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
        "            weight_dict[layer_key].copy_(new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB5N8y2a27JU"
      },
      "source": [
        "### Evaluación del modelo global\n",
        "\n",
        "Podemos evaluar el modelo del servidor sobre el dataset de test que hemos definido anteriormente que residía en el mismo servidor. Para ello, definimos una función `evaluate_global_model` que obtenga las predicciones del modelo con el dataset de test y devuelva las metricas resultantes, que en este caso son simplemente la pérdida y la _accuracy_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFZA3_Sn20tn"
      },
      "outputs": [],
      "source": [
        "def evaluate_global_model(server_flex_model: FlexModel, test_data: Dataset):\n",
        "    model = server_flex_model[\"model\"]\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    total_count = 0\n",
        "    model = model.to(device)\n",
        "    criterion = server_flex_model[\"criterion\"]\n",
        "    # get test data as a torchvision object\n",
        "    test_dataset = test_data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=256, shuffle=True, pin_memory=False\n",
        "    )\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_dataloader:\n",
        "            total_count += target.size(0)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            losses.append(criterion(output, target).item())\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
        "\n",
        "    test_loss = sum(losses) / len(losses)\n",
        "    test_acc /= total_count\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p233PYL9Lum"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "A continuación se muestra el bucle de entrenamiento aplicando todas las fases que hemos implementado anteriormente. En esta versión básica se implementa una selección aleatoria (RandomSampling _baseline_), similar a la propuesta en [1] donde se selecciona de manera aleatoria y uniforme $M$ clientes para la ronda actual. Este proceso se repite de forma iterativa un número determinado de rondas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVv7hlVaVg5y"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "Problem = Literal[\"mnist\", \"cifar\"]\n",
        "\n",
        "def train_n_rounds(pool: FlexPool, n_rounds: int, clients_per_round=20, problem: Problem = \"mnist\"):\n",
        "    \"\"\"\n",
        "    FL training loop for a certain number of rounds and clients selected.\n",
        "    \"\"\"\n",
        "    # select transformations depending on problem to solve\n",
        "    transforms = mnist_transforms if problem == \"mnist\" else cifar_transforms\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    for i in range(n_rounds):\n",
        "        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n",
        "        selected_clients_pool = pool.clients.select(clients_per_round)\n",
        "        selected_clients = selected_clients_pool.clients\n",
        "        print(f\"Selected clients for this round: {len(selected_clients)}\")\n",
        "        # Deploy the server model to the selected clients\n",
        "        pool.servers.map(copy_server_model_to_clients, selected_clients)\n",
        "        # Each selected client trains her model\n",
        "        selected_clients.map(train, transforms=transforms)\n",
        "        # The aggregador collects weights from the selected clients and aggregates them\n",
        "        pool.aggregators.map(get_clients_weights, selected_clients)\n",
        "        pool.aggregators.map(aggregate_with_fedavg)\n",
        "        # The aggregator send its aggregated weights to the server\n",
        "        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n",
        "        metrics = pool.servers.map(evaluate_global_model)\n",
        "        loss, acc = metrics[0]\n",
        "        losses.append(loss)\n",
        "        accuracies.append(acc)\n",
        "        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}\")\n",
        "\n",
        "    return losses, accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L2wrteiiFb_"
      },
      "source": [
        "## Seleccionando clientes con GreedyFed\n",
        "\n",
        "Ahora adaptamos nuestro entorno de Aprendizaje Federado para seleccionar clientes con GreedyFed utilizando las funciones de FLEX. Para ello, vamos a preparar el entorno antes de pasar directamente con el algoritmo de selección.\n",
        "\n",
        "En primer lugar, GreedyFed [1] define un conjunto de datos de validación que utiliza el algoritmo de aproximación de valores de Shapley GTG-Shapley [2] para poder calcular la contribución marginal de los $M$ clientes seleccionados en la ronda actual. Para esto, el algoritmo GTG-Shapley necesita de un conjunto de validación $\\mathcal D_{\\text{val}}$ para poder calcular esta contribución. Para ello, separaremos el dataset de entrenamiento en dos subconjuntos de datos: uno de entrenamieto y otro de validación, siendo el tamaño del de validación un $10\\%$ de los datos totales de entrenamiento aleatoriamente separados, para lo cual contaríamos con $6000$ ejemplos en MNIST y $5000$ ejemplo en CIFAR-10.\n",
        "\n",
        "$$\n",
        "\\mathcal D_\\text{T} = \\mathcal D_\\text{train} \\cup \\mathcal D_\\text{val}\\\\\n",
        "\\begin{cases}\n",
        "N &= |\\mathcal D_\\text{T}|,\\\\\n",
        "N_\\text{val} &= \\lfloor 0.1 \\times N\\rfloor,\\\\\n",
        "N_\\text{train} &= N-N_\\text{val}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "El algoritmo GTG-Shapley es un algoritmo de naturaleza combinatoria, donde en cada ronda sufre una complejidad en tiempo de $\\mathcal O(M \\log M)$ donde $M$ es el número de clientes seleccionados por ronda (i.e. el _presupuesto_ de clientes seleccionados). Por tanto, la distribución de los datos en este esquema de selección tendrá un número total de clientes $K$ más pequeño que en otros métodos, por lo que establecemos un número de $K=300$ nodos sin contar el servidor central para MNIST y $K=100$ para CIFAR-10 que son valores muy similares usados por los autores de [1].\n",
        "\n",
        "Dicho lo anterior, federamos manualmente MNIST para poder definir los parámetros comentados así como en el caso de CIFAR-10.\n",
        "\n",
        "> [1] https://arxiv.org/abs/2312.09108\n",
        "> [2] https://arxiv.org/abs/2109.02053"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga_F2xKqpMY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a893d696-a640-40b4-89aa-3048e54b06f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 56782215.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 2114577.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 13869535.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4861068.84it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/flex/data/dataset.py:130: RuntimeWarning: The input dataset and arguments are not explicitly supported, therefore they might not work as expected.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from flex.data import Dataset, FedDatasetConfig, FedDataDistribution\n",
        "\n",
        "K_mnist = 300\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root='.',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=None  # we apply them later in training process\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root='.',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=None  # we apply them later in training process\n",
        ")\n",
        "\n",
        "# split test data into training and validation datasets (val data ~ 10% of train data)\n",
        "val_len = math.floor(0.1 * len(train_data))\n",
        "train_len = len(train_data) - val_len\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_len, val_len])\n",
        "\n",
        "config = FedDatasetConfig(seed=33)\n",
        "config.replacement = False\n",
        "config.n_nodes = K_mnist\n",
        "\n",
        "flex_dataset_mnist = FedDataDistribution.from_config(\n",
        "    centralized_data=Dataset.from_torchvision_dataset(train_data), config=config\n",
        ")\n",
        "\n",
        "# assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_mnist[server_id] = Dataset.from_torchvision_dataset(test_data)\n",
        "mnist_val_data = deepcopy(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CUa-A4piFcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1764ddfd-def0-42c4-b650-5745523f7f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "K_cifar = 100\n",
        "\n",
        "train_data = datasets.CIFAR10(\n",
        "    root='.',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=None  # we apply them later in training process\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root='.',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=None  # we apply them later in training process\n",
        ")\n",
        "\n",
        "# split train data into training and validation datasets (val data ~ 10% of train data)\n",
        "val_len = math.floor(0.1 * len(train_data))\n",
        "train_len = len(train_data) - val_len\n",
        "\n",
        "train_data, val_data = random_split(train_data, [train_len, val_len])\n",
        "\n",
        "config = FedDatasetConfig(seed=33)\n",
        "config.replacement = False\n",
        "config.n_nodes = K_cifar\n",
        "\n",
        "flex_dataset_cifar = FedDataDistribution.from_config(\n",
        "    centralized_data=Dataset.from_torchvision_dataset(train_data), config=config\n",
        ")\n",
        "\n",
        "# assign test data to server_id\n",
        "server_id = \"server\"\n",
        "flex_dataset_cifar[server_id] = Dataset.from_torchvision_dataset(test_data)\n",
        "cifar_val_data = deepcopy(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVQR6hq6iFcA"
      },
      "source": [
        "### Asignar $\\mathcal D_\\text{val}$ a los agregadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT8GofR6iFcA"
      },
      "source": [
        "Hemos podido separar el dataset de entrenamiento en uno de training y otro de validación. Ahora debemos de crear la función que almacene este conjunto en el modelo del servidor $^1$ para posteriormente usarlo al ejecutar el algoritmo GTG-Shapley. Creamos, por tanto, una función `save_validation_data` que guardará el conjunto de validación que hemos creado en los modelos de los agregadores.\n",
        "\n",
        "> $^1$ Recordemos que en la arquitectura en la que trabajamos, los servidores tienen los roles de _servers_ y _aggregators_ por lo que se usan ambos nombres indistintamente para referirse al servidor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFO7Bgj0iFcB"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def save_validation_data(agg_flex_model: FlexModel, _test_data: Dataset, **kwargs):\n",
        "    agg_flex_model[\"val_data\"] = deepcopy(kwargs[\"val_data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pe6p0qsiFcB"
      },
      "source": [
        "### Función de utilidad $\\mathcal U(w)$\n",
        "\n",
        "El algoritmo de aproximación de valores Shapley GTG-Shapley define una función $\\mathcal U(\\text w) := -\\mathcal L(\\text w;\\mathcal D_\\text{val})$ que estima la utilidad de un modelo $\\text w$ sobre el conjunto de validación $\\mathcal D_\\text{val}$ negando el valor de pérdida sobre este último. De esta manera, se puede dar una valuación a un modelo agregado que es creciente cuanto mejor predice el conjunto de validación, lo que será útil para tener una métrica que mide que tan positiva es la contribución marginal de un cliente concreto $k \\in S_t$.\n",
        "\n",
        "En este caso, esta función de utilidad la usará el algoritmo GTG-Shapley, por lo que definimos una función `utility` al cual le pasamos como parámetro un modelo de aprendizaje (como lo es un módulo de NN de PyTorch), el dataset de validación y la función de pérdida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Y47s2ZiFcB"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose\n",
        "from typing import Callable, Optional\n",
        "\n",
        "def utility(model: nn.Module, val_data: Dataset, criterion: Callable, transforms: Optional[Compose]):\n",
        "    \"\"\"\n",
        "    Computes the negative loss of a model over a validation dataset.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "        - model: Model to calculate utility.\n",
        "        - val_data: Dataset on which utility is going to be computed.\n",
        "        - criterion: loss function of the problem.\n",
        "        - transforms: data transform functions.\n",
        "\n",
        "    Returns:\n",
        "        - Utility of the model over the validation dataset as the negative loss.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # get test data as a torchvision object\n",
        "    val_dataset = val_data.to_torchvision_dataset(transform=transforms)\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset, batch_size=256, shuffle=True, pin_memory=True\n",
        "    )\n",
        "\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_dataloader:\n",
        "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "            output = model(data)\n",
        "\n",
        "            losses.append(criterion(output, target).item())\n",
        "\n",
        "    # compute average loss\n",
        "    val_loss = sum(losses) / len(losses) if losses else 0\n",
        "\n",
        "    return 1 - val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAPCBjV0iFcB"
      },
      "source": [
        "### Test de convergencia de valores SV\n",
        "\n",
        "En el algoritmo GTG-Shapley, se computan los valores de Shapley (contribución marginal media) para cada cliente $k \\in S_t$ sobre un número de determinado de iteraciones o hasta que los valores converjan computando el cambio medio en un número de iteraciones determinado. Definimos por tanto una función para comprobar si los valores de Shapley de un cliente han convergido en un número de mínimo $20$ iteraciones y para un umbral inferior de $1\\%$ [1].\n",
        "\n",
        "> [1] Código extraído del código hecho por los autores originales: [https://github.com/pringlesinghal/GreedyFed/utils.py](https://github.com/pringlesinghal/GreedyFed/blob/b2d928670bcd014035830c531bea9f76c57b4b70/utils.py#L7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ64zRChiFcC"
      },
      "outputs": [],
      "source": [
        "def convergenceTest(values):\n",
        "    \"\"\"\n",
        "    Compute average change in last 20 iterations and if it less that 1% it has converged\n",
        "    \"\"\"\n",
        "    if len(values) < 20:\n",
        "        return False\n",
        "    else:\n",
        "        last_vals = torch.Tensor(values[-20:]).to(device)\n",
        "        last_val = torch.Tensor([values[-1]]).to(device)\n",
        "\n",
        "        # avoid division by zero\n",
        "        if last_val < 1e-6:\n",
        "            return False\n",
        "\n",
        "        return (\n",
        "            torch.mean(\n",
        "                torch.abs(last_vals - last_val)\n",
        "            )\n",
        "            / torch.abs(last_val)\n",
        "        ) < 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrq0iRsiFcC"
      },
      "source": [
        "### Algoritmo GTG-Shapley\n",
        "\n",
        "Implementamos el algoritmo de aproximación de valores de Shapley GTG-Shapley [1]. Este algoritmo depende de la función de agregación $F$ del servidor, para lo que definimos además una función que obtenga el modelo agregado dado una lista de modelos de los clientes.\n",
        "\n",
        "Los hiperparámetros de este algoritmo son por un lado el máximo de iteraciones $T$ para cada cliente y el umbral de error $ϵ$ para considerar la contribución al modelo agregado (si la contribución es menor que $\\epsilon$ los valores de Shapley $SV_k$ son directamente $0$). En nuestro caso, daremos $T=20 \\times M$ iteraciones ($20$ por cliente seleccionado) y un umbral $\\epsilon=10^{-4}$.\n",
        "\n",
        "> [1] https://arxiv.org/abs/2109.02053."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELLvELWwiFcC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from typing import List\n",
        "\n",
        "def _aggregate(weights: List):\n",
        "    \"\"\"Aggregate with FedAvg given a list of weights.\"\"\"\n",
        "    agg_weights = []\n",
        "    for layer_index in range(len(weights[0])):\n",
        "        weights_per_layer = [weights[layer_index] for weights in weights]\n",
        "        weights_per_layer = tl.stack(weights_per_layer)\n",
        "        agg_layer = tl.mean(weights_per_layer, axis=0)\n",
        "        agg_weights.append(agg_layer)\n",
        "    return agg_weights\n",
        "\n",
        "def get_aggregated_model(server_model: nn.Module, client_models: List[nn.Module]):\n",
        "    client_model_weights =  []\n",
        "    for k in client_models:\n",
        "        weight_dict = k.state_dict()\n",
        "        client_model_weights.append([weight_dict[name] for name in weight_dict])\n",
        "\n",
        "    # get aggregated weights\n",
        "    aggregated_weights = _aggregate(client_model_weights)\n",
        "\n",
        "    # copy original model\n",
        "    agg_model = deepcopy(server_model)\n",
        "\n",
        "    # set aggregated weights\n",
        "    with torch.no_grad():\n",
        "        weight_dict = agg_model.state_dict()\n",
        "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
        "            weight_dict[layer_key].copy_(new)\n",
        "\n",
        "    return agg_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd0Z2G66iFcC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# delete above\n",
        "import numpy as np\n",
        "\n",
        "from typing import List\n",
        "\n",
        "# hyperparameters extracted from experimental settings of: https://arxiv.org/abs/2312.09108\n",
        "MAX_ITERATIONS = 50\n",
        "EPSILON = 1e-4\n",
        "\n",
        "def shapley_values_gtg(agg_model: FlexModel, client_flex_models: List[FlexModel]):\n",
        "    \"\"\"\n",
        "    Computes shapley values for the client updates on validation dataset.\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    # get arguments\n",
        "    criterion = agg_model[\"criterion\"]\n",
        "    d_val = agg_model[\"val_data\"]\n",
        "    current_model = agg_model[\"model\"]\n",
        "    transforms = agg_model[\"transforms\"]\n",
        "\n",
        "    num_clients = len(client_flex_models)\n",
        "\n",
        "    shapley_values = [[0] for i in range(num_clients)]\n",
        "    converged = False\n",
        "\n",
        "    T = MAX_ITERATIONS * num_clients\n",
        "    t = 0\n",
        "    threshold = EPSILON\n",
        "\n",
        "    # initial server model t loss\n",
        "    v_init = utility(current_model, d_val, criterion, transforms)\n",
        "\n",
        "    # final model t+1 loss\n",
        "    model_final = get_aggregated_model(\n",
        "        current_model,\n",
        "        [client_flex_models[k][\"model\"] for k in client_flex_models]\n",
        "    )\n",
        "    v_final = utility(model_final, d_val, criterion, transforms)\n",
        "\n",
        "    # if difference in loss on aggregated model is lower than threshold, SVs are 0 (contribution is negligible)\n",
        "    if np.abs(v_final - v_init) < threshold:\n",
        "        epsilon = 1e-9\n",
        "        return [epsilon for i in range(num_clients)]\n",
        "\n",
        "    # id-independent list of client models\n",
        "    client_models = list(client_flex_models.values())\n",
        "\n",
        "    while not converged and t < T:\n",
        "        for k in range(num_clients):\n",
        "            t += 1\n",
        "\n",
        "            # get all possible combinations of selected clients having k in idx=0\n",
        "            client_permutation = np.concatenate(\n",
        "                (\n",
        "                    np.array([k]),\n",
        "                    np.random.permutation(\n",
        "                        [i for i in range(num_clients) if i != k]\n",
        "                    )\n",
        "                )\n",
        "            ).astype(int)\n",
        "\n",
        "            v_j = v_init\n",
        "\n",
        "            for j in range(num_clients):\n",
        "                if np.abs(v_final - v_j) < threshold:\n",
        "                    v_jp1 = v_j\n",
        "                else:\n",
        "                    subset = client_permutation[: (j + 1)]\n",
        "                    client_models_subset = [client_models[i][\"model\"] for i in subset]\n",
        "\n",
        "                    # aggregate the subset-clients\n",
        "                    model_subset = get_aggregated_model(current_model, client_models_subset)\n",
        "                    v_jp1 = utility(model_subset, d_val, criterion, transforms)\n",
        "\n",
        "                phi_old = shapley_values[client_permutation[j]][-1]\n",
        "                phi_new = ((t - 1) * phi_old + (v_jp1 - v_j)) / t\n",
        "                shapley_values[client_permutation[j]].append(phi_new)\n",
        "                v_j = v_jp1\n",
        "\n",
        "        # check if SV values converged\n",
        "        shapley_avg = np.mean(shapley_values, axis=0)\n",
        "        converged = convergenceTest(shapley_avg)\n",
        "\n",
        "    if not converged:\n",
        "        print(\"not converged in SV GTG\")\n",
        "\n",
        "    final_shapley_values = [shapley_values[i][-1] for i in range(num_clients)]\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"GTG-Shapley lasted: {end - start} seconds to complete\")\n",
        "\n",
        "    return final_shapley_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVTNG0iwiFcD"
      },
      "source": [
        "### Training Loop final\n",
        "\n",
        "Finalmente, redefinimos la función de bucle iterativo de aprendizaje donde se siguen los siguientes pasos definidos en [1]:\n",
        "\n",
        "1. Asigna el conjunto de validación al servidor.\n",
        "2. Se seleccionan los clientes por Round-Robin (RR) para las primeras épocas de inicialización de valores Shapley, o seleccionando por criterio Greedy (elegir los clientes con SV más grandes).\n",
        "3. Se actualiza el modelo por los clientes seleccionados.\n",
        "4. Se calculan los valores de Shapley de cada cliente $k\\in S_t$.\n",
        "5. Se computa los valores de Shapley cumulativos de cada cliente $k\\in S_t$ (valores UCB).\n",
        "\n",
        "> [1] https://arxiv.org/abs/2312.09108"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhuOh3p9iFcD"
      },
      "outputs": [],
      "source": [
        "from torch import topk\n",
        "\n",
        "from flex.data import Dataset\n",
        "\n",
        "def train_n_rounds(pool: FlexModel, n_rounds: int, clients_per_round=20, problem: Problem = \"mnist\"):\n",
        "    \"\"\"\n",
        "    FL training loop for a certain number of rounds and clients selected.\n",
        "    \"\"\"\n",
        "    # select transformations depending on problem to solve and assign to server\n",
        "    transforms = mnist_transforms if problem == \"mnist\" else cifar_transforms\n",
        "\n",
        "    def set_transforms(server_model, _):\n",
        "        server_model[\"transforms\"] = transforms\n",
        "    pool.servers.map(set_transforms)\n",
        "\n",
        "    # assign validation dataset to server\n",
        "    val_data = mnist_val_data if problem == \"mnist\" else cifar_val_data\n",
        "    flex_val_data = Dataset.from_torchvision_dataset(val_data)\n",
        "    pool.servers.map(save_validation_data, val_data=flex_val_data)\n",
        "\n",
        "    num_clients = len(pool.clients)\n",
        "\n",
        "    UCB = [0 for i in range(num_clients)]\n",
        "    SV = [0 for i in range(num_clients)]\n",
        "    SV_curr = [0 for i in range(num_clients)]\n",
        "    N_t = [0 for i in range(num_clients)]\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    for i in range(n_rounds):\n",
        "        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n",
        "\n",
        "        # select clients to transmit weights to\n",
        "        # initially sample every client atleast once (RR)\n",
        "        selected = [False for _ in range(num_clients)]\n",
        "        if i < np.floor(num_clients / clients_per_round):\n",
        "            print(\"RR round\")\n",
        "            for idx in range(i * clients_per_round, (i + 1) * clients_per_round):\n",
        "                selected[idx] = True\n",
        "                N_t[idx] += 1\n",
        "        elif i == np.floor(num_clients / clients_per_round):\n",
        "            print(\"Last RR round\")\n",
        "            for idx in range(i * clients_per_round, num_clients):\n",
        "                selected[idx] = True\n",
        "                N_t[idx] += 1\n",
        "\n",
        "            remaining_selections = clients_per_round * (i + 1) - num_clients\n",
        "            if remaining_selections > 0:\n",
        "                unselected_indices = list(range(0, i * clients_per_round))\n",
        "                selected_indices_subset = np.random.choice(\n",
        "                    unselected_indices, size=remaining_selections, replace=False\n",
        "                )\n",
        "\n",
        "                for idx in selected_indices_subset:\n",
        "                    selected[idx] = True\n",
        "                    N_t[idx] += 1\n",
        "        else:\n",
        "            # UCB Greedy Selection (select clients with highest SV)\n",
        "            _, selected_indices = topk(torch.Tensor(UCB), clients_per_round)\n",
        "            print(\"Best selected: \", selected_indices)\n",
        "            for idx in selected_indices.tolist():\n",
        "                selected[idx] = True\n",
        "                N_t[idx] += 1\n",
        "\n",
        "        # take selected clients\n",
        "        selected_indices = [index for index, value in enumerate(selected) if value]\n",
        "        selected_clients_pool = pool.clients.select(lambda actor_id, actor_roles: actor_id in selected_indices)\n",
        "        selected_clients = selected_clients_pool.clients\n",
        "\n",
        "        print(f\"Selected clients for this round: {len(selected_clients)}\")\n",
        "\n",
        "        # Deploy the server model to the selected clients\n",
        "        pool.servers.map(copy_server_model_to_clients, selected_clients)\n",
        "\n",
        "        # Each selected client trains its model\n",
        "        selected_clients.map(train, transforms=transforms)\n",
        "\n",
        "        # The aggregador collects weights from the selected clients and aggregates them\n",
        "        pool.aggregators.map(get_clients_weights, selected_clients)\n",
        "\n",
        "        # The aggregator aggregates client updates with FedAveraging\n",
        "        pool.aggregators.map(aggregate_with_fedavg)\n",
        "\n",
        "        # compute SV values for each selected client in the server\n",
        "        shapley_values = pool.aggregators.map(shapley_values_gtg, selected_clients)[0]\n",
        "\n",
        "        print(\"GTG-Shapley values: \", shapley_values)\n",
        "\n",
        "        # The aggregator send its aggregated weights to the server\n",
        "        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n",
        "\n",
        "        # compute the cumulative SV of each client\n",
        "        counter = 0\n",
        "        for k in range(num_clients):\n",
        "            if selected[k]:\n",
        "                SV_curr[k] = shapley_values[counter]\n",
        "\n",
        "                # previous num selections weight/penalty\n",
        "                prev_wt = (N_t[k] - 1) / N_t[k]\n",
        "                # current num selections weight/penalty\n",
        "                curr_wt = 1 - prev_wt   # 1 / N_t[k]\n",
        "\n",
        "                # previous cumulative SV\n",
        "                prev_sv = SV[k]\n",
        "\n",
        "                # t-round SV\n",
        "                curr_sv = SV_curr[k]\n",
        "\n",
        "                # compute new cumulative SV\n",
        "                SV[k] = prev_wt * prev_sv + curr_wt * curr_sv\n",
        "\n",
        "                counter += 1\n",
        "            else:\n",
        "                SV_curr[k] = 0\n",
        "\n",
        "            UCB[k] = SV[k]\n",
        "\n",
        "        metrics = pool.servers.map(evaluate_global_model)\n",
        "        loss, acc = metrics[0]\n",
        "\n",
        "        losses.append(loss)\n",
        "        accuracies.append(acc)\n",
        "        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}\")\n",
        "\n",
        "    return losses, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool_mnist = FlexPool.client_server_pool(flex_dataset_mnist, init_func=build_server_model_mlp)\n",
        "pool_cifar = FlexPool.client_server_pool(flex_dataset_cifar, init_func=build_server_model_cnn)\n",
        "\n",
        "losses, accuracies = train_n_rounds(pool_mnist, n_rounds=150, clients_per_round=3, problem=\"mnist\")\n",
        "plot_loss_accuracy(losses, accuracies, title=\"Learning curves on MNIST\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "enfdsr6OWefZ",
        "outputId": "db8e0035-83d2-452b-93b1-adc9593f405a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running round: 1 of 150\n",
            "RR round\n",
            "Selected clients for this round: 3\n",
            "GTG-Shapley lasted: 95.65697169303894 seconds to complete\n",
            "GTG-Shapley values:  [0.42250156934772237, 0.32360289943596676, 0.41195312177851096]\n",
            "Server: Test acc: 0.6532, test loss: 1.1745\n",
            "\n",
            "Running round: 2 of 150\n",
            "RR round\n",
            "Selected clients for this round: 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9323fd863045>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpool_cifar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlexPool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_server_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflex_dataset_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_server_model_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_n_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients_per_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mnist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplot_loss_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Learning curves on MNIST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-80bf6519c1d6>\u001b[0m in \u001b[0;36mtrain_n_rounds\u001b[0;34m(pool, n_rounds, clients_per_round, problem)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# compute SV values for each selected client in the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mshapley_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapley_values_gtg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GTG-Shapley values: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapley_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flex/pool/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, dst_pool, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             ]\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mFlexPool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             res = [\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flex/pool/pool.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mFlexPool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             res = [\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             ]\n",
            "\u001b[0;32m<ipython-input-22-21f9648fa4df>\u001b[0m in \u001b[0;36mshapley_values_gtg\u001b[0;34m(agg_model, client_flex_models)\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0;31m# aggregate the subset-clients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0mmodel_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_aggregated_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_models_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mv_jp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mphi_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshapley_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient_permutation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0c4b5c9fa024>\u001b[0m in \u001b[0;36mutility\u001b[0;34m(model, val_data, criterion, transforms)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flex/data/dataset_pt_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPILImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n\u001b[1;32m    129\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupport\u001b[0m \u001b[0mtorchscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ad-STpCNYaJM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}